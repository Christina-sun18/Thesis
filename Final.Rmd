---
compact-title: no
editor_options:
  chunk_output_type: console
  header-includes:
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{setspace}\doublespacing
  - \usepackage[hyphens]{url}
  - \usepackage{float}
  - \usepackage[bottom]{footmisc}
fontsize: 12pt
geometry: margin=1in
output:
  pdf_document:
    fig_height: 3.5
    fig_width: 4.5
link-citations: yes
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE,
  warning = FALSE)
```


```{r}
knitr::include_graphics("thesis cover page .pdf")
```

\newpage 

# Abstract 

With correct model specification being a major challenge for conventional likelihood-based prediction, scoring rules have been used to produce forecasts that are accurate – in a well-defined sense - in the presence of misspecification. Building on previous research, this paper conducts numerical and empirical analyses to explore the impact of the form and degree of model misspecification on probabilistic forecasts that are optimal under a given score. In particular, we assess the predictive performance of optimal forecasts in two empirical contexts: prediction of the Value-at-Risk of a financial return and prediction of the VIX volatility index. Results show that the greater the degree of model misspecification, the more beneficial is optimal prediction, no matter what the form of model misspecification. Most notably, predictions that are optimal according to the censored likelihood score, which rewards predictive accuracy in a tail are substantially more accurate at predicting extreme events in said tail than the alternatives, including likelihood-based predictions.^[I would like to thank my supervisors: Professor Gael Martin and Dr. Ruben Loaiza-Maya, for the very constructive and detailed comments on my thesis and their support and help during this particularly difficult year.]

\newpage

```{=latex}
% Trigger ToC creation in LaTex
\setcounter{tocdepth}{2}
\tableofcontents
```

\newpage

# 1. Introduction

This paper explores the question: ‘When do optimal probabilistic forecasts work’, with particular attention given to the usefulness of optimal forecasts in financial applications. 

Probabilistic forecasts can provide complete information about future uncertainty, which can be more valuable to forecasters than point and interval forecasts. However, previous approaches to prediction - including probabilistic methods – typically assume that the predictive model, or the likelihood function, correctly specifies the true data generating process (DGP) or that the true DGP lies in a specified set of models, which is almost certainly untrue in practice. Optimal probabilistic forecasting refers to the construction of a predictive probability distribution that is optimal according to a user-specified proper scoring rule [@optimal]. It is not based on the conventional likelihood function but on a proper scoring rule that matches the specific problem at hand. Optimal probabilistic forecasts aim to yield accurate predictions in the presence of a misspecified predictive model, whilst the conventional approaches to probabilistic forecasting are tied more closely to the predictive model, and are only as accurate as that model itself. 

Particular proper scoring rules allow users to produce probabilistic predictions that are designed to perform well according to the forecasting metric that is important to the problem at hand [@gneiting2007]. Some recent research shows that forecasts produced using a given scoring rule can yield better out-of-sample accuracy - measured by that scoring rule - than conventional likelihood-based forecasts, in particular in the presence of model misspecification; see @opschoor2017, @fbp2020 and @optimal. Among all these investigations, it is the censored likelihood score (CLS) proposed originally by @diks2011, that captures our attention for the purpose of demonstrating the application of optimal probabilistic forecasts in finance. Its ‘focusing’ feature has great potential in financial risk management. In other words, CLS allows a forecaster to focus on predicting any particular region of interest accurately. Therefore, its use in this context represents an important contribution to risk prediction in financial settings. 

Whilst some empirical investigations of the performance of optimal forecasts were undertaken in @optimal, these were primarily illustrative, and left much scope for exploring the performance of optimal probabilistic forecasting methods in forecasting different types of financial measures, and under different assumed models. This research paper extends the work of @optimal by investigating the effect of the form and degree of model misspecification on the performance of optimal probabilistic forecasts in financial settings; in particular by conducting empirical analyses of how optimal forecasts perform in predicting the Value-at-Risk (VaR) and the VIX volatility index (VIX). Importantly, the empirical dataset used extends over a period of time that precedes the 2008 global financial crisis (GFC) and that includes the latest period in which COVID-19 has had an impact on financial markets. 

The paper proceeds as follows. Section 2 explains the basic idea about how scoring rules are applied in producing and evaluating density forecasts, and provides the definitions of some commonly used scoring rules. Section 3 focuses on investigating the effects of different forms and degrees of model misspecification on the performance of optimal forecasts, using data simulated from the (generalized) autoregressive conditional heteroscedasticity ((G)ARCH) and inversion copula models. The assumed predictive model is fixed as a Gaussian ARCH(1) model for the purpose of controlling the degree of model misspecification. In Section 4, we emulate the simulation exercise in an empirical example for financial returns on the S&P 500 stock index, including the prediction of the VaR. The results illustrate the practical contributions of our predictive approach to financial risk management. The empirical analysis of the VIX is provided in Section 5, where the predictive model is the heterogeneous autoregressive-realized volatility (HAR-RV) model with different error term specifications. We conclude and discuss any possible caveats in Section 6. 

# 2. Scoring rules in prediction
## 2.1 Overview and notation 

Scoring rules $S(\cdot,\cdot)$ are a type of criterion function that can be optimized to produce an optimal estimator and hence, a probabilistic prediction derived from this optimal estimator. They also play the role of evaluating and ranking competing probabilistic forecasts, based on materialized events or values, by assigning a numerical score [@gneiting2007]. That is, if the forecaster produces a predictive distribution $\mathit{P}$ and the event $y$ materializes, then the reward to this prediction given by a scoring rule is denoted by $S(P,y)$. For a positively-oriented score, a higher value will be assigned to a better forecast between two competing candidates, on the condition of the scoring rule being proper. The propriety of the scoring rule is crucial here, as an improper scoring rule may assign a higher average score to an incorrect density forecast [@gneiting2007]. Consequently, only proper scoring rules are used in the production of optimal forecasts.

Suppose $P$ is the predictive distribution and $Q_0$ is the true DGP. $\mathbb{S}(P,Q_0)$ denotes the expected score of $S(P,\cdot)$ under $Q_0$. That is,
\begin{equation} \label{eq:score}
\mathbb{S}(P,Q_0)=\int_{y \in \Omega}S(P,y)dQ_0(y)
\end{equation}
where $\Omega$ is a probability space. 
A scoring rule is said to be proper if $\mathbb{S}(Q_0,Q_0) \geq \mathbb{S}(P,Q_0)$ for all $P$ and $Q_0$, and is strictly proper if $\mathbb{S}(Q_0,Q_0) = \mathbb{S}(P,Q_0)$ only when $P = Q_0$ [@gneiting2007]. This ensures that a proper scoring rule is maximized when the prediction reveals the truth, on the condition that the predictive distribution class $\mathcal{P}$ contains $Q_0$. 

However, in practice, the expected score $\mathbb{S}(\cdot,Q_0)$ is unknown and cannot be obtained, but we can use a sample average $\bar S(\cdot)$ as a reasonable estimation. Asymptotically, the true predictive distribution can be recovered by optimizing a sample criterion of any proper scoring rule, if the true predictive is contained in the predictive class over which the maximization occurs. Even when $Q_0$ is not contained in the predictive distribution class $\mathcal{P}$ proposed by the forecaster, it does not change the fact that a proper scoring rule will reward a particular form of forecast accuracy and hence, select the best forecast within the proposed predictive distribution class, according to this scoring rule. 

Assume the scoring rule is positively-oriented and the unknown parameters of the assumed predictive model are denoted as $\boldsymbol \theta \in \Theta$, and let $P^{t-1}_{\boldsymbol{\theta}}$ := $P(\cdot|\mathcal{F}_{t-1},\boldsymbol \theta)$ be the one-step-ahead predictive distribution function based on the assumed model where $\mathcal{F}_{t-1}$ represents all the information available at time $t-1$, and $p^{t-1}_{\boldsymbol{\theta}}$ := $p(\cdot|\mathcal{F}_{t-1},\boldsymbol \theta)$ be the corresponding predictive density function at time $t$. 
Then an estimator $\boldsymbol{\hat \theta}$ obtained by maximizing $\bar S(\boldsymbol{\theta})$ is said to be 'optimal' based on this scoring rule and $P^{t-1}_{\boldsymbol{\hat \theta}}$ is referred to as the optimal predictive. 
Specifically, we define 
\begin{equation} \label{eq:1}
\boldsymbol{\hat \theta} =arg \max_{\boldsymbol{\theta} \in \Theta} \bar S(\boldsymbol{\theta})
\end{equation}
where 
\begin{equation} \label{eq:sbar}
\bar S = \frac{1}{T-(\tau+1)}\sum_{t=2}^{T-\tau} S(P_{\boldsymbol{\theta}}^{t-1},y_t)
\end{equation}
with $\mathit{T}$ the total number of observations on a time series variable $y_t$ and $\tau$ the out-of-sample size. Under certain conditions, including that the scoring rule is proper and that the predictive model is correctly specified, $\boldsymbol{\hat \theta} \to \boldsymbol{\theta}_0 \ as \ \mathit{T} \to \infty$ where $\boldsymbol{\theta}_0$ represent the true (vector) parameter.

## 2.2 Some commonly used scoring rules 

A variety of scoring rules have been developed to tackle different problems. Some commonly used proper scoring rules are adopted in the analyses in this paper, such as the logarithmic score (LS), the continuously ranked probability score (CRPS) and the CLS. 

The LS is defined as
\begin{equation} \label{eq:ls}
S_{LS}(P^{t-1}_{\boldsymbol \theta}, y_t) = ln[p(y_t|\mathcal{F}_{t-1},\boldsymbol{\theta})]
\end{equation}
where $p(\cdot|\mathcal{F}_{t-1},\boldsymbol{\theta})$ is the predictive density function. It is a local strictly proper scoring rule, which means it will assign a higher score to the correct probabilistic forecast, and it is superior to quadratic and spherical scoring rules when the rank ordering is important or the impact of the nonlinear utility function used is a concern for forecasters [@bickel2007]. One common usage of the LS in frequentist econometrics is maximum likelihood estimation (MLE). That is, 
\begin{equation} \label{eq:mle}
\boldsymbol{\hat \theta}_{MLE}=arg \max_{\boldsymbol{\theta} \in \Theta} \{\frac{1}{T} \sum_{t=1}^T ln[p(y_t|\mathcal{F}_{t-1},\boldsymbol \theta)]\}
\end{equation}
LS yields an asymptotically efficient estimator of $\boldsymbol \theta$ under correct specification and regularity.

However, the LS is criticized for its unboundedness and its local property. @bernardo1979 states "locality requires the utility of probabilistic influence to depend only upon the probability density of the true state". @gneiting2007 also argue that the LS is insensitive to distance and will not reward predictions that are close to but not identical to the materialized event. @gneiting2007 propose the continuously ranked probability score, which is sensitive to distance and is defined as
\begin{equation} \label{eq:crps}
S_{CRPS}(P^{t-1}_{\boldsymbol \theta},y_t)=-\int_{-\infty}^{\infty}[P(y|\mathcal{F}_{t-1},\boldsymbol{\theta})-I(y\geq y_t)]^2dy
\end{equation}
where $\mathit{P}(\cdot|\mathcal{F}_{t-1},\boldsymbol{\theta})$ is the predictive cumulative distribution function, *I* is the indicator function, and $y_t$ is the materialized event. 

The formula can be simplified as follows if the predictive distribution is Gaussian with mean $\mu$ and variance $\sigma^2$, 
\begin{equation} \label{eq:crnorm}
S_{CRPS}(N^{t-1}_{\boldsymbol \theta},y_t)=\sigma[\frac{1}{\sqrt{\pi}}-2\phi(\frac{y_t-\mu}{\sigma})-\frac{y_t-\mu}{\sigma}(2\Phi(\frac{y_t-\mu}{\sigma})-1)]
\end{equation}
where $\phi$ and $\Phi$ are the probability density function and cumulative distribution function of the standard Gaussian distribution.

Both LS and CRPS are used for producing predictive densities and evaluating the out-of-sample predictive performance. In terms of accurately predicting a certain region of a distribution, they are typically used together with weighted likelihood [@gneiting2011]. @diks2011 propose the CLS, which allows users to reward forecasts only on a region (regions) of interest instead of using weights to emphasize a particular part of the entire density forecast. Moreover, it can be easily used to combine density forecasts and yield better predictive accuracy. It is defined as
\begin{equation} \label{eq:cls}
S_{CLS}(P^{t-1}_{\boldsymbol \theta}, y_t)=I(y_{t} \in A)ln p(y_{t}|\mathcal{F}_{t-1},\boldsymbol{\theta})+I(y_{t} \in A^c)ln[\int_{A^c} p(y|\mathcal{F}_{t-1},\boldsymbol{\theta})dy]
\end{equation}
where $p(\cdot|\mathcal{F}_{t-1},\boldsymbol{\theta})$ is the predictive probability density function, $A$ is the region of interest and $A^c$ is the complement of $A$. @opschoor2017 prove that weighted density forecasts based on optimizing CLS outperform those on LS and CRPS. 

# 3. Numerical investigation of optimal predictions
## 3.1 Simulation design 

Optimal forecasts can outperform predictions produced by conventional methods out-of-sample; see @opschoor2017, @fbp2020 abd @optimal; but the underlying reasons driving this phenomenon are still under investigation. Recent research show that the degree of model misspecification impacts the performance of optimal forecasts. The greater the degree of misspecification is, the more benefits are gained from optimal forecasts in both Bayesian and frequentist predictions, conditional on the assumed predictive model being broadly 'compatible' with the true DGP; see @fbp2020 and @optimal. The simulation analysis in this paper will extend the exploration in this field and ask the question ‘Will the performance of optimal forecasts change when a different form of misspecification occurs?’. 

In order to set the scene for the financial application analyses in the following sections of this paper, we simulate a time series variable $y_t$ that mimics the behavior of financial returns and their volatility. Specific simulation scenarios are listed in Table 1 ($t_\nu$ indicates a Student-t distribution with $\nu$ degrees of freedom). 

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{Table 1: Simulation design}} \\ \hline
          & \textbf{Scenario (i)} & \textbf{Scenario (ii)} & \textbf{Scenario (iii)} \\ \hline
\textbf{True DGP} & $y_t = \sigma_t \epsilon_t$ & $y_t = \sigma_t \epsilon_t$ & Stochastic volatility inversion copula \\
& $\sigma_t^2 = 1 + 0.2y_{t-1}^2$ & $\sigma_t^2 = 1 + 0.2y_{t-1}^2+0.7\sigma_{t-1}^2$ & Shape parameter = \{0, -3, -5\} \\ & $\epsilon_t \sim N(0,1)$ & $\epsilon_t \sim (\frac{\nu-2}{\nu})^{0.5} * t_\nu$ & \\ & & $\nu \in \{3,12,35\}$ & \\ \hline

\textbf{Assumed model} & $y_t = \mu +\sigma_t \epsilon_t$ & $y_t = \mu +\sigma_t \epsilon_t$ & $y_t = \mu +\sigma_t \epsilon_t$ \\ & $\sigma_t^2 = \alpha_0 + \alpha_1 (y_{t-1}-\mu)^2$ & $\sigma_t^2 = \alpha_0 + \alpha_1 (y_{t-1}-\mu)^2$ & $\sigma_t^2 = \alpha_0 + \alpha_1 (y_{t-1}-\mu)^2$ \\ & $\epsilon_t \sim N(0,1)$ & $\epsilon_t \sim N(0,1)$ & $\epsilon_t \sim N(0,1)$ \\ \hline
\end{tabular}}
\caption{The three simulation scenarios used in the numerical analysis. Scenario (i) represents correct model specification; Scenario (ii) represents a form of misspecification where the true DGP is GARCH with Student-t distribution while the assumed model is Gaussian ARCH(1); Scenario (iii) represents a more extreme form of misspecification, where the true DGP is an stochastic volatility model with a non-Gaussian marginal distribution while the assumed model is Gaussian ARCH(1).}
\end{table}

GARCH models are used to capture the volatility clustering and serial dependence that are usually observed in empirical stock returns, and the negative marginal skewness will be incorporated by using a stochastic volatility inversion copula model, with the degree of marginal skewness controlled by the shape parameter. 

Copulas are functions that describe the dependence between random variables or the serial dependence. Copula theory allows one to decompose the joint density of dependent random variables to the product of the marginal densities and the ‘copula density’ denoted as $c(\cdot)$, where the dependent structure is completely described by ‘copula density’. Since the marginal density and the copula does not require to be from the same family of distributions, it makes the choice of marginal density more flexible when one uses the copula-based models [@fan2014]. In this simulation, we aim to have a true DGP that captures two main features of financial returns – the volatility clustering and negative skewness in the marginal distribution. Thus, we only consider the use of copula in describing the serial dependence. Then the ‘decomposition’ can be represented as: 
\begin{equation} \label{eq:copula}
p_{t-1}(y_t)=c(P(y_t),P(y_{t-1}))p(y_{t-1})
\end{equation}
where $\mathit{P}()$ and $\mathit{p}$() denote the marginal distribution and density function of $y_t$ respectively. 

Specifically, it is the stochastic volatility inversion copula model used in the simulation Scenario (iii). The marginal distribution $\mathit{P}()$ is chosen to be the standard skew-normal distribution, while $c(\cdot)$ is chosen to be the inversion copula of the stochastic volatility model described in @inversion2018. The stochastic volatility inversion copula model allows us to keep the dependence structure of a state space model and provides the flexibility of using an asymmetric marginal distribution that is often observed in financial returns [@inversion2018].

In order to manipulate the degree and form of model misspecification, we fix the underlying model to a simple and clearly misspecified ARCH(1) model and then adjust degrees of freedom and shape parameters of the true DGP specifications in Scenario (ii) and (iii). 

We apply the scoring rules introduced in Section 2.2 and set CLS to focus on the 10%, 20%, 80% and 90% tails of the empirical marginal distribution, corresponding to the risks of long and short portfolios of financial investments, and we label them as CLS10, CLS20, CLS80 and CLS90 in the following tables and figures. Due to the fact that there is no reliable estimate of the true conditional densities, only the tails of the marginal distributions are set to be the standard when assessing the predictive accuracy by CLS, rather than the conditional distributions. Meanwhile, LS gives insights about the performance of the conventional likelihood-based methods and is used as a benchmark for comparing whether forecasts that are optimal according to alternative scoring rules can provide benefits or not. 

We conduct Monte Carlo simulations using the methodology in @optimal and @fbp2020. Specific steps are as follows: 

Let $P^{t-1}_{\boldsymbol{\hat \theta^{[i]}}}$ be the one-step-ahead predictive distribution function based on the Gaussian ARCH(1) model and optimized according to the scoring rule $S_i$, and $p(y_t|\mathcal{F}_{t-1},\boldsymbol{\hat \theta^{[i]}})$ be the corresponding predictive density function at time $\mathit{t}$. 

  1.	Generate T = 6000 observations of $y_t$ from the true DGP
  
  2.	Use observations $y_t$ where $t = 1,2,...,1000$ to estimate $\boldsymbol{\hat \theta}^{[i]} = \{\hat \mu, \hat \alpha_0, \hat \alpha_1\}^{[i]}$ as in $\eqref{eq:1}$ based on $S_i$, $i \in \{LS, CRPS, CSL10,CSL20,CSL80,CSL90\}$ where $S_i$ refers to $\eqref{eq:ls}$ $\eqref{eq:crps}$ $\eqref{eq:cls}$
  
  3.	Produce the one-step-ahead predictive $P^{t-1}_{\boldsymbol{\hat \theta^{[i]}}}$, and compute the out-of-sample score $S_j$, $j \in \{LS, CRPS, CSL10,CSL20,CSL80,CSL90\}$
where $S_j$ refers to $\eqref{eq:ls}$ $\eqref{eq:crps}$ $\eqref{eq:cls}$:^[The CRPS in $\eqref{eq:crps}$ is transformed to a positively-oriented score for the purpose of convenient comparison among all scores.]

  4. Expand estimation window by one observation and repeat steps 2-3 with $\tau = T-1000$ times and compute the average scores:^[Out-of-sample size $\tau = 5000$ is sufficiently large to let the optimal forecasts take place; see @optimal for full discussion about sample size effects on optimal forecasts.]

\begin{equation} \label{eq:avgscore}
\bar S_j(\boldsymbol{\hat \theta^{[i]}})=\frac{1}{\tau}\sum^T_{t=T-\tau+1}S_j(P_{\boldsymbol{\hat \theta^{[i]}}}^{t-1},y_t)
\end{equation}

For the purpose of conveniently documenting and interpreting the results consistently with previous literature, we introduce the concept of ‘coherence’ and ‘strict coherence’ here. Coherence means that the optimal probabilistic forecast based on a given score is superior, or at least performs the same as, alternative forecasts according to the same score. Strict coherence happens when the optimal prediction is strictly preferable given that score. Let $\boldsymbol{\hat \theta}_1$ and $\boldsymbol{\hat \theta}_2$ be the optimizers based on scoring rules:$S_1$ and $S_2$. Then the optimal predictions $P^{t-1}_{\boldsymbol{\hat \theta_1}}$ and $P^{t-1}_{\boldsymbol{\hat \theta_2}}$ can be derived from $\boldsymbol{\hat \theta}_1$ and $\boldsymbol{\hat \theta}_2$. The coherent results are observed if 
\begin{equation} \label{eq:coh1}
\frac{1}{\tau}\sum_{t=T-\tau+1}^{T}S_1(P^{t-1}_{\boldsymbol{\hat \theta_1}},y_t) \geq \frac{1}{\tau}\sum_{t=T-\tau+1}^{T}S_1(P^{t-1}_{\boldsymbol{\hat \theta_2}},y_t)
\end{equation}
\begin{equation} \label{eq:coh2}
\frac{1}{\tau}\sum_{t=T-\tau+1}^{T}S_2(P^{t-1}_{\boldsymbol{\hat \theta_1}},y_t) \leq \frac{1}{\tau}\sum_{t=T-\tau+1}^{T}S_2(P^{t-1}_{\boldsymbol{\hat \theta_2}},y_t)
\end{equation}
It is said to be strictly coherent if $\eqref{eq:coh1}$ and $\eqref{eq:coh2}$ are strict inequalities [@optimal].


## 3.2 Simulation results
### 3.2.1 Average out-of-sample scores

The first column of each of the following tables presents the labels for the $S_i$ that we used to produce predictions and the third-row shows $S_j$, which is used for the forecast's evaluation. The bolded numbers are the largest values of $\bar S_j(\boldsymbol{\hat \theta^{[i]}})$ in each column. We use all positively-oriented scoring rules, and therefore, the column maximum indicates the optimal prediction based on the scoring rule indicated by the column name. 

Table 2 shows the correct model specification - Scenario (i) - where the optimal forecasting method does not reap benefits and there is no strict coherence observed in the table. Under correct specification, no matter what the score rewards, estimators produced from $S_i$ are expected to concentrate to the same true parameter values as $T$ approches to infinity, which causes no distinct improvement in predictive accuracy among different scoring rules [@optimal].

Table 3 shows the results of Scenario (ii) where the assumed predictive model, ARCH(1), cannot capture the fat tails and volatility clustering feature of the data generated from the GARCH(1,1) model with Student-t error, i.e. the true DGP. Hence, there is misspecification. Tables 2 and 3 replicate some parts of the simulation results in previous literature; see @fbp2020, @optimal. In Panels A and B of Table 3, when the degree of model misspecification is relatively low, the six optimizers cannot produce distinguishable enough probabilistic predictions to yield significantly different out-of-sample performance. There is no strict coherence observed. As the degree of model misspecification increases, in Panel C, the strict coherence is observed. The ARCH(1) model with Gaussian errors is now misspecified enough to let these optimisers produce very distinct predictions and the optimal prediction produced based on one scoring rule cannot be beaten out-of-sample according to that same scoring rule. 

Optimal forecasts are expected to reap more benefits as the degree of model misspecification increases, and predictions based on CLS at 80% and 90% perform poorly when assessed by LS, CRPS, CLS at 10% and CLS at 20%. The question is whether these characteristics of optimal forecasts will change if we move to a different form of model misspecification. 

Table 4 shows another type of model misspecification, listed in Scenario (iii), where the ARCH(1) model cannot capture the asymmetric feature of the data generated from the stochastic volatility inversion copula model (the true DGP). Table 4 shows similar results as in Table 3. In Panel A, when the true DGP is symmetric (shape = 0), optimal predictions are non-distinguishable and hence, yield very similar out-of-sample performance. There are no significant benefits reaped from using optimal probabilistic forecasts. However, as the shape parameter decreases, the degree of negative marginal skewness increases, as shown in Panels B and C, optimal predictions produced are more different from each other and yield different average out-of-sample scores. Even though, strictly speaking, we do not observe strict coherence in Panel C, the most misspecified case, it is close enough to conclude that optimal forecasts perform in the same way even when we have different forms of model misspecification. ^[Tables are produced by R package: @kableextra and @knitr.] 


```{r, message=FALSE}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(tsibble)
library(tsibbledata)
library(feasts)
library(normtest)
library(moments)
library(ggthemes)
library(ggpubr)
library(knitr)
library(kableExtra)
```

```{r}
# Editting the matlab written csv files 

GARCH_t35 <- read.csv("data/GARCH35.csv")
GARCH_t35 <- rename(GARCH_t35, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
GARCH_t35$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

GARCH_t12 <- read.csv("data/GARCH_t12__table.csv")
GARCH_t12 <- rename(GARCH_t12, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
GARCH_t12$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

GARCH_t3 <- read.csv("data/GARCH_t3_table.csv")
GARCH_t3 <- rename(GARCH_t3, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
GARCH_t3$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

ARCH_table <- read.csv("data/ARCH_table.csv")
ARCH_table <- rename(ARCH_table, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
ARCH_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

Copula5_table <- read.csv("data/Copula5_table.csv")
Copula5_table <- rename(Copula5_table, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
Copula5_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

Copula3_table <- read.csv("data/Copula3_table.csv")
Copula3_table <- rename(Copula3_table, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
Copula3_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

Copula0_table <- read.csv("data/Copula0_table.csv")
Copula0_table <- rename(Copula0_table, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
Copula0_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

```


```{r}
ARCH_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 2: the true DGP is ARCH(1)"=7))
```

Table 2: Average out-of-sample scores under the correctly specified ARCH(1) model with Gaussian errors, corresponding to Scenario (i) in Table 1. The row names are the in-sample optimizers used in producing the probabilistic forecasts and the column names are the criterion functions used to evaluate the out-of-sample performance of the probabilistic forecasts produced by each optimizer. The bolded value is the column maximum indicating the best performing optimiser according to the out-of-sample criterion function.


```{r}
GARCH_t35 <- GARCH_t35 %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  )

GARCH_t12 <- GARCH_t12 %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  )

GARCH_t3 <- GARCH_t3 %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) 
garch <- rbind(GARCH_t35,GARCH_t12,GARCH_t3)

garch %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  pack_rows("Panel A: degree of freedom = 35", 1,6) %>%
  pack_rows("Panel B: degree of freedom = 12", 7,12) %>%
  pack_rows("Panel C: degree of freedom = 3",13,18) %>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 3: the true DGP is GARCH(1,1) with Student-t error"=7))
```

Table 3: Average out-of-sample scores under the misspecified ARCH(1) model with Gaussian errors, corresponding to Scenario (ii) in Table 1. The row names are the in-sample optimizers used in producing the probabilistic forecasts and the column names are the criterion functions used to evaluate the out-of-sample performance of the probabilistic forecasts produced by each optimizer. The bolded value is the column maximum indicating the best performing optimiser according to the out-of-sample criterion function. Panel A, B and C shows the average out-of-sample scores of the true DGP - GARCH(1,1) with $t_{\nu=35}$, $t_{\nu=12}$, $t_{\nu=3}$ errors.


```{r}
Copula0_table <- Copula0_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) 

Copula3_table <- Copula3_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) 

Copula5_table <- Copula5_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) 

copula <- rbind(Copula0_table,Copula3_table,Copula5_table)
copula %>% 
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  pack_rows("Panel A: shape = 0", 1,6) %>%
  pack_rows("Panel B: shape = -3", 7,12) %>%
  pack_rows("Panel C: shape = -5", 13,18) %>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 4: the true DGP is stochastic volatility inversion copula"=7))
```

Table 4: Average out-of-sample scores under the misspecified ARCH(1) model with Gaussian errors, corresponding to Scenario (iii) in Table 1. The row names are the in-sample optimizers used in producing the probabilistic forecasts and the column names are the criterion functions used to evaluate the out-of-sample performance of the probabilistic forecasts produced by each optimizer. The bolded value is the column maximum indicating the best performing optimiser according to the out-of-sample criterion function. Panel A, B and C shows the average out-of-sample scores of the true DGP – stochastic volatility inversion copula model with shape parameter specified as {0, -3, -5} respectively.

### 3.2.2 Trace plots & predictive densities

Accurate prediction of extreme values is important for financial risk management. Therefore, we provide score trace plots to exhibit the comparison of LS, CRPS and CLS’s performance in predicting the 10 percentile and 90 percentile in Figure 1 and 2 so that we can evaluate their performance in predicting tails. The trace plots demonstrate the cumulative average out-of-sample score over an expanding evaluation period of CLS10 (left panel) and CLS90 (right panel) for four optimizers \{LS, CRPS, CLS10(80), CLS20(90)\}. The results in Section 3.2.1 show that when the degree of model misspecification is high, CLS can perform the best compared with LS and CRPS in predicting both tails. However, only the average score values at the end of each iteration of the simulation are recorded in the tables, while trace plots provide a dynamic view of how optimal forecasts perform. 

In the left panel of each trace plot, it shows the performance of each score evaluated by CLS 10%. That is, if the average score value of score A used for estimation is higher than score B, then score A predicts the 10 percentile of the distribution more accurately than score B. The right panel shows the same thing but is based on the performance of predicting 90 percentile. 

Figure 1 shows the correct specification case, corresponding to Scenario (i) and Table 2, it is impossible to distinguish these 4 lines in both lower and upper tails, which suggests that the cumulative average out-of-sample scores of these four optimisers are almost identical. It indicates that their out-of-sample performance are almost the same over the expanding evaluation period. Figure 2 corresponds to Scenario (ii) and Table 3 where the ARCH(1) model is misspecified for the GARCH(1,1) model with Student-t error. In panels (a) and (b), the least misspecified case, we can see that CLS almost makes no difference compared with other scoring rules, but as the misspecification increases, in panels (c) to (f), CLS starts to outperform CRPS and LS in predicting tails. Especially in panels (e) and (f), the gap between CLS10/90 (the blue line) and LS (the green line) is easily observed, which indicates that the more misspecified the model is, the more benefits can we gain from using CLS. Similar findings can be observed in Figure 3, which shows another type of model misspecification as in Scenario (iii). The increasing gap between CLS90 and LS, in panels (c) to (f), suggests that CLS makes more improvement in terms of the prediction accuracy in tails as the degree of model misspecification increases. Furthermore, the gap between CLS10/90 and LS tends to be bigger in the upper tail, compared with in the lower tail, especially in Figure 3. It suggests that more improvement is made in predictive accuracy occurs in the upper tail than in the lower tail. This result is consistent with @fbp2020. It is beneficial for econometricians to improve forecast accuracy without working so hard on finding a ‘correct’ model when in practice, a correct model does not exist.

```{r,fig.align="center"}
# need to adjust the size. 
knitr::include_graphics("figure/figure1.pdf")
```

Figure 1: Cumulative average out-of-sample scores of CLS10 (left panel) and CLS90 (right panel) over an expanding evaluation period under the correctly specified ARCH(1) model with Gaussian errors, corresponding to Scenario (i) in Table 1. 
```{r}
knitr::include_graphics("figure/figure 2.pdf")
```

Figure 2: Cumulative average out-of-sample scores of CLS10 (left panel) and CLS90 (right panel) over an expanding evaluation period under the misspecified ARCH(1) model with Gaussian errors, corresponding to Scenario (ii) in Table 1. The two subplots in each row represents the cumulative average out-of-sample scores of CLS10 (left panel) and CLS90 (right panel) of the true DGP – GARCH(1,1) with $t_{\nu=35}$, $t_{\nu=12}$, $t_{\nu=3}$ errors. 

```{r}
knitr::include_graphics("figure/figure 3.pdf")
```

Figure 3: Cumulative average out-of-sample scores of CLS10 (left panel) and CLS90 (right panel) over an expanding evaluation period under the misspecified ARCH(1) model with Gaussian errors, corresponding to Scenario (iii) in Table 1. The two subplots in each row represents the cumulative average out-of-sample scores of CLS10 (left panel) and CLS90 (right panel) of the true DGP – stochastic volatility inversion copula model with shape parameter specified as {0, -3, -5} respectively.

From the average out-of-sample score tables and trace plots above, we can see that optimal forecasts provide the most benefits in improving prediction accuracy under the most misspecified case. In order to provide a more complete view of how optimal forecasts work, the predictive densities of the most misspecified case are plotted in Figure 4. The shape of true conditional predictive distributions plays a role in deciding how much we can gain by using optimal forecasts, since predictions obtained from optimizing CLS aim to match the shape of tails of the true conditional predictive distributions. 

In the left panel of Figure 4, CLS90 (the red line) matches the shape of the upper tail of the true distribution (the pink line) perfectly but did a very poor job in predicting the lower tail. It is the same for CLS10 (the blue line) to match the shape of the lower tail of the true distribution perfectly. This visualizes how CLS reward the prediction accuracy in the region of interest to the forecaster. While in the right panel of Figure 4, it shows another type of model misspecification, and CLS does not manage to match the shape of the tails of the true distribution, but it still does a better job than LS in both tails. This suggests that when it comes to determining when the optimal probabilistic forecasts work, it actually depends on the shape of the true distribution, the assumed model specification, and the scoring rules. However, in practice, the shape of true conditional predictive distributions is usually unknown and it might create the difficulty of determining which tail we can reap more benefits from optimal forecasts without analysing a particular problem beforehand.

```{r,fig.align="center"}
knitr::include_graphics("figure/figure_pred.pdf")
```

Figure 4: The conditional predictive density produced by LS, CLS10 and CLS90 under the misspecified ARCH(1) model with Gaussian errors, corresponding to Panel C in Tables 3 and 4. The left panel is the conditional predictive density at a randomly picked time $t = 2001$ with the true DGP as stochastic volatility inversion copula (shape = -3), and the right panel is the conditional predictive density at a randomly picked time $t = 2501$ with the true DGP as GARCH(1,1) with $t_{\nu = 3}$ errors. 

# 4. Empirical analysis: financial returns 
## 4.1 Overview and preliminary diagnostics 

From the results in Section 3.2, we can conclude that optimal forecasts based on the CLS can improve the prediction accuracy in the tails and its effect is clearest when the degree of model misspecification is high and does not vary much when we have different types of model misspecification. In Section 4, we apply the same method adopted in Section 3 to an empirical setting and investigate how it performs in practice. 

The data used in the analysis is the continuously compounded daily log returns of S&P 500 listed in U.S.A financial market over a broad time period from 27 September,1996 to 30 July,2020.^[Data retrieved from @spdata] It aims to include two very volatile periods, the GFC and the recent COVID-19 pandemic. The initial training sample size and out-of-sample observations are kept consistent with what is in Section 3 with an initial training sample size of 1000 and out-of-sample evaluations of 5000.  

The descriptive statistics in Table 5 show that the distribution of S&P 500 log returns is negatively skewed, has fat tails and strong serial correlation in its volatility. Moreover, during the GFC and COVID-19 pandemic periods, the volatility is very high due to high uncertainty.^[Descriptive statistics are produced by R packages: @dplyr, @tidyverse, @tsibble, @tsibbledata, @feasts, @normtest, @moments.]

```{r}
# data cleaning and import 
sp <- read.csv("data/^GSPC.csv")
sp <- sp %>% dplyr::select("Date","Adj.Close")

sp$log.returns <- c(NA,diff(log(sp$Adj.Close)))
logret <- sp[-c(1:188),c(1,3)] # delete the adjusted price col, only keep 6000 obs

# export the dataset to csv. including only trading days and log returns 
temp <- logret[,c(2)]
temp <- data.frame(temp)
write_csv(temp, "sp500log.csv")
```


```{r, results='hide'}
# descriptive statistics 
summary <- summary(logret$log.returns) # ----> need to be tabulated 
# range(logret$log.returns)
# some test results aginst normal dist h_null
# JB test
jb.norm.test(logret$log.returns)
# qchisq(.95, df=2)
# Ljung box test
Box.test((logret$log.returns)^2,lag = 2,type = "Lj",fitdf = 1)
# qchisq(.95, df=lags)

sk <- skewness(logret$log.returns)
kt <- kurtosis(logret$log.returns)
```

```{r}
sum <- data.frame(Stock="S&P500",Min=as.numeric(summary[1]),Median=as.numeric(summary[3]),Mean=as.numeric(summary[4]),Max=as.numeric(summary[5]),Skewness=sk,Kurtosis=kt,JB.Test=27066,LB.Test=1742.7)
sum %>% 
  kable("latex",booktabs = T) %>% 
  kable_styling("bordered",font_size = 10) %>% 
  add_header_above(c("Table 5: Descriptive statistics for S&P500 log returns"=9))
```

Table 5: Descriptive statistics for S&P 500 index log returns. The Min and Max are the smallest and largest value of the sample series, respectively. Skewness is a measure of the asymmetry and kurtosis is a measure of fat tails with the normal distribution having kurtosis equals to three.  ‘JB.Test’ is the test statistic for the Jarque-Bera test of normality, with a critical value of 5.99. ‘LB.Test’ is the test of serial correlation of the squared returns, with a critical value based on 2 lags as 5.99. 

```{r,message=FALSE,fig.width=10,fig.height=5}

# time series plot
logret$Date <- as.Date(logret$Date,format = "%Y-%m-%d")

p1 <- logret %>% ggplot(aes(x = Date, y = log.returns)) +
  geom_line()+theme_classic()+labs(x="Year",y="Daily log returns")+
  #ggtitle("Figure 9: S&P500 daily log returns")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_y_continuous(labels = scales::percent) + 
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.12,label="Global Financial Crisis",color="red",size=3)+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=-0.12,label="COVID-19",color="red",size=2)+ # the date COVID-19 declared as pandemic
  theme_clean()
  #+labs(caption = "Figure 9: the time series plot of S&P500 daily log returns")

# histogram plots
p2 <- logret %>% ggplot()+geom_histogram(aes(x=log.returns))+xlim(-0.13,0.13)+
  #ggtitle("Figure 10")+
  scale_x_continuous(labels = scales::percent)+theme_clean()
  #+labs(caption = "Figure 10: the distribution of S&P500 log returns is asymmetric")

# ACF plots to see the autocorrelation 
logret <- logret %>% 
  mutate(trading_day = row_number()) %>%
  as_tsibble(index = trading_day,regular = TRUE)

p3 <- logret %>% ACF((log.returns)^2) %>% autoplot() + 
  #ggtitle("Figure 11:Squared log returns autocorrelation")+
  xlab("number of lags")+ylab("ACF of squared returns")+theme_clean()
# +labs(caption = "Figure 11: the autocorrelation plot shows there is a strong serial correlation in volatility of S&P500")

figure <- ggarrange(p1,                                                 
          ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
          nrow = 2, 
          labels = "A"                                        
          ) 
annotate_figure(figure, 
                top = text_grob("Descriptive figures for S&P500 log returns",face = "bold"),
                fig.lab = "Figure 5",fig.lab.face = "bold"
                )
```

Figure 5: The descriptive figures for S&P 500 log returns. Panel A is the time series plot of S&P 500 log returns. High volatility is observed during the GFC and the recent COVID-19 pandemic. Panel B is the histogram which illustrates the estimated marginal distribution of log returns. Panel C represents the autocorrelation function (ACF) plot of the squared returns, which indicates strong autocorrelation in volatility of log returns. 

## 4.2 Empirical results: predictive distributions for returns 

From the wide-usage of ARCH models in modelling financial returns and volatility, and the strict coherence observed in Section 3.2.1, ARCH(1) should be a ‘compatible’, but sufficiently misspecified model to forecast S&P 500 series and to observe the difference made by adopting optimal forecasts. The results in Table 6 show that the CLS outperforms other alternative scores in both the lower and upper tails, which closely follows the results and conclusions drawn in the numerical analysis in Section 3. However, the difference between using CLS10(80) and CLS20(90) is very small. 

```{r}
sp_table <- read.csv("data/sp_table.csv")
sp_table <- rename(sp_table, "In-sample optimizers" = Row,"CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
sp_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

sp_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 6: Average scores for S&P 500 log returns"=7))
```

Table 6: Average out-of-sample scores for S&P log returns, under the misspecified Gaussian ARCH(1) model. The row names are the in-sample optimizers used in producing the probabilistic forecasts and the column names are the criterion functions used to evaluate the out-of-sample performance of the probabilistic forecasts produced by each optimizer. The bolded value is the column maximum indicating the best performing optimiser according to the out-of-sample criterion function.

Unlike in the simulation example, the score trace plots now show how different scoring rules perform at time points that correspond to some actual historical events. From Figure 6(a), the out-of-sample performance of these four optimizers are almost identical before the 2008 GFC. However, the gap between CLS10 (the blue line) and LS (the green line) becomes distinguishable after the GFC. It indicates that CLS10 outperforms LS and CRPS significantly in terms of the predictive accuracy of lower tails. The gap then becomes smaller and smaller over time until the very recent COVID-19 pandemic. This interesting phenomenon suggests that optimal probabilistic forecasts based on CLS can predict extreme events, such as the GFC, more accurately than other alternatives. Hence, CLS yield a much higher out-of-sample score which brings up the cumulative average out-of-sample score that we observed in Figure 6(a). While in Figure 6(b), the similar results can be observed after the GFC in the upper tail. If we compare the gap between CLS and LS in Figure 6(b) with the gap in Figure 6(a), it is also worth noticing that CLS does not appear to provide more benefits in the upper tail than in the lower tail, which is different from the simulation results.^[The figures are produced by R packages: @ggplot2, @ggthemes, @ggpubr.]

```{r,fig.width=10,fig.height=5}
# to compress all the calculated plotting average scores together in the same df 
# this is assessed by cls10 for prediction accuracy
trace_fsr10 <- read.csv("data/fsr10.csv")
trace_fsr10 <- trace_fsr10 %>% rename("CLS10"=FSR10)
trace_fsr20 <- read.csv("data/fsr20.csv")
trace_fsr20 <- trace_fsr20 %>% rename("CLS20"=FSR20)
trace_10ls <- read.csv("data/fsr10_ls.csv")
trace_10ls <- trace_10ls %>% rename("LS"=MLE)
trace_10crps <- read.csv("data/fsr10_crps.csv")

fsr10 <- data.frame(trace_fsr10,trace_fsr20,trace_10ls,trace_10crps)

# add back dates to the fsr10 df, initial step = 1000

fsr10 <- fsr10 %>% mutate(Date = logret$Date[1001:6000])

# plot scores --- lower tail 
fsr10$Date <- as.Date(fsr10$Date,format = "%Y-%m-%d")

color1 <- c("CLS10"="blue","CLS20"="red","LS"="green","CRPS"="black")


fsr10plot <- fsr10 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS10,color = "CLS10"),size=0.3)+
  geom_line(aes(y = CLS20,color = "CLS20"),size=0.3)+
  geom_line(aes(y = LS,color = "LS"),size=0.3)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.3)+
  labs(x="Year",y="Average Scores",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-lower tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color1)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=0.15,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=0.15,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  

# plot scores --- upper tail 
trace_fsr90 <- read.csv("data/fsr90.csv")
trace_fsr90 <- trace_fsr90 %>% rename("CLS90"=FSR90)
trace_fsr80 <- read.csv("data/fsr80.csv")
trace_fsr80 <- trace_fsr80 %>% rename("CLS80"=FSR80)
trace_90ls <- read.csv("data/fsr90_ls.csv")
trace_90ls <- trace_90ls %>% rename("LS"=MLE)
trace_90crps <- read.csv("data/fsr90_crps.csv")

fsr90 <- data.frame(trace_fsr90,trace_fsr80,trace_90ls,trace_90crps)

# add back dates to the fsr10 df, initial step = 1000

fsr90 <- fsr90 %>% mutate(Date = logret$Date[1001:6000])

# plot scores --- lower tail 
fsr90$Date <- as.Date(fsr90$Date,format = "%Y-%m-%d")

color2 <- c("CLS90"="blue","CLS80"="red","LS"="green","CRPS"="black")


fsr90plot <- fsr90 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS90,color = "CLS90"),size=0.25)+
  geom_line(aes(y = CLS80,color = "CLS80"),size=0.25)+
  geom_line(aes(y = LS,color = "LS"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-upper tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color2)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.05,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=-0.05,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  
# put two plots side by side 
ggarrange(fsr10plot,fsr90plot,ncol=2,legend = "bottom",labels = c("Figure 6(a) CLS10","Figure 6(b) CLS90")) 
  #labs(caption = "Figure 6: the predictive distributions are produced by ARCH(1) with Gaussian error term") +
  #theme(plot.caption = element_text(hjust = 0))
```

Figure 6: Cumulative average out-of-sample scores of CLS10 (left panel) and CLS90 (right panel) over an expanding evaluation period, for predicting S&P 500 log returns under the misspecified Gaussian ARCH(1) model. Panel (a) shows the measure of performance in predicting the lower tail by optimizing CLS10, CLS20, CRPS and LS. Panel (b) shows the measure of performance in predicting the upper tail by optimizing CLS80, CLS90, CRPS and LS. 

## 4.3 Empirical results: prediction of Value-at-Risk
### 4.3.1 Overview and preliminaries

Density forecasts have great practical value in finance. They form the foundation of risk management, such as VaR, and are useful for asset allocation and derivative pricing. VaR is a commonly used risk measurement in finance. It can be obtained from a probability density function of returns over a chosen investment horizon. It is the $p$th quantile ($q_p$) of a predictive distribution of a portfolio’s returns over a holding period, according to a given confidence interval $\alpha$ [@dowd2007]. That is, 
\begin{equation} \label{eq:var}
VaR = -q_p
\end{equation}
where 
\begin{equation} \label{eq:p}
p=1-\alpha
\end{equation}
VaR represents the largest possible investment loss in a given investment horizon with the given $\alpha$ confidence level. There are different methods to estimate VaR, but it is clear that the predictive density plays an important role here. Therefore, a more accurate probabilistic prediction can contribute to improving the accuracy of VaR prediction. 

@opschoor2017 combine density forecasts using focused scoring rules, and the results show weighted density forecasts based on optimizing the CLS outperform those based on optimizing CRPS or LS and improve the accuracy of 99% Value-at-Risk estimates. 

We illustrate how optimal forecasts perform in estimating VaR by using predictions produced through simulation first, using the specifications from Section 3, and expect that optimal forecasts based on CLS would give us a better prediction in the tails. Specifically, VaR predictions at \{10%, 20%, 80%, 90%\} are conducted, corresponding to the 10% and 20% expected loss of long and short portfolios. More importantly, we keep the percentage of VaR prediction the same as CLS focused tail because, as mentioned before, CLS at \{10%, 20%, 80%, 90%\} is computed from the marginal distribution due to computational burden and keeping the percentage the same reduces the error caused by this mismatch. 

We assess VaR predictive accuracy by using the VaR backtesting method. The specific steps are as below: 

1. Using the sequence of predictive densities (p) produced by $S_i$ in Section 3:

- $S_i= \{LS, CRPS, CLS10, CLS20, CLS80, CLS90\}$;

- Conditional out-of-sample predictions: $p_{S_i}(y_{1001}|y_{1000})...p_{S_i}(y_{5000}|y_{4999})$;

2. Construct the VaR at \{10%, 20%, 80%, 90%\} using predictive densities from Step 1 for each scoring rule;

3. Compare the true values of $y_{1001}...y_{5000}$ with $VaR_t$ and calculate the proportion of exceedances ($y_t < VaR_t$); 

The closer the proportion of exceedances, calculated in Step 3, to nominal VaR levels, the more accurate are VaR predictions and thus, the more accurate are the optimal probabilistic forecasts. An accurate prediction of the $p$% VaR is observed if the proportion of exceedances equals $p$%. 

Starting from the two different forms of model misspecification investigated in the simulation analysis of Section 3, Table 7 and 8 illustrates the assessment of the predictive VaR corresponding to the most misspecified cases in Sceanrio (ii) and (iii) in Table 1. The values recorded in Table 7 and 8 refer to the proportion of times that the observed out-of-sample values exceed the predictive VaR. The bolded figure shows the out-of-sample proportion of exceedances that is the closest to the nominal tail probability indicated by the column name. In Table 7, CLS80 and CLS90 optimisers produce the best performing VaR predictions for the upper tails, while CRPS is the best performing optimiser in terms of predicting VaR at 10% and 20% but CLS10 and CLS20 are not short of advantages by too much compared with CRPS. In Table 8 which shows predictive VaR assessment under another type of model misspecification – modelling stochastic volatility inversion copula with Gaussian ARCH(1) model, it is obvious that CLS performs the best in predicting VaR in both lower and upper tails. Therefore, from the results of these two tables, CLS can improve the VaR predictive accuracy when the model is sufficiently misspecified, no matter what the form of model misspecification. 

```{r}
garch3var <- read.csv("data/garch3var.csv")

VaR0.1 <- as.numeric(garch3var[,c(1,5,9,13,17,21)])
VaR0.2 <- as.numeric(garch3var[,c(2,6,10,14,18,22)])
VaR0.8 <- as.numeric(garch3var[,c(3,7,11,15,19,23)])
VaR0.9 <- as.numeric(garch3var[,c(4,8,12,16,20,24)])

Optimizers <- c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

df <- data.frame(Optimizers,VaR0.1,VaR0.2,VaR0.8,VaR0.9)
df <- rename(df,"VaR at 10\\%"=VaR0.1, "VaR at 20\\%"=VaR0.2, "VaR at 80\\%"=VaR0.8, "VaR at 90\\%"=VaR0.9)

df %>% 
  mutate(
    `VaR at 10\\%` = cell_spec(format(`VaR at 10\\%`,4),"latex",bold = ifelse(abs(`VaR at 10\\%`-0.1) == min(abs(`VaR at 10\\%`-0.1)),TRUE,FALSE)),
    `VaR at 20\\%` = cell_spec(format(`VaR at 20\\%`,4),"latex",bold = ifelse(abs(`VaR at 20\\%`-0.2) == min(abs(`VaR at 20\\%`-0.2)),TRUE,FALSE)),
    `VaR at 80\\%` = cell_spec(format(`VaR at 80\\%`,4),"latex",bold = ifelse(abs(`VaR at 80\\%`-0.8) == min(abs(`VaR at 80\\%`-0.8)),TRUE,FALSE)),
    `VaR at 90\\%` = cell_spec(format(`VaR at 90\\%`,4),"latex",bold = ifelse(abs(`VaR at 90\\%`-0.9) == min(abs(`VaR at 90\\%`-0.9)),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs=T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  add_header_above(c(" ","Out-of-sample exceedances"=4)) %>%
  add_header_above(c("Table 7: the true DGP is GARCH with degree of freedom = 3"=5))
```

Table 7: Predictive VaR assessment corresponding to the most misspecified case in Scenario (ii) in Table 1, where the assumed predictive model is Gaussian ARCH(1) and the true DGP is GARCH(1,1) with $t_{\nu} = 3$ error. The figures are the proportion of times that the observed out-of-sample values exceed the predictive $VaR_{\alpha}$ indicated by the column name. The bolded figure indicates the empirical out-of-sample exceedance that is the closest to the nominal tail probability. 

```{r}
copula5var <- read.csv("data/copula5var.csv")

VaR_0.1 <- as.numeric(copula5var[,c(1,5,9,13,17,21)])
VaR_0.2 <- as.numeric(copula5var[,c(2,6,10,14,18,22)])
VaR_0.8 <- as.numeric(copula5var[,c(3,7,11,15,19,23)])
VaR_0.9 <- as.numeric(copula5var[,c(4,8,12,16,20,24)])

Optimizers <- c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

df <- data.frame(Optimizers,VaR_0.1,VaR_0.2,VaR_0.8,VaR_0.9)
df <- rename(df,"VaR at 10\\%"=VaR_0.1, "VaR at 20\\%"=VaR_0.2, "VaR at 80\\%"=VaR_0.8, "VaR at 90\\%"=VaR_0.9)

df %>% 
  mutate(
    `VaR at 10\\%` = cell_spec(format(`VaR at 10\\%`,4),"latex",bold = ifelse(abs(`VaR at 10\\%`-0.1) == min(abs(`VaR at 10\\%`-0.1)),TRUE,FALSE)),
    `VaR at 20\\%` = cell_spec(format(`VaR at 20\\%`,4),"latex",bold = ifelse(abs(`VaR at 20\\%`-0.2) == min(abs(`VaR at 20\\%`-0.2)),TRUE,FALSE)),
    `VaR at 80\\%` = cell_spec(format(`VaR at 80\\%`,4),"latex",bold = ifelse(abs(`VaR at 80\\%`-0.8) == min(abs(`VaR at 80\\%`-0.8)),TRUE,FALSE)),
    `VaR at 90\\%` = cell_spec(format(`VaR at 90\\%`,4),"latex",bold = ifelse(abs(`VaR at 90\\%`-0.9) == min(abs(`VaR at 90\\%`-0.9)),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F,booktabs=T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  add_header_above(c(" ","Out-of-sample exceedances"=4)) %>%
  add_header_above(c("Table 8: the true DGP is stochastic volatility inversion copula (shape = -5)"=5))
```

Table 8: Predictive VaR assessment corresponding to the most misspecified case in Scenario (iii) in Table 1, where the assumed predictive model is Gaussian ARCH(1) and the true DGP is the stochastic volatility inversion copula model with the highest negative marginal skewness (shape = -5). The figures are the proportion of times that the observed out-of-sample values exceed the predictive $VaR_{\alpha}$ indicated by the column name. The bolded figure indicates the empirical out-of-sample exceedance that is the closest to the nominal tail probability.

### 4.3.2 Empirical results: S&P 500

The preliminary results obtained from the VaR predictions using simulated data in Section 4.3.1 help us understand what to expect in empirical VaR analysis. We now implement the VaR analysis to continuously compounded daily log returns of S&P 500 in U.S.A financial market. Learning from the trace plots in Section 4.2, the 2008 GFC is the turning point of the optimal forecasts' performance, especially for the CLS. Therefore, we perform the VaR analysis over two different time periods, before the GFC and after the GFC. Table 9 and 10 show that the CLS performs well enough in terms of estimating VaR for both long and short portfolios of S&P 500, but also CRPS is a very robust scoring rule which can provide relatively accurate predictions. The results from Table 9 show the performance of optimizers in predicting VaR of S&P 500 log returns before the GFC. Compared with the results obtained in Table 7 and 8 under simulation scenarios, the answer to whether CLS significantly improve the VaR predictive accuracy is less obvious. Looking at the bolded values, CLS20 and CLS90 can provide the best VaR predictions for 10% and 80% tails, respectively. While CRPS is the optimiser that can provide the best VaR predictions for 20% and 90% among all optimisers. Even though CRPS outperforms CLS in these situations, there is only slight difference between CRPS and CLS. The predictive VaR assessment after the GFC is shown in Table 10 where the similar results can be observed. The bolded values indicate that CLS10 and CLS90 are the best performing optimizers in terms of predicting VaR at 20% and 80%, respectively, and CRPS still outperforms other optimizers in VaR prediction at 10% and 90%. Moreover, in Table 10, the CLS seems to perform better in 20% and 80% tails which have higher probability mass than 10% and 90% tails. If we think of the marginal distribution of S&P 500 estimated by histogram in Figure 5.B, the above results could be a consequence of the very sparse marginal distribution or simply because there is no enough data in the tails to produce an accurate estimate of the CLS optimizer, which increases the difficulty of 'focusing' on the 10% and 90% tails.

```{r}
spvar <- read.csv("data/spvar1_table.csv")

VaR_0.1 <- as.numeric(spvar[,c(1,5,9,13,17,21)])
VaR_0.2 <- as.numeric(spvar[,c(2,6,10,14,18,22)])
VaR_0.8 <- as.numeric(spvar[,c(3,7,11,15,19,23)])
VaR_0.9 <- as.numeric(spvar[,c(4,8,12,16,20,24)])

df <- data.frame(Optimizers,VaR_0.1,VaR_0.2,VaR_0.8,VaR_0.9)
df <- rename(df,"VaR at 10\\%"=VaR_0.1, "VaR at 20\\%"=VaR_0.2, "VaR at 80\\%"=VaR_0.8, "VaR at 90\\%"=VaR_0.9)

df %>% 
  mutate(
    `VaR at 10\\%` = cell_spec(format(`VaR at 10\\%`,4),"latex",bold = ifelse(abs(`VaR at 10\\%`-0.1) == min(abs(`VaR at 10\\%`-0.1)),TRUE,FALSE)),
    `VaR at 20\\%` = cell_spec(format(`VaR at 20\\%`,4),"latex",bold = ifelse(abs(`VaR at 20\\%`-0.2) == min(abs(`VaR at 20\\%`-0.2)),TRUE,FALSE)),
    `VaR at 80\\%` = cell_spec(format(`VaR at 80\\%`,4),"latex",bold = ifelse(abs(`VaR at 80\\%`-0.8) == min(abs(`VaR at 80\\%`-0.8)),TRUE,FALSE)),
    `VaR at 90\\%` = cell_spec(format(`VaR at 90\\%`,4),"latex",bold = ifelse(abs(`VaR at 90\\%`-0.9) == min(abs(`VaR at 90\\%`-0.9)),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs=T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  add_header_above(c(" ","Out-of-sample exceedances"=4)) %>%
  add_header_above(c("Table 9: VaR for S&P 500 before the GFC"=5))

```

Table 9: Predictive VaR assessment for S&P 500 log returns before the GFC, where the assumed predictive model is Gaussian ARCH(1). The figures are the proportion of times that the observed out-of-sample values exceed the predictive $VaR_{\alpha}$ indicated by the column name. The bolded figure indicates the empirical out-of-sample exceedance that is the closest to the nominal tail probability.

```{r}
spvar <- read.csv("data/spvar2_table.csv")
spvar <- round(spvar,digits = 4)

VaR_0.1 <- as.numeric(spvar[,c(1,5,9,13,17,21)])
VaR_0.2 <- as.numeric(spvar[,c(2,6,10,14,18,22)])
VaR_0.8 <- as.numeric(spvar[,c(3,7,11,15,19,23)])
VaR_0.9 <- as.numeric(spvar[,c(4,8,12,16,20,24)])

df <- data.frame(Optimizers,VaR_0.1,VaR_0.2,VaR_0.8,VaR_0.9)
df <- rename(df,"VaR at 10\\%"=VaR_0.1, "VaR at 20\\%"=VaR_0.2, "VaR at 80\\%"=VaR_0.8, "VaR at 90\\%"=VaR_0.9)

df %>% 
  mutate(
    `VaR at 10\\%` = cell_spec(format(`VaR at 10\\%`,4),"latex",bold = ifelse(abs(`VaR at 10\\%`-0.1) == min(abs(`VaR at 10\\%`-0.1)),TRUE,FALSE)),
    `VaR at 20\\%` = cell_spec(format(`VaR at 20\\%`,4),"latex",bold = ifelse(abs(`VaR at 20\\%`-0.2) == min(abs(`VaR at 20\\%`-0.2)),TRUE,FALSE)),
    `VaR at 80\\%` = cell_spec(format(`VaR at 80\\%`,4),"latex",bold = ifelse(abs(`VaR at 80\\%`-0.8) == min(abs(`VaR at 80\\%`-0.8)),TRUE,FALSE)),
    `VaR at 90\\%` = cell_spec(format(`VaR at 90\\%`,4),"latex",bold = ifelse(abs(`VaR at 90\\%`-0.9) == min(abs(`VaR at 90\\%`-0.9)),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs=T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  add_header_above(c(" ","Out-of-sample exceedances"=4)) %>%
  add_header_above(c("Table 10: VaR for S&P 500 after the GFC"=5))

# formattable(df,list(VaR_0.1 = max_bold1,VaR_0.2 = max_bold2,VaR_0.8 = max_bold8,VaR_0.9 = max_bold9))
```

Table 10: Predictive VaR assessment for S&P 500 log returns after the GFC, where the assumed predictive model is Gaussian ARCH(1). The figures are the proportion of times that the observed out-of-sample values exceed the predictive $VaR_{\alpha}$ indicated by the column name. The bolded figure indicates the empirical out-of-sample exceedance that is the closest to the nominal tail probability.

# 5. Empirical analysis: VIX
## 5.1 Background and notation 

Time-varying volatility is an important part in finance modelling and an accurate prediction will give investors a better understanding of the risks they take on. GARCH-type models used in Section 3 and 4, which are defined as:
\begin{equation} \label{eq:reg}
r_t = \mu+\sigma_t\epsilon_t
\end{equation}
and 
\begin{equation} \label{eq:garch}
\sigma_t^2=\alpha_0+\alpha_1(r_{t-1}-\mu)^2+\beta_1\sigma_{t-1}^2
\end{equation}
where 
\begin{equation} \label{eq:ret}
r_t=ln(\frac{P_t}{P_{t-1}})
\end{equation}
with $P_t$ the stock price and $\sigma_t$ the volatility, are conditionally deterministic. That is, $\sigma_t^2$ is a deterministic function of given past returns. 

GARCH models also neglect the fact that volatility in stock markets and derivative markets can have long memory, which means it slowly reverts to its long-run mean. They record changes usually in daily frequency and do not exploit the information in intraday data. Therefore, it is better off if we model volatility in a continuous-time model where $lnP_t$ and $\sigma_t$ are allowed to vary continuously over time. 

The realized volatility (RV) approach to modelling volatility allows us to exploit the information in intraday data, consider long memory feature, sudden jumps in the market and market microstructure. When the price follows the process as in:
\begin{equation} \label{eq:gbm}
d\ln(P_t)=\mu_tdt+\sigma_tdw_t
\end{equation}
RV is a direct estimate of integrated volatility (IV) for $lnP_t$ based on continuously recorded observations of $P_t$ over the day, and is defined as:
\begin{equation} \label{eq:RV}
RV_t = \sum^{1/\Delta t}_{j=1}r^2_{t-1+j\Delta t} \xrightarrow{p} \int_{t-1}^t\sigma_s^2ds = IV_t \ as\ \Delta t \to 0
\end{equation}
When considering the sudden jumps, that is, the price follows the process in:  
\begin{equation} \label{eq:jump}
d\ln(P_t)=\mu_tdt+\sigma_tdw_t+\kappa_tdq_t
\end{equation}
RV is also a consistent estimate of quadratic variation (QV) where $\kappa_s$ represents the sudden jump on day t and $q_t$ is the jump occasions, which is defined as: 
\begin{equation} \label{eq:RV2}
RV_t=\sum^{1/\Delta t}_{j=1}r^2_{t-1+j\Delta t} \xrightarrow{p} \int_{t-1}^t\sigma_s^2ds\ + \sum_{s=1}^{q_t}\kappa_s^2= QV_t=IV_t+\sum_{s=1}^{q_t}\kappa_s^2 \ as\ \Delta t \to 0
\end{equation}
In addition to the fact that volatility is a direct measure of the risk of portfolios, it is also important for pricing derivatives, such as options. Thus, option-implied volatility has been used for measuring volatility. The option-implied volatility using the famous Black-Scholes (BS) model is less accurate since the BS model misspecifies the volatility as constant over time. Therefore, the model-free option-implied volatility has been introduced to better reflect the market and to construct the VIX, which allows for stochastic volatility with jumps. More specifically, the VIX index estimates QV using the prices of options written on the S&P 500 index, and under a risk-neutral version of the process in $\eqref{eq:jump}$ [@cboe]. It thus acknowledges the occurrence of jumps and the fact that assumptions of the famous Black-Scholes option pricing model, i.e. that returns on the index are normal with a constant variance, do not hold in practice. Since both the VIX and RV are estimations of QV, it is reasonable to model the VIX with a HAR-RV model, which is usually used for modelling RV in the literature [@andersen2007; @corsi2009; @m2009does; @manee2012probabilistic].

## 5.2 Preliminary diagnostics
We collect daily VIX index data starting from 27 August,1996 to 30 July,2020.^[Data retrieved from @vixdata] The descriptive statistics of $log(VIX_t)$ are shown in Table 11 and Figure 7. 

Table 11 and Figure 7 show that $log(VIX_t)$ is volatile and shows strong persistence over time. It is positively skewed and the autocorrelation of its volatility is slowly decaying. From the descriptive statistics, we can see that the features of the VIX is similar to what we have observed from S&P 500 log returns. Similarly, there are unusual jumps happened during the GFC and the COVID-19. Thus, we expect that the CLS could outperform MLE and CRPS in predicting tails as in Section 4.2 and significantly improve accuracy after the GFC. 

```{r,message=F}
# data cleaning and import 
vix <- read.csv("data/^VIX.csv")
vix <- vix %>% dplyr::select("Date","Adj.Close")

vix$logvix <- log(vix$Adj.Close)

# previous day log vix values
prev <- matrix(0,nrow = nrow(vix)+1,ncol = 1)
for (i in 1:nrow(vix)){
  prev[1,1] <- c("NA")
  prev[i+1,1] <- vix$logvix[i]
}
prev <- as.data.frame(prev)
prev <- prev[-c(1:188,6189),1]

# need to be very careful about the time index -- dependent variable starting with t+1
MA5 <- matrix(0,nrow=nrow(vix)-5,ncol = 1)
for (i in 1:(nrow(vix)-5)){
  sum = 0
  for (j in 1:5){
    sum = sum + vix$logvix[i+j-1]
  }
  MA5[i,1] = (1/5)*sum
}

MA22 <- matrix(0,nrow = nrow(vix)-22,ncol = 1)
for (i in 1:(nrow(vix)-22)){
  sum = 0
  for (j in 1:22){
    sum = sum + vix$logvix[i+j-1]
  }
  MA22[i,1] = (1/22)*sum
}


#require(zoo)
#MA5 <- rollmean(vix$logvix,5,align = "center")
MA5 <- as.data.frame(MA5) # average of past week
MA5 <- MA5[-c(1:183),1] # including only 6000 obs 

#MA22 <- rollmean(vix$logvix,22,align = "center")
MA22 <- as.data.frame(MA22) # average of past month
MA22 <- MA22[-c(1:166),1] # including only 6000 obs

logvix <- vix[-c(1:188),c(1,3)] # delete the adjusted vix col, only keep 6000 obs
logvix <- cbind(logvix,prev,MA5,MA22)
# export the dataset to csv. including only trading days and log returns

temp <- logvix[,2:5]
write_csv(temp,"vixlog.csv")

# the logvix, previous log vix and moving average values have the same length in the final file. 
```


```{r, results='hide'}
# descriptive statistics 
summary <- summary(vix$logvix) # ----> need to be tabulated 
# range(vix$logvix)
# some test results aginst normal dist h_null
# JB test
jb.norm.test(vix$logvix)
# Ljung box test
Box.test((vix$logvix)^2,lag = 2,type = "Lj",fitdf = 1)

vixsk <- skewness(vix$logvix)
vixkt <- kurtosis(vix$logvix)
```


```{r}
sum <- data.frame(Series="Log(VIX)",Min = as.numeric(summary[1]), Median = as.numeric(summary[3]), Mean = as.numeric(summary[4]), Max = as.numeric(summary[6]),Skewness= vixsk,Kurtosis=vixkt,JB.Test=410.77,LB.Test=11763)
sum %>% 
  kable("latex",booktabs = T) %>% 
  kable_styling("bordered",font_size = 10)%>% 
  add_header_above(c("Table 11: Descriptive statistics for log(VIX)"=9))
```

Table 11: Descriptive statistics for logarithmic of the VIX volatility index. The Min and Max are the smallest and largest value of the sample series, respectively. Skewness is a measure of the asymmetry and kurtosis is a measure of fat tails with the normal distribution having kurtosis equals to three.  ‘JB.Test’ is the test statistic for the Jarque-Bera test of normality, with a critical value of 5.99. ‘LB.Test’ is the test of serial correlation of the squared volatility, with a critical value based on 2 lags as 5.99. 

```{r,message=FALSE,fig.width=10,fig.height=5}

# time series plot
logvix$Date <- as.Date(logvix$Date,format = "%Y-%m-%d")

p1 <- logvix %>% ggplot(aes(x = Date, y = logvix)) +
  geom_line()+theme_classic()+labs(x="Year",y="Daily log(VIX) values")+
  #ggtitle("Figure ?: Daily values for Log(VIX)")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  #scale_y_continuous(labels = scales::percent) + 
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=2,label="Global Financial Crisis",color="red",size=3)+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=2,label="COVID-19",color="red",size=3)+
  theme_clean()

# histogram plots
p2 <- logvix %>% ggplot()+geom_histogram(aes(x=logvix))+
  theme_clean()

# ACF plots to see the autocorrelation 
logvix <- logvix %>% 
  mutate(trading_day = row_number()) %>%
  as_tsibble(index = trading_day,regular = TRUE)

p3 <- logvix %>% ACF(logvix) %>% autoplot()+
      xlab("number of lags")+ylab("ACF of log(VIX)")+theme_clean()

#p4 <- logvix %>% ACF(logvix) %>% autoplot()+
  #xlab("number of lags")+ylab("ACF of log(VIX)")+theme_clean()

figure <- ggarrange(p1,                                                
          ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
          nrow = 2, 
          labels = "A"                                        
          ) 
annotate_figure(figure, 
                top = text_grob("Descriptive figures for log(VIX)",face = "bold"),
                fig.lab = "Figure 7",fig.lab.face = "bold"
                )
```

Figure 7: The descriptive figures for logarithmic of the VIX volatility index. Panel A is the time series plot of log(VIX). Panel B is the histogram which illustrates the estimated marginal distribution of log(VIX). Panel C represents the ACF plot of the squared log(VIX), which indicates strong autocorrelation in volatility of the VIX volatility index. 

## 5.3 Model specification

Proposed by @corsi2009, the HAR-RV model is a simple additive linear model which takes lagged squared returns as regressors. It does not belong to the class of long memory models, but it is able to produce the volatility persistence that is almost indistinguishable from what is observed in financial markets, through the simple autoregressive-type structure.  

We design the VIX analysis with different error term specifications of the HAR-RV model: 

1. $z_{t+1} \sim N(0,1)$ and constant $\sigma$.

2. $z_{t+1} \sim Student-t(0,1,\nu)$ and constant $\sigma$. ^[We specify $z_{t+1}$ as standardised Student-t distribution so that $\sigma^2$ can reflect the variance of the predicted VIX series. The CRPS is removed in Specification 2 and 4, since there is no closed form for this score. @crpst2016 provides a way to evaluate CRPS for Student-t distribution numerically.]

3. $z_{t+1} \sim N(0,1)$ and time-varying $\sigma_{t+1}$ following ARCH(1) process.

4. $z_{t+1} \sim Student-t(0,1,\nu)$ and time-varying $\sigma_{t+1}$ following ARCH(1) process.
^[Specification 1 and 2 are based on equations $\eqref{eq:HAR}$-$\eqref{eq:ma22}$; Specification 3 and 4 are based on equations $\eqref{eq:ma22}$-$\eqref{eq:arch}$]

The HAR-RV itself is defined as: 

\begin{equation} \label{eq:HAR}
log(VIX_{t+1})=\beta_0+\beta_1log(VIX_t)+\beta_2log(VIX_{t-5,t})+\beta_3log(VIX_{t-22,t})+\sigma z_{t+1}
\end{equation}
where 
\begin{equation} \label{eq:ma5}
log(VIX_{t-5,t})=\frac{1}{5}[log(VIX_t)+...+log(VIX_{t-4})]
\end{equation}
\begin{equation} \label{eq:ma22}
log(VIX_{t-22,t})=\frac{1}{22}[log(VIX_t)+...+log(VIX_{t-21})]
\end{equation}
or in Specification 3 and 4, it is 
\begin{equation} \label{eq:HAR2}
log(VIX_{t+1})=\beta_0+\beta_1log(VIX_t)+\beta_2log(VIX_{t-5,t})+\beta_3log(VIX_{t-22,t})+\sigma_{t+1}z_{t+1}
\end{equation}
\begin{equation} \label{eq:arch}
\sigma_{t+1}^2=\alpha_0+\alpha_1 (\sigma_t z_t)^2
\end{equation}
where $log(VIX_{t-5,t})$ and $log(VIX_{t-22,t})$ are defined as in $\eqref{eq:ma5}$ and $\eqref{eq:ma22}$ respectively. 
By using the same expanding window design in Section 3 with initial sample size = 1000 and out-of-sample size $\tau=T-1000=5000$, we produce and evaluate predictive densities with the assumed model as HAR-RV. The results are shown in the following sections. 

## 5.4 Empirical results 
### 5.4.1 Results of specification 1

Section 5.4.1 provides the result of Specification 1 where the conditional predictive density of the VIX is the normal distribution with a conditional mean produced by the HAR-RV model and a constant variance. In Table 12, CLS produces a better prediction for the upper tail as expected and does a relatively good job for the lower tail as well, even though CRPS is the optimizer that produce the best predictive for the 20th percentile of the true distribution. The score trace plot, Figure 8, shows the predictions of optimizing four scoring rules evaluated by CLS10 (left panel) and CLS90 (right panel). It indicates how accurate these predictions are, according to their performance in 10 percentile and 90 percentile. In the lower tail, it seems that there is not much difference among four predictions, but zooming in, we can see that CLS is at the top of other scores, which means it can still provide the most accurate prediction. However, we only gain very little from using it. On the contrary, in the upper tail, CLS has a much bigger effect in improving accuracy after the GFC. One possible explanation could be that the HAR model is more misspecified in the upper tail than in the lower tail. Thus, we benefit more from CLS optimal forecasts in the upper tail. 

```{r}
vix_table <- read.csv("data/vix_table.csv")
vix_table <- rename(vix_table, "In-sample optimizers" = Row,"CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
vix_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

vix_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 12: Average scores for the VIX with Gaussian error"=7))
```

Table 12: Average out-of-sample scores under model specification 1 where the HAR-RV model has a constant $\sigma$ and normally distributed $z_{t+1}$. The row names are the in-sample optimizers used in producing the probabilistic forecasts and the column names are the criterion functions used to evaluate the out-of-sample performance of the probabilistic forecasts produced by each optimizer. The bolded value is the column maximum indicating the best performing optimiser according to the out-of-sample criterion function. 

```{r,fig.width=10,fig.height=5}
# to compress all the calculated plotting average scores together in the same df 
# this the accuracy assessed by fsr10
trace_fsr10 <- read.csv("data/vixfsr10.csv")
trace_fsr10 <- trace_fsr10 %>% rename("CLS10"=FSR10)
trace_fsr20 <- read.csv("data/vixfsr20.csv")
trace_fsr20 <- trace_fsr20 %>% rename("CLS20"=FSR20)
trace_10ls <- read.csv("data/vixfsr10_ls.csv")
trace_10crps <- read.csv("data/vixfsr10_crps.csv")

fsr10 <- data.frame(trace_fsr10,trace_fsr20,trace_10ls,trace_10crps)

# add back dates to the fsr10 df, initial step = 1000

fsr10 <- fsr10 %>% mutate(Date = logvix$Date[1001:6000])

# plot scores --- lower tail 
fsr10$Date <- as.Date(fsr10$Date,format = "%Y-%m-%d")

color1 <- c("CLS10"="blue","CLS20"="red","MLE"="green","CRPS"="black")

fsr10plot <- fsr10 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS10,color = "CLS10"),size=0.25)+
  geom_line(aes(y = CLS20,color = "CLS20"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="Average Scores",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-lower tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color1)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=0.08,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=0.08,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  

# plot scores --- upper tail (assessed by fsr90)
trace_fsr90 <- read.csv("data/vixfsr90.csv")
trace_fsr90 <- trace_fsr90 %>% rename("CLS90"=FSR90)
trace_fsr80 <- read.csv("data/vixfsr80.csv")
trace_fsr80 <- trace_fsr80 %>% rename("CLS80"=FSR80)
trace_90ls <- read.csv("data/vixfsr90_ls.csv")
trace_90crps <- read.csv("data/vixfsr90_crps.csv")

fsr90 <- data.frame(trace_fsr90,trace_fsr80,trace_90ls,trace_90crps)

# add back dates to the fsr10 df, initial step = 1000

fsr90 <- fsr90 %>% mutate(Date = logvix$Date[1001:6000])

# plot scores --- lower tail 
fsr90$Date <- as.Date(fsr90$Date,format = "%Y-%m-%d")

color2 <- c("CLS90"="blue","CLS80"="red","MLE"="green","CRPS"="black")

fsr90plot <- fsr90 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS90,color = "CLS90"),size=0.25)+
  geom_line(aes(y = CLS80,color = "CLS80"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-upper tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color2)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.05,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=-0.05,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  
# put two plots side by side 
ggarrange(fsr10plot,fsr90plot,ncol=2,legend = "bottom",labels = c("Figure 8(a) CLS10","Figure 8(b) CLS90"))
  #labs(caption = "Figure 8: the predictive distributions are produced by HAR-RV model with Gaussian error and constant volatility") +
  #theme(plot.caption = element_text(hjust = 0))

```

Figure 8: Cumulative average out-of-sample scores of CLS10 (left panel) and CLS90 (right panel) over an expanding evaluation period, for predicting log(VIX) under the misspecified HAR-RV model with constant $\sigma$ and normally distributed $z_{t+1}$. Panel (a) shows the measure of performance in predicting the lower tail by optimizing CLS10, CLS20, CRPS and LS. Panel (b) shows the measure of performance in predicting the upper tail by optimizing CLS80, CLS90, CRPS and LS. 

### 5.4.2 Results of specification 2

The numerical results in Section 3 show that the more misspecified the model is, the more benefits are gained in improving prediction accuracy. According to the marginal distribution estimated by histogram in Figure 7.B, the conditional prediction distribution of the VIX is likely to be positively skewed and has fatter tails than the normal distribution. Moving from Specification 1 to Specification 2, fatter tails are allowed in the predictive distribution and thus, the assumed model is less misspecified. Table 13 and Figure 9 show consistent performance of optimal forecasts compared with our investigations of simulated data and S&P 500 returns. That is, improvement of prediction accuracy by using CLS is still observed in the upper tail but the amount of improvement decreases as the degree of misspecification decreases. 

```{r}
vix_table <- read.csv("data/vix_table_st.csv")
vix_table <- rename(vix_table, "In-sample optimizers" = Row,"CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
vix_table$`In-sample optimizers`<-c("LS","CLS10","CLS20","CLS80","CLS90")

vix_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    #CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=5)) %>% 
  add_header_above(c("Table 13: Average scores for the VIX with Student-t error"=6))
```

Table 13: Average out-of-sample scores under model specification 2 where the HAR-RV model has a constant $\sigma$ and $z_{t+1}$ is from a standardized Student-t distribution. The row names are the in-sample optimizers used in producing the probabilistic forecasts and the column names are the criterion functions used to evaluate the out-of-sample performance of the probabilistic forecasts produced by each optimizer. The bolded value is the column maximum indicating the best performing optimiser according to the out-of-sample criterion function. 

```{r,fig.width=10,fig.height=5}
# to compress all the calculated plotting average scores together in the same df 
trace_fsr10 <- read.csv("data/vixfsr10_st.csv")
trace_fsr10 <- trace_fsr10 %>% rename("CLS10"=FSR10)
trace_fsr20 <- read.csv("data/vixfsr20_st.csv")
trace_fsr20 <- trace_fsr20 %>% rename("CLS20"=FSR20)
trace_10ls <- read.csv("data/vixfsr10_ls_st.csv")
#trace_10crps <- read.csv("vixfsr10_crps.csv")

fsr10 <- data.frame(trace_fsr10,trace_fsr20,trace_10ls)

# add back dates to the fsr10 df, initial step = 1000

fsr10 <- fsr10 %>% mutate(Date = logvix$Date[1001:6000])

# plot scores --- lower tail 
fsr10$Date <- as.Date(fsr10$Date,format = "%Y-%m-%d")

color1 <- c("CLS10"="blue","CLS20"="red","MLE"="green")

fsr10plot <- fsr10 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS10,color = "CLS10"),size=0.25)+
  geom_line(aes(y = CLS20,color = "CLS20"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
 # geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="Average Scores",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-lower tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color1)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=0.08,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=0.08,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  

# plot scores --- upper tail 
trace_fsr90 <- read.csv("data/vixfsr90_st.csv")
trace_fsr90 <- trace_fsr90 %>% rename("CLS90"=FSR90)
trace_fsr80 <- read.csv("data/vixfsr80_st.csv")
trace_fsr80 <- trace_fsr80 %>% rename("CLS80"=FSR80)
trace_90ls <- read.csv("data/vixfsr90_ls_st.csv")
#trace_90crps <- read.csv("vixfsr90_crps_st.csv")

fsr90 <- data.frame(trace_fsr90,trace_fsr80,trace_90ls)

# add back dates to the fsr10 df, initial step = 1000

fsr90 <- fsr90 %>% mutate(Date = logvix$Date[1001:6000])

# plot scores --- lower tail 
fsr90$Date <- as.Date(fsr90$Date,format = "%Y-%m-%d")

color2 <- c("CLS90"="blue","CLS80"="red","MLE"="green")

fsr90plot <- fsr90 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS90,color = "CLS90"),size=0.25)+
  geom_line(aes(y = CLS80,color = "CLS80"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  # geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-upper tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color2)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.05,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=-0.05,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  
# put two plots side by side 
ggarrange(fsr10plot,fsr90plot,ncol=2,legend = "bottom",labels = c("Figure 9(a) CLS10","Figure 9(b) CLS90")) 
  #labs(caption = "Figure 9: the predictive distributions are produced by HAR-RV model with Student-t error and constant volatility") +
  #theme(plot.caption = element_text(hjust = 0))
```

Figure 9: Cumulative average out-of-sample scores of CLS10 (left panel) and CLS90 (right panel) over an expanding evaluation period, for predicting log(VIX) under the misspecified HAR-RV model with constant $\sigma$ and $z_{t+1}$ from the standardized Student-t distribution. Panel (a) shows the measure of performance in predicting the lower tail by optimizing CLS10, CLS20, CRPS and LS. Panel (b) shows the measure of performance in predicting the upper tail by optimizing CLS80, CLS90, CRPS and LS. 

### 5.4.3 Results of specification 3 & 4: 

Specification 3 and 4 move from the constant variance of the predictive distribution to the time-varying variance, which is better specified since strong serial correlation of the volatility is observed in the VIX series as shown in Figure 7.C. Table 14 and Figure 10 illustrate the performance of optimal forecasts in Specification 3. The amount of improvement decreased very slightly, compared with results of Specification 1 which also specifies a Gaussian error term. Similar conclusions can be drawn from comparing Table 15 and Figure 11 with the results of Specification 2. However, overall, as the model becomes less misspecified, we gain less from optimal forecasts in predicting the upper tail of VIX. 

```{r}
vix_table <- read.csv("data/vix_table_garch.csv")
vix_table <- rename(vix_table, "In-sample optimizers" = Row,"CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
vix_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

vix_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 14: Average scores for the VIX with ARCH Gaussian error"=7))
```

Table 14: Average out-of-sample scores under model specification 3 where the HAR-RV model has a time-varying $\sigma$ following ARCH(1) process and normally distributed $z_{t+1}$. The row names are the in-sample optimizers used in producing the probabilistic forecasts and the column names are the criterion functions used to evaluate the out-of-sample performance of the probabilistic forecasts produced by each optimizer. The bolded value is the column maximum indicating the best performing optimiser according to the out-of-sample criterion function. 

```{r,fig.width=10,fig.height=5}
# to compress all the calculated plotting average scores together in the same df 
trace_fsr10 <- read.csv("data/vixfsr10_garch.csv")
trace_fsr10 <- trace_fsr10 %>% rename("CLS10"=FSR10)
trace_fsr20 <- read.csv("data/vixfsr20_garch.csv")
trace_fsr20 <- trace_fsr20 %>% rename("CLS20"=FSR20)
trace_10ls <- read.csv("data/vixfsr10_ls_gar.csv")
trace_10crps <- read.csv("data/vixfsr10_crps_gar.csv")

fsr10 <- data.frame(trace_fsr10,trace_fsr20,trace_10ls,trace_10crps)

# add back dates to the fsr10 df, initial step = 1000

fsr10 <- fsr10 %>% mutate(Date = logvix$Date[1001:6000])

# plot scores --- lower tail 
fsr10$Date <- as.Date(fsr10$Date,format = "%Y-%m-%d")

color1 <- c("CLS10"="blue","CLS20"="red","MLE"="green","CRPS"="black")

fsr10plot <- fsr10 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS10,color = "CLS10"),size=0.25)+
  geom_line(aes(y = CLS20,color = "CLS20"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="Average Scores",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-lower tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color1)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=0.08,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=0.08,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  

# plot scores --- upper tail 
trace_fsr90 <- read.csv("data/vixfsr90_gar.csv")
trace_fsr90 <- trace_fsr90 %>% rename("CLS90"=FSR90)
trace_fsr80 <- read.csv("data/vixfsr80_gar.csv")
trace_fsr80 <- trace_fsr80 %>% rename("CLS80"=FSR80)
trace_90ls <- read.csv("data/vixfsr90_ls_gar.csv")
trace_90crps <- read.csv("data/vixfsr90_crps_gar.csv")

fsr90 <- data.frame(trace_fsr90,trace_fsr80,trace_90ls,trace_90crps)

# add back dates to the fsr10 df, initial step = 1000

fsr90 <- fsr90 %>% mutate(Date = logvix$Date[1001:6000])

# plot scores --- lower tail 
fsr90$Date <- as.Date(fsr90$Date,format = "%Y-%m-%d")

color2 <- c("CLS90"="blue","CLS80"="red","MLE"="green","CRPS"="black")

fsr90plot <- fsr90 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS90,color = "CLS90"),size=0.25)+
  geom_line(aes(y = CLS80,color = "CLS80"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-upper tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color2)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.05,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=-0.05,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  
# put two plots side by side 
ggarrange(fsr10plot,fsr90plot,ncol=2,legend = "bottom",labels = c("Figure 10(a) CLS10","Figure 10(b) CLS90"))
  #labs(caption = "Figure 10: the predictive distributions are produced by HAR-RV model with Gaussian error and time-varying volatility") +
  #theme(plot.caption = element_text(hjust = 0))

```

Figure 10: Cumulative average out-of-sample scores of CLS10 (left panel) and CLS90 (right panel) over an expanding evaluation period, for predicting log(VIX) under the misspecified HAR-RV model with time-varying $\sigma$ and normally distributed $z_{t+1}$. Panel (a) shows the measure of performance in predicting the lower tail by optimizing CLS10, CLS20, CRPS and LS. Panel (b) shows the measure of performance in predicting the upper tail by optimizing CLS80, CLS90, CRPS and LS. 

```{r}
vix_table <- read.csv("data/vix_table_gst.csv")
vix_table <- rename(vix_table, "In-sample optimizers" = Row,"CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
vix_table$`In-sample optimizers`<-c("LS","CLS10","CLS20","CLS80","CLS90")

vix_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    #CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=5)) %>% 
  add_header_above(c("Table 15: Average scores for the VIX with ARCH Student-t error"=6))
```

Table 15: Average out-of-sample scores under model specification 4 where the HAR-RV model has a time-varying $\sigma$ following ARCH(1) process and $z_{t+1}$ from the standardized Student-t distribution. The row names are the in-sample optimizers used in producing the probabilistic forecasts and the column names are the criterion functions used to evaluate the out-of-sample performance of the probabilistic forecasts produced by each optimizer. The bolded value is the column maximum indicating the best performing optimiser according to the out-of-sample criterion function. 

```{r,fig.width=10,fig.height=5}
# to compress all the calculated plotting average scores together in the same df 
trace_fsr10 <- read.csv("data/vixfsr10_gst.csv")
trace_fsr10 <- trace_fsr10 %>% rename("CLS10"=FSR10)
trace_fsr20 <- read.csv("data/vixfsr20_gst.csv")
trace_fsr20 <- trace_fsr20 %>% rename("CLS20"=FSR20)
trace_10ls <- read.csv("data/vixfsr10_ls_gst.csv")
#trace_10crps <- read.csv("vixfsr10_crps.csv")

fsr10 <- data.frame(trace_fsr10,trace_fsr20,trace_10ls)

# add back dates to the fsr10 df, initial step = 1000

fsr10 <- fsr10 %>% mutate(Date = logvix$Date[1001:6000])

# plot scores --- lower tail 
fsr10$Date <- as.Date(fsr10$Date,format = "%Y-%m-%d")

color1 <- c("CLS10"="blue","CLS20"="red","MLE"="green")

fsr10plot <- fsr10 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS10,color = "CLS10"),size=0.25)+
  geom_line(aes(y = CLS20,color = "CLS20"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  #geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="Average Scores",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-lower tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color1)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=0.08,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=0.08,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  

# plot scores --- upper tail 
trace_fsr90 <- read.csv("data/vixfsr90_gst.csv")
trace_fsr90 <- trace_fsr90 %>% rename("CLS90"=FSR90)
trace_fsr80 <- read.csv("data/vixfsr80_gst.csv")
trace_fsr80 <- trace_fsr80 %>% rename("CLS80"=FSR80)
trace_90ls <- read.csv("data/vixfsr90_ls_gst.csv")
#trace_90crps <- read.csv("vixfsr90_crps.csv")

fsr90 <- data.frame(trace_fsr90,trace_fsr80,trace_90ls)

# add back dates to the fsr10 df, initial step = 1000

fsr90 <- fsr90 %>% mutate(Date = logvix$Date[1001:6000])

# plot scores --- lower tail 
fsr90$Date <- as.Date(fsr90$Date,format = "%Y-%m-%d")

color2 <- c("CLS90"="blue","CLS80"="red","MLE"="green")

fsr90plot <- fsr90 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS90,color = "CLS90"),size=0.25)+
  geom_line(aes(y = CLS80,color = "CLS80"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  #geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-upper tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color2)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.05,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=-0.05,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  
# put two plots side by side 
ggarrange(fsr10plot,fsr90plot,ncol=2,legend = "bottom",labels = c("Figure 11(a): CLS10","Figure 11(b): CLS90"))
  #labs(caption = "Figure 11: predictive distributions are produced by HAR-RV model with Student-t error and time-varying volatility") +
  #theme(plot.caption = element_text(hjust = 0))

```

Figure 11: Cumulative average out-of-sample scores of CLS10 (left panel) and CLS90 (right panel) over an expanding evaluation period, for predicting log(VIX) under the misspecified HAR-RV model with time-varying $\sigma$ and $z_{t+1}$ from the standardized Student-t distribution. Panel (a) shows the measure of performance in predicting the lower tail by optimizing CLS10, CLS20, CRPS and LS. Panel (b) shows the measure of performance in predicting the upper tail by optimizing CLS80, CLS90, CRPS and LS. 

Comparing the results obtained in VIX prediction with the results in returns prediction, it indicates that the amount of improvement of prediction accuracy might mainly depend on how the shape of predictive density matches the shape of true density or how misspecified the assumed predictive model is in terms of predicting the tails. 

Even though optimal forecasts can only reap significant benefits in the upper tail of the VIX prediction, there is still great practical value in finance. Firstly, by definition, high values of the VIX indicates that the S&P 500, or the stock market in general, is expected to be more volatile (high volatility), which is an opportunity for either long or short position investors to gain profits or hedge risks, since no one can make money if the stock price doesn’t change at all. Besides, investors are typically risk averse, which makes the higher risk (the upper tail) is more of an interest to them, compared with the lower risk (the lower tail). More importantly, volatility is treated as an asset class and market participants are provided with liquid volatility products to trade in the market. For example, @cboefutures introduces futures contracts written on the VIX Index and a better prediction of VIX could be helpful for better futures pricing. Besides, there is literature discussing VIX futures and trading strategies based on different volatility forecasting techniques like in @vixfutures2020. Our results will make a contribution to the literature in this field as well. 

# 6. Conclusions 

According to the numerical and empirical results, the benefits of optimal forecasts can be observed when we have sufficiently misspecified model, subject to the assumed predictive model being compatible, and their performance does not vary among multiple types of model misspecification investigated in this paper. By mainly focusing on investigating the tail performance of optimal forecasts, the results show that the more misspecified the model is, the more benefits can be gained from using the censored likelihood score in improving the tail prediction accuracy in comparison with the conventional likelihood-based counterparts. From the empirical investigation results, the censored likelihood score tends to significantly improve the predictive accuracy for extreme events. This result is in evidence for both financial returns and the VIX volatility index when the stock market is extremely volatile during the 2008 Global financial crisis and the COVID-19 pandemic. What is interesting is that significant improvement of prediction accuracy is observed in both lower and upper tails of S&P 500 log returns but in the VIX volatility index, it only shows up in the upper tail. 

There are some caveats as well. Firstly, the predictions produced by the censored likelihood score are based on the tail regions of the marginal distribution, while Value-at-Risk is calculated from the conditional predictive distribution which is updated through each iteration. Therefore, there is a slight mismatch in distributions when predicting Value-at-Risk. Secondly, the practical usage of Value-at-Risk is often around 1% or 5%, but at this very small tail, the censored likelihood score might be less accurate than predicting 10% or 20% tail, since there are less observations in the tail and consequently, harder to focus correctly. However, this drawback might be corrected by changing to a more suitable assumed predictive model which provides a heavy tail distribution. As mentioned above, the difference between the performance of optimal forecasts in S&P 500 log returns and in the VIX volatility index is hard to interpret. The performance of optimal forecasts depends on the true conditional predictive distribution, which is always unknown in practice. Even though the marginal distributions can provide some insights about the shape of true conditional predictive distribution, it still creates a problem for determining which tail could optimal forecasts reap significant benefits. More importantly, the reasons why we can gain considerably from optimal forecasts after the Global financial crisis needs further investigation. Nevertheless, the results in this paper provide important insights to the financial forecasting literature. Optimal probabilistic forecasts can produce better Value-at-Risk predictions for the financial returns or for the VIX volatility index. This method also can improve the predictive accuracy in the upper tail of the VIX volatility index distribution, which suggests that extremely high volatility in the market can be more accurately predicted. It is an important contribution to hedging risks and trading derivatives in the financial market. 

