---
compact-title: no
editor_options:
  chunk_output_type: console
  header-includes:
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{setspace}\doublespacing
  - \usepackage[hyphens]{url}
  - \usepackage{float}
fontsize: 12pt
geometry: margin=1in
output:
  pdf_document:
    fig_height: 3.5
    fig_width: 4.5
link-citations: yes
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE,
  warning = FALSE)
```


```{r}
knitr::include_graphics("thesis cover page .pdf")
```

\pagebreak

# Abstract 

With correct model specification being a major challenge for conventional likelihood-based prediction, scoring rules have been used to produce forecasts that are accurate – in a well-defined sense - in the presence of misspecification. Building on previous research, this paper conducts numerical and empirical analyses to explore the impact of the form and degree of model misspecification on probabilistic forecasts that are optimal under a given score. In particular, we assess the predictive performance of optimal forecasts in two empirical contexts: prediction of the Value-at-Risk of a financial return and prediction of the VIX volatility index. Results show that the greater the degree of model misspecification, the more beneficial is optimal prediction, no matter what the form of model misspecification. Most notably, predictions that are optimal according to the censored likelihood score, which rewards predictive accuracy in a tail are substantially more accurate at predicting extreme events in said tail than the alternatives, including likelihood-based predictions. 

\pagebreak

```{=latex}
% Trigger ToC creation in LaTex
\setcounter{tocdepth}{2}
\tableofcontents
```

\pagebreak

# 1. Introduction

This paper explores the question: ‘When do optimal probabilistic forecasts work’, with particular attention given to the usefulness of optimal forecasts in financial applications. 

Probabilistic forecasts can provide complete information about future uncertainty, which can be more valuable to forecasters than point and interval forecasts. However, previous approaches to prediction - including probabilistic methods – typically assume that the predictive model, or the likelihood function, correctly specifies the true data generating process (DGP) or that the true DGP lies in a specified set of models, which is almost certainly untrue in practice. Optimal probabilistic forecasting refers to the construction of a predictive probability distribution that is optimal according to a user-specified proper scoring rule [@optimal]. It is not based on the conventional likelihood function but on a proper scoring rule that matches the specific problem at hand. Optimal probabilistic forecasts aim to yield accurate predictions in the presence of a misspecified predictive model, whilst the conventional approaches to probabilistic forecasting are tied more closely to the predictive model, and are only as accurate as that model itself. 

Particular proper scoring rules allow users to produce probabilistic predictions that are designed to perform well according to the forecasting metric that is important to the problem at hand [@gneiting2007]. Some recent research shows that forecasts produced using a given scoring rule can yield better out-of-sample accuracy - measured by that scoring rule - than conventional likelihood-based forecasts, in particular in the presence of model misspecification; see @opschoor2017, @fbp2020 and @optimal. Among all these investigations, it is the censored likelihood score (CLS) proposed originally by @diks2011, that captures our attention for the purpose of demonstrating the application of optimal probabilistic forecasts in finance. Its ‘focusing’ feature has great potential in financial risk management. In other words, CLS allows a forecaster to focus on predicting any particular region of interest accurately. Therefore, its use in this context represents an important contribution to risk prediction in financial settings. 

Whilst some empirical investigations of the performance of optimal forecasts were undertaken in @optimal, these were primarily illustrative, and left much scope for exploring the performance of optimal probabilistic forecasting methods in forecasting different types of financial measures, and under different assumed models. This research paper extends the work of @optimal by investigating the effect of the form and degree of model misspecification on the performance of optimal probabilistic forecasts in financial settings; in particular by conducting empirical analyses of how optimal forecasts perform in predicting the Value-at-Risk (VaR) and the VIX volatility index (VIX). Importantly, the empirical dataset used extends over a period of time that precedes the 2008 global financial crisis (GFC) and that includes the latest period in which COVID-19 has had an impact on financial markets. 

The paper proceeds as follows. Section 2 explains the basic idea about how scoring rules are applied in producing and evaluating density forecasts, and provides the definitions of some commonly used scoring rules. Section 3 focuses on investigating the effects of different forms and degrees of model misspecification on the performance of optimal forecasts, using data simulated from the (generalized) autoregressive conditional heteroscedasticity ((G)ARCH) and inversion copula models. The assumed predictive model is fixed as a Gaussian ARCH(1) model for the purpose of controlling the degree of model misspecification. In Section 4, we emulate the simulation exercise in an empirical example for financial returns on the S&P 500 stock index, including the prediction of the VaR. The results illustrate the practical contributions of our predictive approach to financial risk management. The empirical analysis of the VIX is provided in Section 5, where the predictive model is the heterogeneous autoregressive-realized volatility (HAR-RV) model with different error term specifications. We conclude and discuss any possible caveats in Section 6. 

# 2. Scoring rules in prediction
## 2.1 Overview and notation 

Scoring rules $S(\cdot,\cdot)$ are a type of criterion function that can be optimized to produce an optimal estimator and hence, a probabilistic prediction derived from this optimal estimator. They also play the role of evaluating and ranking competing probabilistic forecasts, based on materialized events or values, by assigning a numerical score [@gneiting2007]. That is, if the forecaster produces a predictive distribution $\mathit{P}$ and the event $y$ materializes, then the reward to this prediction given by a scoring rule is denoted by $S(P,y)$. For a positively-oriented score, a higher value will be assigned to a better forecast between two competing candidates, on the condition of the scoring rule being proper. The propriety of the scoring rule is crucial here, as an improper scoring rule may assign a higher average score to an incorrect density forecast [@gneiting2007]. Consequently, only proper scoring rules are used in the production of optimal forecasts.

Suppose $P$ is the predictive distribution and $Q_0$ is the true DGP. $\mathbb{S}(P,Q_0)$ denotes the expected score of $S(P,\cdot)$ under $Q_0$. That is,
\begin{equation} \label{eq:score}
\mathbb{S}(P,Q_0)=\int_{y \in \Omega}S(P,y)dQ_0(y)
\end{equation}
where $\Omega$ is a probability space. 
A scoring rule is said to be proper if $\mathbb{S}(Q_0,Q_0) \geq \mathbb{S}(P,Q_0)$ for all $P$ and $Q_0$, and is strictly proper if $\mathbb{S}(Q_0,Q_0) = \mathbb{S}(P,Q_0)$ only when $P = Q_0$ [@gneiting2007]. This ensures that a proper scoring rule is maximized when the prediction reveals the truth, on the condition that the predictive distribution class $\mathcal{P}$ contains $Q_0$. 

However, in practice, the expected score $\mathbb{S}(\cdot,Q_0)$ is unknown and cannot be obtained, but we can use a sample average $\bar S(\cdot)$ as a reasonable estimation. Asymptotically, the true predictive distribution can be recovered by optimizing a sample criterion of any proper scoring rule, if the true predictive is contained in the predictive class over which the maximization occurs. Even when $Q_0$ is not contained in the predictive distribution class $\mathcal{P}$ proposed by the forecaster, it does not change the fact that a proper scoring rule will reward a particular form of forecast accuracy and hence, select the best forecast within the proposed predictive distribution class, according to this scoring rule. 

Assume the scoring rule is positively-oriented and the unknown parameters of the assumed predictive model are denoted as $\boldsymbol \theta \in \Theta$, and let $P^{t-1}_{\boldsymbol{\theta}}$ := $P(\cdot|\mathcal{F}_{t-1},\boldsymbol \theta)$ be the one-step-ahead predictive distribution function based on the assumed model where $\mathcal{F}_{t-1}$ represents all the information available at time $t-1$, and $p^{t-1}_{\boldsymbol{\theta}}$ := $p(\cdot|\mathcal{F}_{t-1},\boldsymbol \theta)$ be the corresponding predictive density function at time $t$. 
Then an estimator $\boldsymbol{\hat \theta}$ obtained by maximizing $\bar S(\boldsymbol{\theta})$ is said to be 'optimal' based on this scoring rule and $P^{t-1}_{\boldsymbol{\hat \theta}}$ is referred to as the optimal predictive. 
Specifically, we define 
\begin{equation} \label{eq:1}
\boldsymbol{\hat \theta} =arg \max_{\boldsymbol{\theta} \in \Theta} \bar S(\boldsymbol{\theta})
\end{equation}
where 
\begin{equation} \label{eq:sbar}
\bar S = \frac{1}{T-\tau+1}\sum_{t=2}^{T-\tau} S(P_{\boldsymbol{\theta}}^{t-1},y_t)
\end{equation}
with $\mathit{T}$ the total number of observations on a time series variable $y_t$ and $\tau$ the out-of-sample size. Under certain conditions, including that the scoring rule is proper and that the predictive model is correctly specified, $\boldsymbol{\hat \theta} \to \boldsymbol{\theta}_0 \ as \ \mathit{T} \to \infty$ where $\boldsymbol{\theta}_0$ represent the true (vector) parameter.

## 2.2 Some commonly used scoring rules 

A variety of scoring rules have been developed to tackle different problems. Some commonly used proper scoring rules are adopted in the analyses in this paper, such as the logarithmic score (LS), the continuously ranked probability score (CRPS) and the CLS. 

The LS is defined as
\begin{equation} \label{eq:ls}
S_{LS}(P^{t-1}_{\boldsymbol \theta}, y_t) = ln[p(y_t|\mathcal{F}_{t-1},\boldsymbol{\theta})]
\end{equation}
where $p(\cdot|\mathcal{F}_{t-1},\boldsymbol{\theta})$ is the predictive density function. It is a local strictly proper scoring rule, which means it will assign a higher score to the correct probabilistic forecast, and it is superior to quadratic and spherical scoring rules when the rank ordering is important or the impact of the nonlinear utility function used is a concern for forecasters [@bickel2007]. One common usage of the LS in frequentist econometrics is maximum likelihood estimation (MLE). That is, 
\begin{equation} \label{eq:mle}
\boldsymbol{\hat \theta}_{MLE}=arg \max_{\boldsymbol{\theta} \in \Theta} \{\frac{1}{T} \sum_{t=1}^T ln[p(y_t|\mathcal{F}_{t-1},\boldsymbol \theta)]\}
\end{equation}
LS yields an asymptotically efficient estimator of $\boldsymbol \theta$ under correct specification and regularity.

However, the LS is criticized for its unboundedness and its local property. @bernardo1979 states "locality requires the utility of probabilistic influence to depend only upon the probability density of the true state". @gneiting2007 also argue that the LS is insensitive to distance and will not reward predictions that are close to but not identical to the materialized event. @gneiting2007 propose the continuously ranked probability score, which is sensitive to distance and is defined as
\begin{equation} \label{eq:crps}
S_{CRPS}(P^{t-1}_{\boldsymbol \theta},y_t)=-\int_{-\infty}^{\infty}[P(y|\mathcal{F}_{t-1},\boldsymbol{\theta})-I(y\geq y_t)]^2dy
\end{equation}
where $\mathit{P}(\cdot|\mathcal{F}_{t-1},\boldsymbol{\theta})$ is the predictive cumulative distribution function, *I* is the indicator function, and $y_t$ is the materialized event. 

The formula can be simplified as follows if the predictive distribution is Gaussian with mean $\mu$ and variance $\sigma^2$, 
\begin{equation} \label{eq:crnorm}
S_{CRPS}(N^{t-1}_{\boldsymbol \theta},y_t)=\sigma[\frac{1}{\sqrt{\pi}}-2\phi(\frac{y_t-\mu}{\sigma})-\frac{y_t-\mu}{\sigma}(2\Phi(\frac{y_t-\mu}{\sigma})-1)]
\end{equation}
where $\phi$ and $\Phi$ are the probability density function and cumulative distribution function of the standard Gaussian distribution.

Both LS and CRPS are used for producing predictive densities and evaluating the out-of-sample predictive performance. In terms of accurately predicting a certain region of a distribution, they are typically used together with weighted likelihood [@gneiting2011]. @diks2011 propose the CLS, which allows users to reward forecasts only on a region (regions) of interest instead of using weights to emphasize a particular part of the entire density forecast. Moreover, it can be easily used to combine density forecasts and yield better predictive accuracy. It is defined as
\begin{equation} \label{eq:cls}
S_{CLS}(P^{t-1}_{\boldsymbol \theta}, y_t)=I(y_{t} \in A)ln p(y_{t}|\mathcal{F}_{t-1},\boldsymbol{\theta})+I(y_{t} \in A^c)ln[\int_{A^c} p(y|\mathcal{F}_{t-1},\boldsymbol{\theta})dy]
\end{equation}
where $p(\cdot|\mathcal{F}_{t-1},\boldsymbol{\theta})$ is the predictive probability density function, $A$ is the region of interest and $A^c$ is the complement of $A$. @opschoor2017 prove that weighted density forecasts based on optimizing CLS outperform those on LS and CRPS. 

# 3. Numerical investigation of optimal predictions
## 3.1 Simulation design 

Optimal forecasts can outperform predictions produced by conventional methods out-of-sample; see @opschoor2017, @fbp2020 abd @optimal; but the underlying reasons driving this phenomenon are still under investigation. Recent research show that the degree of model misspecification impacts the performance of optimal forecasts. The greater the degree of misspecification is, the more benefits are gained from optimal forecasts in both Bayesian and frequentist predictions, conditional on the assumed predictive model being broadly 'compatible' with the true DGP; see @fbp2020 and @optimal. The simulation analysis in this paper will extend the exploration in this field and ask the question ‘Will the performance of optimal forecasts change when a different form of misspecification occurs?’. 

In order to set the scene for the financial application analyses in the following sections of this paper, we simulate a time series variable $y_t$ that mimics the behavior of financial returns and their volatility. Specific simulation scenarios are listed in Table 1 ($t_\nu$ indicates a Student-t distribution with $\nu$ degrees of freedom). 

*Table 1: The three simulation scenarios used in the numerical analysis. Scenario (i) represents correct model specification; Scenario (ii) represents a form of misspecification where the true DGP is GARCH with Student-t distribution while the assumed model is Gaussian ARCH(1); Scenario (iii) represents a more extreme form of misspecification, where the true DGP is an stochastic volatility model with a non-Gaussian marginal distribution while the assumed model is Gaussian ARCH(1).*

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{Table 1: Simulation design}} \\ \hline
          & \textbf{Scenario (i)} & \textbf{Scenario (ii)} & \textbf{Scenario (iii)} \\ \hline
\textbf{True DGP} & $y_t = \sigma_t \epsilon_t$ & $y_t = \sigma_t \epsilon_t$ & Stochastic volatility inversion copula \\
& $\sigma_t^2 = 1 + 0.2y_{t-1}^2$ & $\sigma_t^2 = 1 + 0.2y_{t-1}^2+0.7\sigma_{t-1}^2$ & Shape parameter = \{0, -3, -5\} \\ & $\epsilon_t \sim N(0,1)$ & $\epsilon_t \sim (\frac{\nu-2}{\nu})^{0.5} * t_\nu$ & \\ & & $\nu \in \{3,12,35\}$ & \\ \hline

\textbf{Assumed model} & $y_t = \mu +\sigma_t \epsilon_t$ & $y_t = \mu +\sigma_t \epsilon_t$ & $y_t = \mu +\sigma_t \epsilon_t$ \\ & $\sigma_t^2 = \alpha_0 + \alpha_1 y_{t-1}^2$ & $\sigma_t^2 = \alpha_0 + \alpha_1 y_{t-1}^2$ & $\sigma_t^2 = \alpha_0 + \alpha_1 y_{t-1}^2$ \\ & $\epsilon_t \sim N(0,1)$ & $\epsilon_t \sim N(0,1)$ & $\epsilon_t \sim N(0,1)$ \\ \hline
\end{tabular}}
\end{table}

GARCH models are used to capture the volatility clustering and serial dependence that are usually observed in empirical stock returns, and the negative marginal skewness will be incorporated by using a stochastic volatility inversion copula model, with the degree of marginal skewness controlled by the shape parameter. 

Copula models are a flexible way of representing the joint distribution function of a set of variables [@fan2014]. In recent years, these models have been applied to modelling serial dependence, which can be constructed as: 
\begin{equation} \label{eq:copula}
p_{t-1}(y_t)=c(P(y_t),P(y_{t-1}))p(y_{t-1})
\end{equation}
where $\mathit{P}()$ and $\mathit{p}$() denote the marginal distribution and density function of $y_t$ respectively, and c() is the copula density function that captures the dependence between $y_t$ and $y_{t-1}$. 

For the simulation exercise, the marginal distribution $\mathit{P}()$ is chosen to be the standard skew-normal distribution, while c() is chosen to be the inversion copula of the stochastic volatility model described in @inversion2018. The stochastic volatility inversion copula model allows us to keep the dependence structure of a state space model and provides the flexibility of using an asymmetric marginal distribution that is often observed in financial returns [@inversion2018].

In order to manipulate the degree and form of model misspecification, we fix the underlying model to a simple and clearly misspecified ARCH(1) model and then adjust degrees of freedom and shape parameters of the true DGP specifications in Scenario (ii) and (iii). 

We apply the scoring rules introduced in Section 2.2 and set CLS to focus on the 10%, 20%, 80% and 90% tails of the empirical marginal distribution, corresponding to the risks of long and short portfolios of financial investments, and we label them as CLS10, CLS20, CLS80 and CLS90 in the following tables and figures. Due to the fact that there is no reliable estimate of the true conditional densities, only the tails of the marginal distributions are set to be the standard when assessing the predictive accuracy by CLS, rather than the conditional distributions. Meanwhile, LS gives insights about the performance of the conventional likelihood-based methods and is used as a benchmark for comparing whether optimal forecasts can provide benefits or not. 

We conduct Monte Carlo simulations using the methodology in @optimal and @fbp2020. Specific steps are as follows: 

Let $P^{t-1}_{\boldsymbol{\hat \theta^{[i]}}}$ be the one-step-ahead predictive distribution function based on the Gaussian ARCH(1) model and optimized according to the scoring rule $S_i$, and $p(y_t|\mathcal{F}_{t-1},\boldsymbol{\hat \theta^{[i]}})$ be the corresponding predictive density function at time $\mathit{t}$. 

  1.	Generate T = 6000 observations of $y_t$ from the true DGP
  
  2.	Use observations $y_t$ where $t = 1,2,...,1000$ to estimate $\boldsymbol{\hat \theta}^{[i]} = \{\hat \mu, \hat \alpha_0, \hat \alpha_1\}^{[i]}$ as in $\eqref{eq:1}$ based on $S_i$, $i \in \{LS, CRPS, CSL10,CSL20,CSL80,CSL90\}$ where $S_i$ refers to $\eqref{eq:ls}$ $\eqref{eq:crps}$ $\eqref{eq:cls}$
  
  3.	Produce the one-step-ahead predictive $P^{t-1}_{\boldsymbol{\hat \theta^{[i]}}}$, and compute the out-of-sample score $S_j$, $j \in \{LS, CRPS, CSL10,CSL20,CSL80,CSL90\}$
where $S_j$ refers to $\eqref{eq:ls}$ $\eqref{eq:crps}$ $\eqref{eq:cls}$:^[The CRPS in $\eqref{eq:crps}$ is transformed to a positively-oriented score for the purpose of convenient comparison among all scores.]

  4. Expand estimation window by one observation and repeat steps 2-3 with $\tau = T-1000$ times and compute the average scores:^[Out-of-sample size $\tau = 5000$ is sufficiently large to let the optimal forecasts take place; see @optimal for full discussion about sample size effects on optimal forecasts.]

\begin{equation} \label{eq:avgscore}
\bar S_j(\boldsymbol{\hat \theta^{[i]}})=\frac{1}{\tau}\sum^T_{t=T-\tau+1}S_j(P_{\boldsymbol{\hat \theta^{[i]}}}^{t-1},y_t)
\end{equation}

For the purpose of conveniently documenting and interpreting the results consistently with previous literature, we introduce the concept of ‘coherence’ and ‘strict coherence’ here. Coherence means that the optimal probabilistic forecast based on a given score is superior, or at least performs the same as, alternative forecasts according to the same score. Strict coherence happens when the optimal prediction is strictly preferable given that score. Let $\boldsymbol{\hat \theta}_1$ and $\boldsymbol{\hat \theta}_2$ be the optimizers based on scoring rules:$S_1$ and $S_2$. Then the optimal predictions $P^{t-1}_{\boldsymbol{\hat \theta_1}}$ and $P^{t-1}_{\boldsymbol{\hat \theta_2}}$ can be derived from $\boldsymbol{\hat \theta}_1$ and $\boldsymbol{\hat \theta}_2$. The coherent results are observed if 
\begin{equation} \label{eq:coh1}
\frac{1}{\tau}\sum_{t=T-\tau+1}^{T}S_1(P^{t-1}_{\boldsymbol{\hat \theta_1}},y_t) \geq \frac{1}{\tau}\sum_{t=T-\tau+1}^{T}S_1(P^{t-1}_{\boldsymbol{\hat \theta_2}},y_t)
\end{equation}
\begin{equation} \label{eq:coh2}
\frac{1}{\tau}\sum_{t=T-\tau+1}^{T}S_2(P^{t-1}_{\boldsymbol{\hat \theta_1}},y_t) \leq \frac{1}{\tau}\sum_{t=T-\tau+1}^{T}S_2(P^{t-1}_{\boldsymbol{\hat \theta_2}},y_t)
\end{equation}
It is said to be strictly coherent if $\eqref{eq:coh1}$ and $\eqref{eq:coh2}$ are strict inequalities [@optimal].


## 3.2 Simulation results
### 3.2.1 Average out-of-sample scores

The first column of each of the following tables presents the labels for the $S_i$ that we used to produce predictions and the third-row shows $S_j$, which is used for the forecast's evaluation. The bolded numbers are the largest values of $\bar S_j(\boldsymbol{\hat \theta^{[i]}})$ in each column. We use all positively-oriented scoring rules, and therefore, the column maximum indicates the optimal prediction based on the scoring rule indicated by the column name. 

Table 2 shows the correct model specification - Scenario (i) - where the optimal forecasting method does not reap benefits and there is no strict coherence observed in the table. Under correct specification, no matter what the score rewards, estimators produced from $S_i$ are expected to concentrate to the same true parameter values as $T$ approches to infinity, which causes no distinct improvement in predictive accuracy among different scoring rules [@optimal].

Table 3 shows the results of Scenario (ii) where the assumed predictive model, ARCH(1), cannot capture the fat tails and volatility clustering feature of the data generated from the GARCH(1,1) model with Student-t error, i.e. the true DGP. Hence, there is misspecification. Tables 2 and 3 replicate some parts of the simulation results in previous literature; see @fbp2020, @optimal. Optimal forecasts are expected to reap more benefits as the degree of model misspecification increases, and predictions based on CLS at 80% and 90% perform poorly when assessed by LS, CRPS, CLS at 10% and CLS at 20%. The question is whether these characteristics of optimal forecasts will change if we move to a different form of model misspecification. 

Table 4 shows another type of model misspecification, listed in Scenario (iii), where the ARCH(1) model cannot capture the asymmetric feature of the data generated from the stochastic volatility inversion copula model (the true DGP). Table 4 shows similar results as in Table 3. As the shape parameter decreases, the degree of negative marginal skewness increases, and optimal forecasts provide more benefits. Even though, strictly speaking, we do not observe strict coherence in Panel C, the most misspecified case, it is close enough to conclude that optimal forecasts perform in the same way even when we have different forms of model misspecification. ^[Tables are produced by R package: @kableextra and @knitr.] 

*Table 2: It shows the results of Scenario (i), the correct model specification case. Both the true DGP and assumed predictive model are ARCH(1) with Gaussian error.* 

```{r, message=FALSE}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(tsibble)
library(tsibbledata)
library(feasts)
library(normtest)
library(moments)
library(ggthemes)
library(ggpubr)
library(knitr)
library(kableExtra)
```

```{r}
# Editting the matlab written csv files 

GARCH_t35 <- read.csv("data/GARCH35.csv")
GARCH_t35 <- rename(GARCH_t35, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
GARCH_t35$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

GARCH_t12 <- read.csv("data/GARCH_t12__table.csv")
GARCH_t12 <- rename(GARCH_t12, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
GARCH_t12$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

GARCH_t3 <- read.csv("data/GARCH_t3_table.csv")
GARCH_t3 <- rename(GARCH_t3, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
GARCH_t3$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

ARCH_table <- read.csv("data/ARCH_table.csv")
ARCH_table <- rename(ARCH_table, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
ARCH_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

Copula5_table <- read.csv("data/Copula5_table.csv")
Copula5_table <- rename(Copula5_table, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
Copula5_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

Copula3_table <- read.csv("data/Copula3_table.csv")
Copula3_table <- rename(Copula3_table, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
Copula3_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

Copula0_table <- read.csv("data/Copula0_table.csv")
Copula0_table <- rename(Copula0_table, "In-sample optimizers" = Row, "CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
Copula0_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

```


```{r}
ARCH_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 2: the true DGP is ARCH(1)"=7))
```

*Table 3: It corresponds to Scenario (ii) where the assumed predictive model is ARCH(1) with Gaussian error and the true DGP is GARCH(1,1) with Student-t error*

```{r}
GARCH_t35 <- GARCH_t35 %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  )

GARCH_t12 <- GARCH_t12 %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  )

GARCH_t3 <- GARCH_t3 %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) 
garch <- rbind(GARCH_t35,GARCH_t12,GARCH_t3)

garch %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  pack_rows("Panel A: degree of freedom = 35", 1,6) %>%
  pack_rows("Panel B: degree of freedom = 12", 7,12) %>%
  pack_rows("Panel C: degree of freedom = 3",13,18) %>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 3: the true DGP is GARCH(1,1) with Student-t error"=7))
```

*Table 4: It corresponds to Scenario (iii) where the assumed model is ARCH(1) with Gaussian error and the true DGP is stochastic volatility inversion copula.*

```{r}
Copula0_table <- Copula0_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) 

Copula3_table <- Copula3_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) 

Copula5_table <- Copula5_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) 

copula <- rbind(Copula0_table,Copula3_table,Copula5_table)
copula %>% 
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  pack_rows("Panel A: shape = 0", 1,6) %>%
  pack_rows("Panel B: shape = -3", 7,12) %>%
  pack_rows("Panel C: shape = -5", 13,18) %>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 4: the true DGP is stochastic volatility inversion copula"=7))
```

### 3.2.2 Trace plots & predictive densities

Accurate prediction of extreme values is important for financial risk management. Therefore, we provide score trace plots to exhibit the comparison of MLE, CRPS and CLS’s performance in predicting the 10 percentile and 90 percentile in Figure 1 and 2 so that we can evaluate their performance in predicting tails. The results in Section 3.2.1 show that when the degree of model misspecification is high, CLS can perform the best compared with MLE and CRPS in predicting both tails. However, only the average score values at the end of each iteration of the simulation are recorded in the tables, while trace plots provide a dynamic view of how optimal forecasts perform. 

In the left panel of each trace plot, it shows the performance of each score evaluated by CLS 10%. That is, if the average score value of score A used for estimation is higher than score B, then score A predicts the 10 percentile of the distribution more accurately than score B. The right panel shows the same thing but is based on the performance of predicting 90 percentile. From panels (a) and (b) in Figures 1 and 2, we can see that CLS almost makes no difference if the degree of model misspecification is very small compared with other scoring rules, but as the misspecification increases, CLS starts to outperform CRPS and MLE in predicting tails. The more misspecified the model is, the more benefits can we gain from using CLS. Furthermore, more improvement in predictive accuracy occurs in the upper tail than in the lower tail, especially in Figure 2 where we have a negatively skewed marginal distribution. This result is consistent with @fbp2020. It is beneficial for econometricians to improve forecast accuracy without working so hard on finding a ‘correct’ model when in practice, a correct model does not exist. 

```{r}
# need to adjust the size. 
knitr::include_graphics("figure/figure1.pdf")
```

```{r}
knitr::include_graphics("figure/figure 2.pdf")
```

From the average out-of-sample score tables and trace plots above, we can see that optimal forecasts provide the most benefits in improving prediction accuracy under the most misspecified case. In order to provide a more complete view of how optimal forecasts work, the predictive densities of the most misspecified case are plotted in Figure 3. The shape of true conditional predictive distributions plays a role in deciding how much we can gain by using optimal forecasts, since predictions obtained from optimizing CLS aim to match the shape of tails of the true conditional predictive distributions. CLS focusing on upper tails (80%, 90%) is expected to perform poorly in the lower tail so the forecasters should have a clear expectation about what they aim to achieve [@fbp2020]. However, in practice, the shape of true conditional predictive distributions is usually unknown and it might create the difficulty of determining which tail we can reap more benefits from optimal forecasts without analysing a particular problem beforehand. 

```{r,fig.align="center"}
knitr::include_graphics("figure/figure_pred.pdf")
```

# 4. Empirical analysis: financial returns 
## 4.1 Overview and preliminary diagnostics 

From the results in Section 3.2, we can conclude that optimal forecasts based on the CLS can improve the prediction accuracy in tails and its effect is clearest when the degree of model misspecification is high and does not vary much when we have different types of model misspecification. In Section 4, we apply the same method adopted in Section 3 to an empirical setting and investigate how it performs in practice. 

The data used in the analysis is the continuously compounded daily log returns of S&P 500 listed in U.S.A financial market over a broad time period from 27 Sep,1996 to 30 July,2020.^[Data retrieved from @spdata] It aims to include two very volatile periods, the GFC and the recent COVID-19 pandemic. The initial training sample size and out-of-sample observations are kept consistent with what is in Section 3 with initial training sample size = 1000 and out-of-sample evaluations = 5000.  

The descriptive statistics in Table 5 show that the distribution of S&P 500 log returns is negatively skewed and has fat tails and strong serial correlation in its volatility. Besides, during the GFC and COVID-19 pandemic periods, the volatility is very high due to high uncertainty.^[Descriptive statistics are produced by R packages: @dplyr, @tidyverse, @tsibble, @tsibbledata, @feasts, @normtest, @moments.]

```{r}
# data cleaning and import 
sp <- read.csv("data/^GSPC.csv")
sp <- sp %>% dplyr::select("Date","Adj.Close")

sp$log.returns <- c(NA,diff(log(sp$Adj.Close)))
logret <- sp[-c(1:188),c(1,3)] # delete the adjusted price col, only keep 6000 obs

# export the dataset to csv. including only trading days and log returns 
temp <- logret[,c(2)]
temp <- data.frame(temp)
write_csv(temp, "sp500log.csv")
```


```{r, results='hide'}
# descriptive statistics 
summary <- summary(logret$log.returns) # ----> need to be tabulated 
# range(logret$log.returns)
# some test results aginst normal dist h_null
# JB test
jb.norm.test(logret$log.returns)
# Ljung box test
Box.test(logret$log.returns,lag = 2,type = "Lj",fitdf = 1)

sk <- skewness(logret$log.returns)
kt <- kurtosis(logret$log.returns)
```

```{r}
sum <- data.frame(Stock="S&P500",Min=as.numeric(summary[1]),Median=as.numeric(summary[3]),Mean=as.numeric(summary[4]),Max=as.numeric(summary[5]),Skewness=sk,Kurtosis=kt,JB.Test=28270,LB.Test=61.489)
sum %>% 
  kable("latex",booktabs = T) %>% 
  kable_styling("bordered",font_size = 10) %>% 
  add_header_above(c("Table 5: Descriptive statistics"=9))
```


```{r,message=FALSE,fig.width=10,fig.height=5}

# time series plot
logret$Date <- as.Date(logret$Date,format = "%Y-%m-%d")

p1 <- logret %>% ggplot(aes(x = Date, y = log.returns)) +
  geom_line()+theme_classic()+labs(x="Year",y="Daily log returns")+
  #ggtitle("Figure 9: S&P500 daily log returns")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_y_continuous(labels = scales::percent) + 
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.12,label="Global Financial Crisis",color="red",size=3)+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=-0.12,label="COVID-19",color="red",size=2)+ # the date COVID-19 declared as pandemic
  theme_clean()
  #+labs(caption = "Figure 9: the time series plot of S&P500 daily log returns")

# histogram plots
p2 <- logret %>% ggplot()+geom_histogram(aes(x=log.returns))+xlim(-0.13,0.13)+
  #ggtitle("Figure 10")+
  scale_x_continuous(labels = scales::percent)+theme_clean()
  #+labs(caption = "Figure 10: the distribution of S&P500 log returns is asymmetric")

# ACF plots to see the autocorrelation 
logret <- logret %>% 
  mutate(trading_day = row_number()) %>%
  as_tsibble(index = trading_day,regular = TRUE)

p3 <- logret %>% ACF((log.returns)^2) %>% autoplot() + 
  #ggtitle("Figure 11:Squared log returns autocorrelation")+
  xlab("number of lags")+ylab("ACF for variance")+theme_clean()
# +labs(caption = "Figure 11: the autocorrelation plot shows there is a strong serial correlation in volatility of S&P500")

figure <- ggarrange(p1,                                                 
          ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
          nrow = 2, 
          labels = "A"                                        
          ) 
annotate_figure(figure, 
                top = text_grob("Descriptive figures",face = "bold"),
                fig.lab = "Figure 4",fig.lab.face = "bold"
                )
```

## 4.2 Empirical results: predictive distributions for returns 

From the wide-usage of ARCH models in modelling financial returns and volatility, and the strict coherence observed in Section 3.2.1, ARCH(1) should be a ‘compatible’ and sufficiently misspecified model to forecast S&P 500 series and to observe the difference made by adopting optimal forecasts. The results in Table 6 show that the CLS outperforms other alternative scores in both lower and upper tails, which closely follows the results and conclusions drawn in the numerical analysis in Section 3. However, the difference between using CLS10(80) and CLS20(90) is very small. 

```{r}
sp_table <- read.csv("data/sp_table.csv")
sp_table <- rename(sp_table, "In-sample optimizers" = Row,"CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
sp_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

sp_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 6: Average scores for S&P 500"=7))
```

Unlike in the simulation example, the score trace plots now show how different scoring rules perform at time points that correspond to some actual historical events. In Figure 5, one interesting phenomenon is that after the GFC, the CLS performs much better than CRPS and MLE, in terms of predicting both upper and lower tails. It indicates that optimal probabilistic forecasts based on CLS can predict extreme events, such as the GFC, more accurately than other alternatives. This improvement brings up the average score of CLS optimizer after the GFC as shown in Figure 5. The difference between CLS and MLE/CRPS gets smaller and smaller over time until the recent COVID-19 pandemic. It is also worth noticing that CLS does not appear to provide more benefits in the upper tail than in the lower tail, which is different from the simulation results.^[The figures are produced by R packages: @ggplot2, @ggthemes, @ggpubr.] 

```{r,fig.width=10,fig.height=5}
# to compress all the calculated plotting average scores together in the same df 
# this is assessed by cls10 for prediction accuracy
trace_fsr10 <- read.csv("data/fsr10.csv")
trace_fsr10 <- trace_fsr10 %>% rename("CLS10"=FSR10)
trace_fsr20 <- read.csv("data/fsr20.csv")
trace_fsr20 <- trace_fsr20 %>% rename("CLS20"=FSR20)
trace_10ls <- read.csv("data/fsr10_ls.csv")
trace_10crps <- read.csv("data/fsr10_crps.csv")

fsr10 <- data.frame(trace_fsr10,trace_fsr20,trace_10ls,trace_10crps)

# add back dates to the fsr10 df, initial step = 1000

fsr10 <- fsr10 %>% mutate(Date = logret$Date[1001:6000])

# plot scores --- lower tail 
fsr10$Date <- as.Date(fsr10$Date,format = "%Y-%m-%d")

color1 <- c("CLS10"="blue","CLS20"="red","MLE"="green","CRPS"="black")

fsr10plot <- fsr10 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS10,color = "CLS10"),size=0.25)+
  geom_line(aes(y = CLS20,color = "CLS20"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="Average Scores",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-lower tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color1)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=0.15,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=0.15,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  

# plot scores --- upper tail 
trace_fsr90 <- read.csv("data/fsr90.csv")
trace_fsr90 <- trace_fsr90 %>% rename("CLS90"=FSR90)
trace_fsr80 <- read.csv("data/fsr80.csv")
trace_fsr80 <- trace_fsr80 %>% rename("CLS80"=FSR80)
trace_90ls <- read.csv("data/fsr90_ls.csv")
trace_90crps <- read.csv("data/fsr90_crps.csv")

fsr90 <- data.frame(trace_fsr90,trace_fsr80,trace_90ls,trace_90crps)

# add back dates to the fsr10 df, initial step = 1000

fsr90 <- fsr90 %>% mutate(Date = logret$Date[1001:6000])

# plot scores --- lower tail 
fsr90$Date <- as.Date(fsr90$Date,format = "%Y-%m-%d")

color2 <- c("CLS90"="blue","CLS80"="red","MLE"="green","CRPS"="black")

fsr90plot <- fsr90 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS90,color = "CLS90"),size=0.25)+
  geom_line(aes(y = CLS80,color = "CLS80"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-upper tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color2)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.05,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=-0.05,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  
# put two plots side by side 
ggarrange(fsr10plot,fsr90plot,ncol=2,legend = "bottom",labels = c("Figure 5(a) CLS10","Figure 5(b) CLS90")) + 
  labs(caption = "Figure 5: the predictive distributions are produced by ARCH(1) with Gaussian error term") +
  theme(plot.caption = element_text(hjust = 0))

```

## 4.3 Empirical results: prediction of Value-at-Risk
### 4.3.1 Overview and preliminaries

Density forecasts have great practical value in finance. They form the foundation of risk management, such as Value-at-Risk, and are useful for asset allocation and derivative pricing. VaR is a commonly used risk measurement in finance. It can be obtained from a probability density function of returns over a chosen investment horizon. It is the *p*th quantile ($q_p$) of a predictive distribution of a portfolio’s returns over a holding period, according to a given confidence interval $\alpha$ [@dowd2007].
\begin{equation} \label{eq:var}
VaR = -q_p
\end{equation}
\begin{equation} \label{eq:p}
p=1-\alpha
\end{equation}
VaR represents the largest possible investment loss in a given investment horizon with the given $\alpha$ confidence level. There are different methods to estimate VaR, but it is clear that predictive density plays an important role here. Therefore, a more accurate probabilistic prediction can contribute to improving the accuracy of VaR prediction. 

@opschoor2017 combine density forecasts using focused scoring rules, and the results show weighted density forecasts based on optimizing the CLS outperform those based on optimizing CRPS or LS and improve the accuracy of 99% Value-at-Risk estimates. 

We illustrate how optimal forecasts perform in estimating VaR by using predictions produced through simulation (Section 3) first, and expect that optimal forecasts based on CLS would give us a better prediction in tails. Specifically, VaR predictions at \{10%, 20%, 80%, 90%\} are conducted, corresponding to the 10% and 20% expected loss of long and short portfolios. More importantly, we keep the percentage of VaR prediction the same as CLS focused tail because, as mentioned before, CLS at \{10%, 20%, 80%, 90%\} is computed from the marginal distribution due to computational burden and keeping the percentage the same reduce the error caused by this mismatch. 

We assess VaR predictive accuracy by using the VaR backtesting method. The specific steps are as below: 

1. Using the sequence of predictive densities (p) produced by $S_i$ in Section 3:

- $S_i= \{LS, CRPS, CLS10, CLS20, CLS80, CLS90\}$;

- Conditional out-of-sample predictions: $p_{S_i}(y_{1001}|y_{1000})...p_{S_i}(y_{5000}|y_{4999})$;

2. Construct the VaR at \{10%, 20%, 80%, 90%\} using predictive densities from Step 1 for each scoring rule;

3. Compare the true values of $y_{1001}...y_{5000}$ with $VaR_t$ and calculate the proportion of exceedances ($y_t < VaR_t$); 

The closer the proportion of exceedances, calculated in Step 3, to nominal VaR levels, the more accurate are VaR predictions and thus, the more accurate are the optimal probabilistic forecasts. An accurate prediction of the $p$% VaR is observed if the proportion of exceedances equals $p$%. 

Starting from the two different forms of model misspecification investigated in simulation analysis (Section 3), the values showed in the Table 7 and 8 refer to the out-of-sample proportion of exceedances over the nominal VaR level indicated by the column name. The CLS can provide the most accurate VaR prediction in the upper tail, while in the lower tail, it is not short of advantages by too much compared with CRPS. This conclusion does not vary much when we have different forms of model misspecification as illustrated in the Table 7 and 8. 

*Table 7: The predictive distributions are produced by ARCH(1) with Gaussian error while the true DGP is stochastic volatility inversion copula with the highest negative marginal skewness in our simulation. The values show the out-of-sample exceedances of each optimizer.*

```{r}
copula5var <- read.csv("data/copula5var.csv")

VaR_0.1 <- as.numeric(copula5var[,c(1,5,9,13,17,21)])
VaR_0.2 <- as.numeric(copula5var[,c(2,6,10,14,18,22)])
VaR_0.8 <- as.numeric(copula5var[,c(3,7,11,15,19,23)])
VaR_0.9 <- as.numeric(copula5var[,c(4,8,12,16,20,24)])

Optimizers <- c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

df <- data.frame(Optimizers,VaR_0.1,VaR_0.2,VaR_0.8,VaR_0.9)
df <- rename(df,"VaR at 10\\%"=VaR_0.1, "VaR at 20\\%"=VaR_0.2, "VaR at 80\\%"=VaR_0.8, "VaR at 90\\%"=VaR_0.9)

df %>% 
  mutate(
    `VaR at 10\\%` = cell_spec(format(`VaR at 10\\%`,4),"latex",bold = ifelse(abs(`VaR at 10\\%`-0.1) == min(abs(`VaR at 10\\%`-0.1)),TRUE,FALSE)),
    `VaR at 20\\%` = cell_spec(format(`VaR at 20\\%`,4),"latex",bold = ifelse(abs(`VaR at 20\\%`-0.2) == min(abs(`VaR at 20\\%`-0.2)),TRUE,FALSE)),
    `VaR at 80\\%` = cell_spec(format(`VaR at 80\\%`,4),"latex",bold = ifelse(abs(`VaR at 80\\%`-0.8) == min(abs(`VaR at 80\\%`-0.8)),TRUE,FALSE)),
    `VaR at 90\\%` = cell_spec(format(`VaR at 90\\%`,4),"latex",bold = ifelse(abs(`VaR at 90\\%`-0.9) == min(abs(`VaR at 90\\%`-0.9)),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F,booktabs=T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  add_header_above(c(" ","Out-of-sample exceedances"=4)) %>%
  add_header_above(c("Table 7: the true DGP is stochastic volatility inversion copula (shape = -5)"=5))
```

*Table 8: The predictive distributions are produced by ARCH(1) with Gaussian error and the true DGP is GARCH(1,1) with Student-t error where $t_\nu$ = 3. The values show the out-of-sample exceedances of each optimizer.*

```{r}
garch3var <- read.csv("data/garch3var.csv")

VaR0.1 <- as.numeric(garch3var[,c(1,5,9,13,17,21)])
VaR0.2 <- as.numeric(garch3var[,c(2,6,10,14,18,22)])
VaR0.8 <- as.numeric(garch3var[,c(3,7,11,15,19,23)])
VaR0.9 <- as.numeric(garch3var[,c(4,8,12,16,20,24)])

df <- data.frame(Optimizers,VaR0.1,VaR0.2,VaR0.8,VaR0.9)
df <- rename(df,"VaR at 10\\%"=VaR0.1, "VaR at 20\\%"=VaR0.2, "VaR at 80\\%"=VaR0.8, "VaR at 90\\%"=VaR0.9)

df %>% 
  mutate(
    `VaR at 10\\%` = cell_spec(format(`VaR at 10\\%`,4),"latex",bold = ifelse(abs(`VaR at 10\\%`-0.1) == min(abs(`VaR at 10\\%`-0.1)),TRUE,FALSE)),
    `VaR at 20\\%` = cell_spec(format(`VaR at 20\\%`,4),"latex",bold = ifelse(abs(`VaR at 20\\%`-0.2) == min(abs(`VaR at 20\\%`-0.2)),TRUE,FALSE)),
    `VaR at 80\\%` = cell_spec(format(`VaR at 80\\%`,4),"latex",bold = ifelse(abs(`VaR at 80\\%`-0.8) == min(abs(`VaR at 80\\%`-0.8)),TRUE,FALSE)),
    `VaR at 90\\%` = cell_spec(format(`VaR at 90\\%`,4),"latex",bold = ifelse(abs(`VaR at 90\\%`-0.9) == min(abs(`VaR at 90\\%`-0.9)),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs=T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  add_header_above(c(" ","Out-of-sample exceedances"=4)) %>%
  add_header_above(c("Table 8: the true DGP is GARCH with degree of freedom = 3"=5))
```

### 4.3.2 Empirical results: S&P 500

The preliminary results obtained from the VaR predictions using simulated data in Section 4.3.1 help us understand what to expect in empirical VaR analysis. We now implement the VaR analysis to continuously compounded daily log returns of S&P 500 in U.S.A financial market. Learning from the trace plots in Section 4.2, the 2008 GFC is the turning point of the optimal forecasts' performance, especially for the CLS. Therefore, we perform the VaR analysis over two different time periods, before the GFC and after the GFC. The results from Table 9 and 10 show that the CLS perform well enough in terms of estimating VaR for both long and short portfolios of S&P 500, but also CRPS is a very robust scoring rule which can provide relatively accurate predictions. Moreover, in Table 10, the CLS seems to perform better in 20% and 80% tails which have higher probability mass than 10% and 90% tails. If we think of the marginal distribution of S&P 500 estimated by histogram in Figure 4.B, the above results could be a consequence of the very sparse marginal distribution or simply because there is no enough data in the tails to produce an accurate estimate of the CLS optimizer, which increases the difficulty of 'focusing' on the 10% and 90% tails.

*Table 9: The estimation of VaR through out-of-sample exceedances is based on predictive distributions produced by ARCH(1) with Gaussian error. This table shows the VaR prediction of S&P 500 before the GFC.*

```{r}
spvar <- read.csv("data/spvar1_table.csv")

VaR_0.1 <- as.numeric(spvar[,c(1,5,9,13,17,21)])
VaR_0.2 <- as.numeric(spvar[,c(2,6,10,14,18,22)])
VaR_0.8 <- as.numeric(spvar[,c(3,7,11,15,19,23)])
VaR_0.9 <- as.numeric(spvar[,c(4,8,12,16,20,24)])

df <- data.frame(Optimizers,VaR_0.1,VaR_0.2,VaR_0.8,VaR_0.9)
df <- rename(df,"VaR at 10\\%"=VaR_0.1, "VaR at 20\\%"=VaR_0.2, "VaR at 80\\%"=VaR_0.8, "VaR at 90\\%"=VaR_0.9)

df %>% 
  mutate(
    `VaR at 10\\%` = cell_spec(format(`VaR at 10\\%`,4),"latex",bold = ifelse(abs(`VaR at 10\\%`-0.1) == min(abs(`VaR at 10\\%`-0.1)),TRUE,FALSE)),
    `VaR at 20\\%` = cell_spec(format(`VaR at 20\\%`,4),"latex",bold = ifelse(abs(`VaR at 20\\%`-0.2) == min(abs(`VaR at 20\\%`-0.2)),TRUE,FALSE)),
    `VaR at 80\\%` = cell_spec(format(`VaR at 80\\%`,4),"latex",bold = ifelse(abs(`VaR at 80\\%`-0.8) == min(abs(`VaR at 80\\%`-0.8)),TRUE,FALSE)),
    `VaR at 90\\%` = cell_spec(format(`VaR at 90\\%`,4),"latex",bold = ifelse(abs(`VaR at 90\\%`-0.9) == min(abs(`VaR at 90\\%`-0.9)),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs=T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  add_header_above(c(" ","Out-of-sample exceedances"=4)) %>%
  add_header_above(c("Table 9: VaR for S&P 500 before the GFC"=5))

```

*Table 10: The estimation of VaR through out-of-sample exceedances is based on predictive distributions produced by ARCH(1) with Gaussian error. This table shows the VaR prediction of S&P 500 after the GFC.*

```{r}
spvar <- read.csv("data/spvar2_table.csv")
spvar <- round(spvar,digits = 4)

VaR_0.1 <- as.numeric(spvar[,c(1,5,9,13,17,21)])
VaR_0.2 <- as.numeric(spvar[,c(2,6,10,14,18,22)])
VaR_0.8 <- as.numeric(spvar[,c(3,7,11,15,19,23)])
VaR_0.9 <- as.numeric(spvar[,c(4,8,12,16,20,24)])

df <- data.frame(Optimizers,VaR_0.1,VaR_0.2,VaR_0.8,VaR_0.9)
df <- rename(df,"VaR at 10\\%"=VaR_0.1, "VaR at 20\\%"=VaR_0.2, "VaR at 80\\%"=VaR_0.8, "VaR at 90\\%"=VaR_0.9)

df %>% 
  mutate(
    `VaR at 10\\%` = cell_spec(format(`VaR at 10\\%`,4),"latex",bold = ifelse(abs(`VaR at 10\\%`-0.1) == min(abs(`VaR at 10\\%`-0.1)),TRUE,FALSE)),
    `VaR at 20\\%` = cell_spec(format(`VaR at 20\\%`,4),"latex",bold = ifelse(abs(`VaR at 20\\%`-0.2) == min(abs(`VaR at 20\\%`-0.2)),TRUE,FALSE)),
    `VaR at 80\\%` = cell_spec(format(`VaR at 80\\%`,4),"latex",bold = ifelse(abs(`VaR at 80\\%`-0.8) == min(abs(`VaR at 80\\%`-0.8)),TRUE,FALSE)),
    `VaR at 90\\%` = cell_spec(format(`VaR at 90\\%`,4),"latex",bold = ifelse(abs(`VaR at 90\\%`-0.9) == min(abs(`VaR at 90\\%`-0.9)),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs=T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  add_header_above(c(" ","Out-of-sample exceedances"=4)) %>%
  add_header_above(c("Table 10: VaR for S&P 500 after the GFC"=5))

# formattable(df,list(VaR_0.1 = max_bold1,VaR_0.2 = max_bold2,VaR_0.8 = max_bold8,VaR_0.9 = max_bold9))
```

# 5. Empirical analysis: VIX
## 5.1 Background and notation 

Time-varying volatility is an important part in finance modelling and an accurate prediction will give investors a better understanding of the risks they take on. GARCH-type models used in Section 3 and 4 are conditionally deterministic. That is, $\sigma_t^2$ is a deterministic function of given past returns. 
\begin{equation} \label{eq:ret}
r_t=ln(\frac{P_t}{P_{t-1}})
\end{equation}
\begin{equation} \label{eq:reg}
r_t = \mu+\sigma_t\epsilon_t
\end{equation}
\begin{equation} \label{eq:garch}
\sigma_t^2=\alpha_0+\alpha_1(r_{t-1}-\mu)^2+\beta_1\sigma_{t-1}^2
\end{equation}
where $P_t$ is the stock price and $\sigma_t$ is the volatility. 

GARCH models also neglect the fact that volatility in stock markets and derivative markets can have long memory, which means it slowly reverts to its long-run mean. They record changes usually in daily frequency and do not exploit the information in intraday data. Therefore, it is better off if we model volatility in a continuous-time model where $lnP_t$ and $\sigma_t$ are allowed to vary continuously over time. 

The realized volatility (RV) approach to modelling volatility allows us to exploit the information in intraday data, consider long memory feature, sudden jumps in the market and market microstructure. When the price follows the process as $\eqref{eq:gbm}$:
\begin{equation} \label{eq:gbm}
d\ln(P_t)=\mu_tdt+\sigma_tdw_t
\end{equation}
RV is a direct estimate of integrated volatility (IV) for $lnP_t$ based on continuously recorded observations of $P_t$ over the day.
\begin{equation} \label{eq:RV}
RV_t = \sum^{1/\Delta t}_{j=1}r^2_{t-1+j\Delta t} \xrightarrow{p} \int_{t-1}^t\sigma_s^2ds = IV_t \ as\ \Delta t \to 0
\end{equation}
When considering the sudden jumps, that is, the price follows the process in $\eqref{eq:jump}$. RV is also a consistent estimate of quadratic variation (QV) where $\kappa_s$ represents the sudden jump on day t and $q_t$ is the jump occasions.  
\begin{equation} \label{eq:jump}
d\ln(P_t)=\mu_tdt+\sigma_tdw_t+\kappa_tdq_t
\end{equation}
\begin{equation} \label{eq:RV2}
RV_t=\sum^{1/\Delta t}_{j=1}r^2_{t-1+j\Delta t} \xrightarrow{p} \int_{t-1}^t\sigma_s^2ds\ + \sum_{s=1}^{q_t}\kappa_s^2= QV_t=IV_t+\sum_{s=1}^{q_t}\kappa_s^2 \ as\ \Delta t \to 0
\end{equation}
In addition to the fact that volatility is a direct measure of the risk of portfolios, it is also important for pricing derivatives, such as options, since it is the only unknown parameter to estimate. The famous Black-Scholes model assumes a constant volatility over time which does not hold in practice. Implied volatility allows us to incorporate the jumps and continuous variation of volatility over time. In other words, implied volatility is an estimate of QV extracted from option prices. 

The VIX index estimates QV that is implied by option prices under a risk-neutral $\eqref{eq:jump}$ process, using a finite number of strike prices of S&P 500 index [@cboe]. It acknowledges the occurrence of jumps and the fact that assumptions of the Black-Scholes model do not hold in practice. Since both the VIX and RV are estimations of QV, it is reasonable to model the VIX with a HAR-RV model which is usually used for modelling RV in the literature [@andersen2007; @corsi2009; @m2009does; @manee2012probabilistic].

## 5.2 Preliminary diagnostics
We collect daily VIX index data starting from 27 Aug,1996 to 30 July,2020.^[Data retrieved from @vixdata] The descriptive statistics of $log(VIX_t)$ are shown in Table 11 and Figure 6: 

Table 11 and Figure 6 show that $log(VIX_t)$ is volatile and shows strong persistence over time. It is positively skewed and the autocorrelation of its volatility is slowly decaying. From the descriptive statistics, we can see that the features of the VIX is similar to what we have observed from S&P 500 log returns. Similarly, there are unusual jumps happened during the GFC and the COVID-19. Thus, we expect that the CLS could outperform MLE and CRPS in predicting tails as in Section 4.2 and significantly improve accuracy after the GFC. 

```{r,message=F}
# data cleaning and import 
vix <- read.csv("data/^VIX.csv")
vix <- vix %>% dplyr::select("Date","Adj.Close")

vix$logvix <- log(vix$Adj.Close)

# previous day log vix values
prev <- matrix(0,nrow = nrow(vix)+1,ncol = 1)
for (i in 1:nrow(vix)){
  prev[1,1] <- c("NA")
  prev[i+1,1] <- vix$logvix[i]
}
prev <- as.data.frame(prev)
prev <- prev[-c(1:188,6189),1]

# need to be very careful about the time index -- dependent variable starting with t+1
MA5 <- matrix(0,nrow=nrow(vix)-5,ncol = 1)
for (i in 1:(nrow(vix)-5)){
  sum = 0
  for (j in 1:5){
    sum = sum + vix$logvix[i+j-1]
  }
  MA5[i,1] = (1/5)*sum
}

MA22 <- matrix(0,nrow = nrow(vix)-22,ncol = 1)
for (i in 1:(nrow(vix)-22)){
  sum = 0
  for (j in 1:22){
    sum = sum + vix$logvix[i+j-1]
  }
  MA22[i,1] = (1/22)*sum
}


#require(zoo)
#MA5 <- rollmean(vix$logvix,5,align = "center")
MA5 <- as.data.frame(MA5) # average of past week
MA5 <- MA5[-c(1:183),1] # including only 6000 obs 

#MA22 <- rollmean(vix$logvix,22,align = "center")
MA22 <- as.data.frame(MA22) # average of past month
MA22 <- MA22[-c(1:166),1] # including only 6000 obs

logvix <- vix[-c(1:188),c(1,3)] # delete the adjusted vix col, only keep 6000 obs
logvix <- cbind(logvix,prev,MA5,MA22)
# export the dataset to csv. including only trading days and log returns

temp <- logvix[,2:5]
write_csv(temp,"vixlog.csv")

# the logvix, previous log vix and moving average values have the same length in the final file. 
```


```{r, results='hide'}
# descriptive statistics 
summary <- summary(vix$logvix) # ----> need to be tabulated 
# range(vix$logvix)
# some test results aginst normal dist h_null
# JB test
jb.norm.test(vix$logvix)
# Ljung box test
Box.test(vix$logvix,lag = 2,type = "Lj",fitdf = 1)

vixsk <- skewness(vix$logvix)
vixkt <- kurtosis(vix$logvix)
```


```{r}
sum <- data.frame(Series="Log(VIX)",Min = as.numeric(summary[1]), Median = as.numeric(summary[3]), Mean = as.numeric(summary[4]), Max = as.numeric(summary[6]),Skewness= vixsk,Kurtosis=vixkt,JB.Test=410.77,LB.Test=11765)
sum %>% 
  kable("latex",booktabs = T) %>% 
  kable_styling("bordered",font_size = 10)%>% 
  add_header_above(c("Table 11: Descriptive statistics"=9))
```


```{r,message=FALSE,fig.width=10,fig.height=5}

# time series plot
logvix$Date <- as.Date(logvix$Date,format = "%Y-%m-%d")

p1 <- logvix %>% ggplot(aes(x = Date, y = logvix)) +
  geom_line()+theme_classic()+labs(x="Year",y="Daily log(VIX) values")+
  #ggtitle("Figure ?: Daily values for Log(VIX)")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  #scale_y_continuous(labels = scales::percent) + 
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=2,label="Global Financial Crisis",color="red",size=3)+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=2,label="COVID-19",color="red",size=3)+
  theme_clean()

# histogram plots
p2 <- logvix %>% ggplot()+geom_histogram(aes(x=logvix))+
  theme_clean()

# ACF plots to see the autocorrelation 
logvix <- logvix %>% 
  mutate(trading_day = row_number()) %>%
  as_tsibble(index = trading_day,regular = TRUE)

p3 <- logvix %>% ACF((logvix)^2) %>% autoplot()+
      xlab("number of lags")+ylab("ACF")+theme_clean()

figure <- ggarrange(p1,                                                 
          ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
          nrow = 2, 
          labels = "A"                                        
          ) 
annotate_figure(figure, 
                top = text_grob("Descriptive figures",face = "bold"),
                fig.lab = "Figure 6",fig.lab.face = "bold"
                )
```

## 5.3 Model specification

Proposed by @corsi2009, the HAR-RV model is a simple additive linear model which takes lagged squared returns as regressors. It does not belong to the class of long memory models, but it is able to produce the volatility persistence that is almost indistinguishable from what is observed in financial markets through the simple autoregressive-type structure.  

We design the VIX analysis with different error term specifications of the HAR-RV model: 

1. $z_{t+1} \sim N(0,1)$ and constant $\sigma$  

2. $z_{t+1} \sim Student-t(0,1,\nu)$ and constant $\sigma$ ^[We specify $z_{t+1}$ as standardised Student-t distribution so that $\sigma^2$ can reflect the variance of the predicted VIX series. The CRPS is removed in Specification 2 and 4, since there is no closed form for this score. @crpst2016 provides a way to evaluate CRPS for Student-t distribution numerically.]

3. $z_{t+1} \sim N(0,1)$ and time-varying $\sigma_{t+1}$ following ARCH process

4. $z_{t+1} \sim Student-t(0,1,\nu)$ and time-varying $\sigma_{t+1}$ following ARCH process
^[Specification 1 and 2 are based on equations $\eqref{eq:HAR}$-$\eqref{eq:ma22}$; Specification 3 and 4 are based on equations $\eqref{eq:ma22}$-$\eqref{eq:arch}$]

\begin{equation} \label{eq:HAR}
log(VIX_{t+1})=\beta_0+\beta_1log(VIX_t)+\beta_2log(VIX_{t-5,t})+\beta_3log(VIX_{t-22,t})+\sigma z_{t+1}
\end{equation}
\begin{equation} \label{eq:ma5}
log(VIX_{t-5,t})=\frac{1}{5}[log(VIX_t)+...+log(VIX_{t-4})]
\end{equation}
\begin{equation} \label{eq:ma22}
log(VIX_{t-22,t})=\frac{1}{22}[log(VIX_t)+...+log(VIX_{t-21})]
\end{equation}

\begin{equation} \label{eq:HAR2}
log(VIX_{t+1})=\beta_0+\beta_1log(VIX_t)+\beta_2log(VIX_{t-5,t})+\beta_3log(VIX_{t-22,t})+\sigma_{t+1}z_{t+1}
\end{equation}
\begin{equation} \label{eq:arch}
\sigma_{t+1}^2=\alpha_0+\alpha_1e^2_t
\end{equation}
By using the same expanding window design in Section 3 with initial sample size = 1000 and out-of-sample size $\tau=T-1000=5000$, we produce and evaluate predictive densities with the assumed model as HAR-RV. The results are shown in the following sections. 

## 5.4 Empirical results 
### 5.4.1 Results of specification 1

Section 5.4.1 provides the result of Specification 1 where the predictive density of the VIX is the normal distribution with a constant variance. In Table 12, CLS performs better in predicting the upper tail as expected and does a relatively good job in predicting lower tail as well, even though CRPS ranks the best in terms of forecasting 20 percentile of the distribution. The score trace plot, Figure 7, shows the predictions of optimizing four scoring rules evaluated by CLS10 (left panel) and CLS90 (right panel). It indicates how accurate these predictions are, according to their performance in 10 percentile and 90 percentile. In the lower tail, it seems that there is not much difference among four predictions, but zooming in, we can see that CLS is at the top of other scores, which means it can still provide the most accurate prediction. However, we only gain very little from using it. On the contrary, in the upper tail, CLS has a much bigger effect on improving accuracy after the GFC. One possible explanation could be that the HAR model is more misspecified in the upper tail than in the lower tail. Thus, we benefit more from CLS optimal forecasts in the upper tail. 

*Table 12: The predictive distributions for VIX are produced by HAR-RV model with Gaussian error and constant volatility.*

```{r}
vix_table <- read.csv("data/vix_table.csv")
vix_table <- rename(vix_table, "In-sample optimizers" = Row,"CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
vix_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

vix_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 12: Average scores for the VIX with Gaussian error"=7))
```

```{r,fig.width=10,fig.height=5}
# to compress all the calculated plotting average scores together in the same df 
# this the accuracy assessed by fsr10
trace_fsr10 <- read.csv("data/vixfsr10.csv")
trace_fsr10 <- trace_fsr10 %>% rename("CLS10"=FSR10)
trace_fsr20 <- read.csv("data/vixfsr20.csv")
trace_fsr20 <- trace_fsr20 %>% rename("CLS20"=FSR20)
trace_10ls <- read.csv("data/vixfsr10_ls.csv")
trace_10crps <- read.csv("data/vixfsr10_crps.csv")

fsr10 <- data.frame(trace_fsr10,trace_fsr20,trace_10ls,trace_10crps)

# add back dates to the fsr10 df, initial step = 1000

fsr10 <- fsr10 %>% mutate(Date = vix$Date[1001:6000])

# plot scores --- lower tail 
fsr10$Date <- as.Date(fsr10$Date,format = "%Y-%m-%d")

color1 <- c("CLS10"="blue","CLS20"="red","MLE"="green","CRPS"="black")

fsr10plot <- fsr10 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS10,color = "CLS10"),size=0.25)+
  geom_line(aes(y = CLS20,color = "CLS20"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="Average Scores",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-lower tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color1)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=0.08,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=0.08,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  

# plot scores --- upper tail (assessed by fsr90)
trace_fsr90 <- read.csv("data/vixfsr90.csv")
trace_fsr90 <- trace_fsr90 %>% rename("CLS90"=FSR90)
trace_fsr80 <- read.csv("data/vixfsr80.csv")
trace_fsr80 <- trace_fsr80 %>% rename("CLS80"=FSR80)
trace_90ls <- read.csv("data/vixfsr90_ls.csv")
trace_90crps <- read.csv("data/vixfsr90_crps.csv")

fsr90 <- data.frame(trace_fsr90,trace_fsr80,trace_90ls,trace_90crps)

# add back dates to the fsr10 df, initial step = 1000

fsr90 <- fsr90 %>% mutate(Date = vix$Date[1001:6000])

# plot scores --- lower tail 
fsr90$Date <- as.Date(fsr90$Date,format = "%Y-%m-%d")

color2 <- c("CLS90"="blue","CLS80"="red","MLE"="green","CRPS"="black")

fsr90plot <- fsr90 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS90,color = "CLS90"),size=0.25)+
  geom_line(aes(y = CLS80,color = "CLS80"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-upper tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color2)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.05,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=-0.05,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  
# put two plots side by side 
ggarrange(fsr10plot,fsr90plot,ncol=2,legend = "bottom",labels = c("Figure 7(a) CLS10","Figure 7(b) CLS90"))+ 
  labs(caption = "Figure 7: the predictive distributions are produced by HAR-RV model with Gaussian error and constant volatility") +
  theme(plot.caption = element_text(hjust = 0))

```

### 5.4.2 Results of specification 2

The numerical results in Section 3 show that the more misspecified the model is, the more benefits are gained in improving prediction accuracy. According to the marginal distribution estimated by histogram in Figure 6.B, the conditional prediction distribution of the VIX is likely to be positively skewed and has fatter tails than the normal distribution. Moving from Specification 1 to Specification 2, fatter tails are allowed in the predictive distribution and thus, the assumed model is less misspecified. Table 13 and Figure 8 show consistent performance of optimal forecasts compared with our investigations of simulated data and S&P 500 returns. That is, improvement of prediction accuracy by using CLS is still observed in the upper tail but the amount of improvement decreases as the degree of misspecification decreases. 

*Table 13: The predictive distributions for VIX are produced by HAR-RV model with Student-t error and constant volatility.*

```{r}
vix_table <- read.csv("data/vix_table_st.csv")
vix_table <- rename(vix_table, "In-sample optimizers" = Row,"CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
vix_table$`In-sample optimizers`<-c("LS","CLS10","CLS20","CLS80","CLS90")

vix_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    #CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=5)) %>% 
  add_header_above(c("Table 13: Average scores for the VIX with Student-t error"=6))
```

```{r,fig.width=10,fig.height=5}
# to compress all the calculated plotting average scores together in the same df 
trace_fsr10 <- read.csv("data/vixfsr10_st.csv")
trace_fsr10 <- trace_fsr10 %>% rename("CLS10"=FSR10)
trace_fsr20 <- read.csv("data/vixfsr20_st.csv")
trace_fsr20 <- trace_fsr20 %>% rename("CLS20"=FSR20)
trace_10ls <- read.csv("data/vixfsr10_ls_st.csv")
#trace_10crps <- read.csv("vixfsr10_crps.csv")

fsr10 <- data.frame(trace_fsr10,trace_fsr20,trace_10ls)

# add back dates to the fsr10 df, initial step = 1000

fsr10 <- fsr10 %>% mutate(Date = vix$Date[1001:6000])

# plot scores --- lower tail 
fsr10$Date <- as.Date(fsr10$Date,format = "%Y-%m-%d")

color1 <- c("CLS10"="blue","CLS20"="red","MLE"="green")

fsr10plot <- fsr10 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS10,color = "CLS10"),size=0.25)+
  geom_line(aes(y = CLS20,color = "CLS20"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
 # geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="Average Scores",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-lower tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color1)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=0.08,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=0.08,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  

# plot scores --- upper tail 
trace_fsr90 <- read.csv("data/vixfsr90_st.csv")
trace_fsr90 <- trace_fsr90 %>% rename("CLS90"=FSR90)
trace_fsr80 <- read.csv("data/vixfsr80_st.csv")
trace_fsr80 <- trace_fsr80 %>% rename("CLS80"=FSR80)
trace_90ls <- read.csv("data/vixfsr90_ls_st.csv")
#trace_90crps <- read.csv("vixfsr90_crps_st.csv")

fsr90 <- data.frame(trace_fsr90,trace_fsr80,trace_90ls)

# add back dates to the fsr10 df, initial step = 1000

fsr90 <- fsr90 %>% mutate(Date = vix$Date[1001:6000])

# plot scores --- lower tail 
fsr90$Date <- as.Date(fsr90$Date,format = "%Y-%m-%d")

color2 <- c("CLS90"="blue","CLS80"="red","MLE"="green")

fsr90plot <- fsr90 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS90,color = "CLS90"),size=0.25)+
  geom_line(aes(y = CLS80,color = "CLS80"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  # geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-upper tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color2)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.05,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=-0.05,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  
# put two plots side by side 
ggarrange(fsr10plot,fsr90plot,ncol=2,legend = "bottom",labels = c("Figure 8(a) CLS10","Figure 8(b) CLS90"))+ 
  labs(caption = "Figure 8: the predictive distributions are produced by HAR-RV model with Student-t error and constant volatility") +
  theme(plot.caption = element_text(hjust = 0))
```

### 5.4.3 Results of specification 3 & 4: 

Specification 3 and 4 move from the constant variance of the predictive distribution to the time-varying variance, which is better specified since strong serial correlation of the volatility is observed in the VIX series as shown in Figure 6.C. Table 14 and Figure 9 illustrate the performance of optimal forecasts in Specification 3. The amount of improvement decreased very slightly, compared with results of Specification 1 which also specifies a Gaussian error term. Similar conclusions can be drawn from comparing Table 15 and Figure 10 with the results of Specification 2. However, overall, as the model becomes less misspecified, we gain less from optimal forecasts in predicting the upper tail of VIX. 

*Table 14: The predictive distributions for VIX are produced by HAR-RV model with Gaussian error and time-varying volatility which is modelled by ARCH(1).*

```{r}
vix_table <- read.csv("data/vix_table_garch.csv")
vix_table <- rename(vix_table, "In-sample optimizers" = Row,"CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
vix_table$`In-sample optimizers`<-c("LS","CRPS","CLS10","CLS20","CLS80","CLS90")

vix_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 14: Average scores for the VIX with ARCH Gaussian error"=7))
```

```{r,fig.width=10,fig.height=5}
# to compress all the calculated plotting average scores together in the same df 
trace_fsr10 <- read.csv("data/vixfsr10_garch.csv")
trace_fsr10 <- trace_fsr10 %>% rename("CLS10"=FSR10)
trace_fsr20 <- read.csv("data/vixfsr20_garch.csv")
trace_fsr20 <- trace_fsr20 %>% rename("CLS20"=FSR20)
trace_10ls <- read.csv("data/vixfsr10_ls_gar.csv")
trace_10crps <- read.csv("data/vixfsr10_crps_gar.csv")

fsr10 <- data.frame(trace_fsr10,trace_fsr20,trace_10ls,trace_10crps)

# add back dates to the fsr10 df, initial step = 1000

fsr10 <- fsr10 %>% mutate(Date = vix$Date[1001:6000])

# plot scores --- lower tail 
fsr10$Date <- as.Date(fsr10$Date,format = "%Y-%m-%d")

color1 <- c("CLS10"="blue","CLS20"="red","MLE"="green","CRPS"="black")

fsr10plot <- fsr10 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS10,color = "CLS10"),size=0.25)+
  geom_line(aes(y = CLS20,color = "CLS20"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="Average Scores",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-lower tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color1)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=0.08,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=0.08,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  

# plot scores --- upper tail 
trace_fsr90 <- read.csv("data/vixfsr90_gar.csv")
trace_fsr90 <- trace_fsr90 %>% rename("CLS90"=FSR90)
trace_fsr80 <- read.csv("data/vixfsr80_gar.csv")
trace_fsr80 <- trace_fsr80 %>% rename("CLS80"=FSR80)
trace_90ls <- read.csv("data/vixfsr90_ls_gar.csv")
trace_90crps <- read.csv("data/vixfsr90_crps_gar.csv")

fsr90 <- data.frame(trace_fsr90,trace_fsr80,trace_90ls,trace_90crps)

# add back dates to the fsr10 df, initial step = 1000

fsr90 <- fsr90 %>% mutate(Date = vix$Date[1001:6000])

# plot scores --- lower tail 
fsr90$Date <- as.Date(fsr90$Date,format = "%Y-%m-%d")

color2 <- c("CLS90"="blue","CLS80"="red","MLE"="green","CRPS"="black")

fsr90plot <- fsr90 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS90,color = "CLS90"),size=0.25)+
  geom_line(aes(y = CLS80,color = "CLS80"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-upper tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color2)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.05,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=-0.05,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  
# put two plots side by side 
ggarrange(fsr10plot,fsr90plot,ncol=2,legend = "bottom",labels = c("Figure 9(a) CLS10","Figure 9(b) CLS90"))+ 
  labs(caption = "Figure 9: the predictive distributions are produced by HAR-RV model with Gaussian error and time-varying volatility") +
  theme(plot.caption = element_text(hjust = 0))

```

*Table 15: The predictive distributions for VIX are produced by HAR-RV model with Student-t error and time-varying volatility which is modelled by ARCH(1).*

```{r}
vix_table <- read.csv("data/vix_table_gst.csv")
vix_table <- rename(vix_table, "In-sample optimizers" = Row,"CLS10" = FSR10, "CLS20"=FSR20, "CLS80"=FSR80,"CLS90"=FSR90)
vix_table$`In-sample optimizers`<-c("LS","CLS10","CLS20","CLS80","CLS90")

vix_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    #CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `CLS10` = cell_spec(format(`CLS10`,4),"latex",bold = ifelse(`CLS10`==max(`CLS10`),TRUE,FALSE)),
    `CLS20` = cell_spec(format(`CLS20`,4),"latex",bold = ifelse(`CLS20`==max(`CLS20`),TRUE,FALSE)),
    `CLS80` = cell_spec(format(`CLS80`,4),"latex",bold = ifelse(`CLS80`==max(`CLS80`),TRUE,FALSE)),
    `CLS90` = cell_spec(format(`CLS90`,4),"latex",bold = ifelse(`CLS90`==max(`CLS90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=5)) %>% 
  add_header_above(c("Table 15: Average scores for the VIX with ARCH Student-t error"=6))
```

```{r,fig.width=10,fig.height=5}
# to compress all the calculated plotting average scores together in the same df 
trace_fsr10 <- read.csv("data/vixfsr10_gst.csv")
trace_fsr10 <- trace_fsr10 %>% rename("CLS10"=FSR10)
trace_fsr20 <- read.csv("data/vixfsr20_gst.csv")
trace_fsr20 <- trace_fsr20 %>% rename("CLS20"=FSR20)
trace_10ls <- read.csv("data/vixfsr10_ls_gst.csv")
#trace_10crps <- read.csv("vixfsr10_crps.csv")

fsr10 <- data.frame(trace_fsr10,trace_fsr20,trace_10ls)

# add back dates to the fsr10 df, initial step = 1000

fsr10 <- fsr10 %>% mutate(Date = vix$Date[1001:6000])

# plot scores --- lower tail 
fsr10$Date <- as.Date(fsr10$Date,format = "%Y-%m-%d")

color1 <- c("CLS10"="blue","CLS20"="red","MLE"="green")

fsr10plot <- fsr10 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS10,color = "CLS10"),size=0.25)+
  geom_line(aes(y = CLS20,color = "CLS20"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  #geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="Average Scores",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-lower tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color1)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=0.08,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=0.08,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  

# plot scores --- upper tail 
trace_fsr90 <- read.csv("data/vixfsr90_gst.csv")
trace_fsr90 <- trace_fsr90 %>% rename("CLS90"=FSR90)
trace_fsr80 <- read.csv("data/vixfsr80_gst.csv")
trace_fsr80 <- trace_fsr80 %>% rename("CLS80"=FSR80)
trace_90ls <- read.csv("data/vixfsr90_ls_gst.csv")
#trace_90crps <- read.csv("vixfsr90_crps.csv")

fsr90 <- data.frame(trace_fsr90,trace_fsr80,trace_90ls)

# add back dates to the fsr10 df, initial step = 1000

fsr90 <- fsr90 %>% mutate(Date = vix$Date[1001:6000])

# plot scores --- lower tail 
fsr90$Date <- as.Date(fsr90$Date,format = "%Y-%m-%d")

color2 <- c("CLS90"="blue","CLS80"="red","MLE"="green")

fsr90plot <- fsr90 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = CLS90,color = "CLS90"),size=0.25)+
  geom_line(aes(y = CLS80,color = "CLS80"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  #geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-upper tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color2)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.05,label="Global Financial Crisis",color="darkred")+
  annotate(geom = 'text',x=as.Date("2020-03-11"),y=-0.05,label="COVID-19",color="darkred",size = 2)+
  theme_clean()
  
# put two plots side by side 
ggarrange(fsr10plot,fsr90plot,ncol=2,legend = "bottom",labels = c("Figure 10(a) CLS10","Figure 10(b) CLS90"))+ 
  labs(caption = "Figure 10: predictive distributions are produced by HAR-RV model with Student-t error and time-varying volatility") +
  theme(plot.caption = element_text(hjust = 0))

```

Comparing the results obtained in VIX prediction with the results in returns prediction, it indicates that the amount of improvement of prediction accuracy might mainly depend on how the shape of predictive density matches the shape of true density or how misspecified the assumed predictive model is in terms of predicting the tails. 

Even though optimal forecasts can only reap significant benefits in the upper tail of the VIX prediction, there is still great practical value in finance. Firstly, by definition, high values of the VIX indicates that the S&P 500, or the stock market in general, is expected to be more volatile (high volatility), which is an opportunity for either long or short position investors to gain profits or hedge risks, since no one can make money if the stock price doesn’t change at all. Besides, investors are typically risk averse, which makes the higher risk (the upper tail) is more of an interest to them, compared with the lower risk (the lower tail). More importantly, volatility is treated as an asset class and market participants are provided with liquid volatility products to trade in the market. For example, @cboefutures introduces futures contracts written on the VIX Index and a better prediction of VIX could be helpful for better futures pricing. Besides, there is literature discussing VIX futures and trading strategies based on different volatility forecasting techniques like in @vixfutures2020. Our results will make a contribution to the literature in this field as well. 

# 6. Conclusions 

According to the numerical and empirical results, the benefits of optimal forecasts can be observed when we have model misspecification, subject to the assumed predictive model is compatible, and their performance does not vary between two different types of model misspecification investigated in this paper. By mainly focusing on investigating the tail performance of optimal forecasts, the results show that the more misspecified the model is, the more benefits can be gained from using the CLS in improving the tail prediction accuracy in comparison with the conventional likelihood-based counterparts. This result is particularly in evidence after the stock market turbulence in both financial returns and the VIX index. What is interesting is that significant improvement of prediction accuracy is observed in both lower and upper tails of S&P 500 log returns but in the VIX index, it only shows up in the upper tail. 

There are some caveats as well. Firstly, the predictions conducted by CLS is based on the tail regions of the marginal distribution, while VaR is calculated from the conditional predictive distribution which is updated through each iteration. Therefore, there is a slight mismatch in distributions when predicting VaR. Secondly, the practical usage of VaR is often around 1% or 5%, but at this very small tail, CLS might be less accurate than predicting 10% or 20% tail, since there are less observations in the tail and consequently, harder to focus correctly. However, this drawback might be corrected by changing to a more suitable assumed predictive model which provides a heavy tail distribution. 
As mentioned above, the difference between optimal forecasts’ performance in S&P 500 log returns and in the VIX Index is hard to interpret.  The performance of optimal forecasts depends on the true conditional predictive distribution, which is always unknown in practice. Even though the marginal distributions can provide some insights about the shape of true conditional predictive distribution, it still creates a problem for determining which tail could optimal forecasts reap significant benefits. More importantly, the reasons why we can gain considerably from optimal forecasts after the GFC needs further investigation. 
Nevertheless, the results in this paper provide important insights to the financial forecasting literature. In terms of financial returns, optimal forecasts can give better VaR predictions when it comes to the VIX index, the improvement of predictive accuracy in the upper tail - which indicates that higher volatility in the market is more accurately predicted - is an important contribution to literature on hedging risks and trading derivatives in financial market. 

