---
compact-title: no
editor_options:
  chunk_output_type: console
  header-includes: usepackage{amsmath}
output:
  pdf_document:
    fig_height: 3.5
    fig_width: 4.5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE,
  warning = FALSE)
```


```{r}
knitr::include_graphics("draft cover page.pdf")
```

\pagebreak

```{r, message=FALSE}
library(formattable)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(tsibble)
library(tsibbledata)
library(feasts)
library(normtest)
library(moments)
library(ggthemes)
library(ggpubr)
library(knitr)
library(kableExtra)
```

# Abstract 

\pagebreak

# 1. Introduction

# 2. Scoring rule in prediction
## 2.1 Overview and notation 

Scoring rules can be used in both estimation and prediction. In estimation, an optimum estimator $\hat \theta$ can be obtained by maximizing a scoring rule $S_n$. That is, $\hat \theta_n=arg \max_\theta S_n(\theta)$. $\hat \theta \to \theta$ as $n \to \infty$ under asymptotic situations. While in prediction, scoring rules assign a numerical value to density forecasts, addressing both sharpness and calibration. For a positively oriented score, higher value will be assigned to a better forecast between two competing candidates. A variety of proper scoring rules allows users to produce probability predictions based on their interests, and to compare these predictions, as long as they refer to the same forecast quantity. Forecasts candidates are often compared by their average scores. Therefore, the crucial importance of the propreity of scoring rules must be emphasized.Gneiting & Raftery, (2007) illustrate the importance of propreity via a case study in weather forecasts and point out that improper scoring rules will assign higher average score to an incorrect density forecast. A scoring rule is said to be proper if $S(Q,Q) \geq S(P,Q)$ for all P and Q, and it is strictly proper if $S(Q,Q) = S(P,Q)$ only happens when P = Q. P and Q are predictive distributions and Q is the best forecast given all the available information. S(P,Q) means the expected value of S(P,$\cdot$) under Q (Gneiting & Raftery, 2007). 

Some commonly used proper scoring rules are the logarithmic score (LS), the continuously ranked probability score (CRPS) and censored likelihood score (CSL) or focused score (FSR) in a sense that CSL is used for focusing on a certain region(s) of densities. 

The logarithmic score is defined as (1) where $p_t$ is the density forecasts. It is a local strictly proper scoring rule, which means it will assign a higher score to the correct probabilistic forecast, and it is superior to quadratic and spherical scoring rules when the rank ordering is important or the impact of nonlinear utility function used is a concern for forecasters (Bickel, 2007). 
\begin{align}
S_{LS}(p_t, y_{t+1}) = log[p_t(y_{t+1})]
\end{align}
However, logarithmic score is criticized by its unboundness and its local property. Its useness is also questioned in (Bernardo, 1979), since he shows that every proper scoring rule is equivalent to logarithmic score. Gneiting & Raftery, (2007) argue that logarithmic score is insensitive to distance and will not reward predictions that are close to but not identical to the materialized one. Therefore, Gneiting & Raftery, (2007) propose the continuously ranked probability score, which is sensitive to distance and is defined as in (2) where P is the cumulative distriubtion function, I is the indication function, and x is the materialized events. 

\begin{align}
CRPS(P,x)=-\int_{-\infty}^{\infty}(P(y)-I(y\geq x))^2dy
\end{align}

The formula can be simplifed to (3) if the prediction distribution is Gaussian with mean = $\mu$ and variance = $\sigma^2$.
\begin{align}
CRPS(N,x)=\sigma[\frac{1}{\sqrt{\pi}}-2\phi(\frac{x-\mu}{\sigma})-\frac{x-\mu}{\sigma}(2\Phi(\frac{x-\mu}{\sigma})-1)]
\end{align}
where $\phi$ and $\Phi$ are the probability density function and cumulative density function of the Gaussian predictive distribution.

Continuously ranked probability score is defined as a negatively oriented scoring rule, but it can be easily transformed to positively oriented score for convenient comparison among scoring rules as shown in (4). 

\begin{align}
CRPS^*(N,x)=\sigma[-\frac{1}{\sqrt{\pi}}+2\phi(\frac{x-\mu}{\sigma})+\frac{x-\mu}{\sigma}(2\Phi(\frac{x-\mu}{\sigma})-1)]
\end{align}

Both logarithmic score and continuously ranked probability score are used for producing and evaluating the entire predictive density. In terms of accurately predicting a certain region of a distribution, they are typically used together with weighted likelihood (Gneiting & Ranjan, 2011). Diks, Panchenko & van Dijk, (2011) propose a new proper scoring rule, censored likelihood score, which allows users to assess the forecasts only in the interested region(s) instead of using weights to emphasize a particular part of the entire density forecast. It is defined as in (5) where $p_t$ is the predictive probability density function, $A_t$ is the interested region(s) and $A_t^c$ is the complement of $A_t$.

\begin{align}
S_{CSL}(p_t, y_{t+1})=I(y_{t+1} \in A_t)log[ p_t(y_{t+1})]+I(y_{t+1} \in A_t^c)log[\int_{A_t^c} p_t(s)ds]
\end{align}

[ref 5] prove that weighted density forecasts based on optimizing the censored likelihood score outperform those on logarithmic score and continuously ranked probability score. 

## 2.2 Illustration example 
### 2.2.1 Simulation overview 

Density forecasts provide more complete information of uncertainty of a prediction than point and interval forecasts. Moreover, the tails of a density have a great importance in finance. It forms the foundation of risk management, such as Value-at-Risk (VaR), and asset allocation. It can also be used in derivatives pricing. (reference ?) --- find some papers. 

[ref 5] provide empirical results showing that using censored likelihood score can improve 99% Value-at-Risk estimation. Moreover, Loaiza-Maya, Martin & Frazier, (2019) propose a new method, Focused Bayes, in Bayeisian prediction by replacing the conventional likelihood function with the censored likelihood score. Their results show that Focused Bayes outperform exact Bayes which is using logarithmic score for updating the prior probability. Their work also points out the degree of model misspecification play a role in the performance of Focused Bayes. 

Loaiza-Maya et al., (2020) extend the above discussion to frequentist probability forecasting. They address questions about when we can/cannot benefit from optimal probabilistic forecasts out of sample. A predictive probability distribution is said to be optimal if it is optimal based on a user-specified scoring rule, not generally speaking. 

Building on the previous research in this field, we conduct a numerical analysis first in section 2.2.2 and 2.2.3 to further investigate the effects of the form and degree of model misspecification on optimal forecast performance using data simulated from the (generalized) autoregressive conditional heteroscedasticity ((G)ARCH) and copula models.

### 2.2.2 Simulation design 

In order to prepare the financial application analysis in following sections of this paper, we simulate a time series dependnet variable $y_t$ that mimic the behavior of financial returns and volatility. Specifically, GARCH models can capture the volatility clustering and serial dependence, and the negative marginal skewness will be introducted using an inversion copula with degree of skewness controlled by the shape parameter showed below. In order to manipulate the degree and form of model misspecification, we fix the underlying model to a simple and clearly misspecified ARCH(1) model.

Specific simulation scenarios are listed in following table ($t_\nu$ indicates a Student-t distribution with $\nu$ degrees of freedom). 

\begin{table}[!h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\multicolumn{3}{|c|}{Simulation design} \\ \hline
          & \textbf{Scenario (i)} & \textbf{Scenario (ii)} \\ \hline
\textbf{True DGP} & $y_t = \sigma_t \epsilon_t$ & copula \\
& $\sigma_t^2 = 1 + 0.2y_{t-1}^2+0.7\sigma_{t-1}^2$ & Shape parameter = {0, -3, -5}\\ & $\epsilon_t \sim (\frac{\nu-2}{\nu})^0.5 * t_\nu$ & \\ & $\nu \in (3, 12, 10000)$ & \\ \hline

\textbf{Assumed Model} & $y_t = \beta+\sigma_t \epsilon_t$ & $y_t = \beta +\sigma_t \epsilon_t$\\ & $\sigma_t^2 = \alpha_0 + \alpha_1 \sigma_{t-1}^2$ & $\sigma_t^2 = \alpha_0 + \alpha_1 \sigma_{t-1}^2$\\ & $\epsilon_t \sim N(0,1)$ & $\epsilon_t \sim N(0,1)$\\ \hline
\end{tabular}
\end{table}

The tails of density are important for risk management in finance so that we decide to apply focused scoring rule (FSR) to 10%, 20%, 80% and 90% tails of the distribution, corresponding to short and long portfolios in finance respectively. The methodology in Loaiza-Maya et al., (2020) and Loaiza-Maya, Martin & Frazier, (2019) is adopted. We conduct each simulation scenario as follows (Loaiza-Maya et al., 2020): 

  1.	Generate T = 6000 observations for $y_t$ from the true DGP
  
  2.	Use $y_{1:1000}$ to estimate $\boldsymbol{\hat \theta}$ in the assumed predictive model based on the positively-oriented score $S_i$ where $\boldsymbol{\hat \theta} = \{\hat \beta, \hat \alpha_0, \hat \alpha_1\}$

\begin{align}
{\hat \theta^{[i]}}:=arg\max_{\theta \in \Theta}\bar S_i({\theta})
\end{align}

\begin{align}
\bar S_i(\theta):=\frac{1}{T-(\tau+1)}\sum^{T-\tau}_{t=2}S(P_{\theta}^{t-1},y_t)
\end{align}

  3.	Produce the one-step-ahead predictive $P^{t-1}_{\hat \theta_1^{[i]}}$ And compute the out-of-sample score using $S_j$, where $S_i$ and $S_j$ refer to (?)(?)(?)


....

### 2.2.3 Simulation results
#### 2.2.3.1 Average score tables

Loaiza-Maya et al., (2020) introduce the concepts of ‘coherence’ and ‘strict coherence’ for conveniently documenting the optimal forecasts’ performance. Coherence means when the optimal probabilistic forecast based on a given score is superior, or at least performs the same as, alternative forecasts according to the same score. Strict coherence is when the optimal prediction is strictly preferable given that score. Let $\boldsymbol{\hat \theta}_1$ and $\boldsymbol{\hat \theta}_2$ be the optimizers based on $S_1, S_2$, then the prediction is said to coherent if 
$$\frac{1}{\tau}\sum_{t=T-\tau+1}^{T}S_1(P^{t-1}_{\boldsymbol{\hat \theta_1}},y_t) \geq \frac{1}{\tau}\sum_{t=T-\tau+1}^{T}S_1(P^{t-1}_{\boldsymbol{\hat \theta_2}},y_t)$$

$$\frac{1}{\tau}\sum_{t=T-\tau+1}^{T}S_2(P^{t-1}_{\boldsymbol{\hat \theta_1}},y_t) \leq \frac{1}{\tau}\sum_{t=T-\tau+1}^{T}S_2(P^{t-1}_{\boldsymbol{\hat \theta_1}},y_t)$$

It is said to be strictly coherent if the equality doesn't hold in (?)(?). 

\textbf{[need to mention some results from opt paper - one sentence]}

The first column of following tables shows $S_i$ that we used to produce predictions and the first row shows $S_j$, which is used for forecasts evaluation. The bolded value(s) is the maximum(s) in each column. We use all positively oriented scoring rules, and therefore, the column maximum(s) indicate(s) the optimal prediction(s) based on the score indicated by the column name. 

```{r}
# Editting the matlab written csv files 

GARCH_t12 <- read.csv("GARCH_t12__table.csv")
GARCH_t12 <- rename(GARCH_t12, "In-sample optimizers" = Row, "FSR10" = FSR10, "FSR20"=FSR20, "FSR80"=FSR80,"FSR90"=FSR90)
GARCH_t12$`In-sample optimizers`<-c("LS","CRPS","FSR10","FSR20","FSR80","FSR90")

GARCH_t3 <- read.csv("GARCH_t3_table.csv")
GARCH_t3 <- rename(GARCH_t3, "In-sample optimizers" = Row, "FSR10" = FSR10, "FSR20"=FSR20, "FSR80"=FSR80,"FSR90"=FSR90)
GARCH_t3$`In-sample optimizers`<-c("LS","CRPS","FSR10","FSR20","FSR80","FSR90")

ARCH_table <- read.csv("ARCH_table.csv")
ARCH_table <- rename(ARCH_table, "In-sample optimizers" = Row, "FSR10" = FSR10, "FSR20"=FSR20, "FSR80"=FSR80,"FSR90"=FSR90)
ARCH_table$`In-sample optimizers`<-c("LS","CRPS","FSR10","FSR20","FSR80","FSR90")

Copula5_table <- read.csv("Copula5_table.csv")
Copula5_table <- rename(Copula5_table, "In-sample optimizers" = Row, "FSR10" = FSR10, "FSR20"=FSR20, "FSR80"=FSR80,"FSR90"=FSR90)
Copula5_table$`In-sample optimizers`<-c("LS","CRPS","FSR10","FSR20","FSR80","FSR90")

Copula3_table <- read.csv("Copula3_table.csv")
Copula3_table <- rename(Copula3_table, "In-sample optimizers" = Row, "FSR10" = FSR10, "FSR20"=FSR20, "FSR80"=FSR80,"FSR90"=FSR90)
Copula3_table$`In-sample optimizers`<-c("LS","CRPS","FSR10","FSR20","FSR80","FSR90")

Copula0_table <- read.csv("Copula0_table.csv")
Copula0_table <- rename(Copula0_table, "In-sample optimizers" = Row, "FSR10" = FSR10, "FSR20"=FSR20, "FSR80"=FSR80,"FSR90"=FSR90)
Copula0_table$`In-sample optimizers`<-c("LS","CRPS","FSR10","FSR20","FSR80","FSR90")

```


```{r}
ARCH_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `FSR10` = cell_spec(format(`FSR10`,4),"latex",bold = ifelse(`FSR10`==max(`FSR10`),TRUE,FALSE)),
    `FSR20` = cell_spec(format(`FSR20`,4),"latex",bold = ifelse(`FSR20`==max(`FSR20`),TRUE,FALSE)),
    `FSR80` = cell_spec(format(`FSR80`,4),"latex",bold = ifelse(`FSR80`==max(`FSR80`),TRUE,FALSE)),
    `FSR90` = cell_spec(format(`FSR90`,4),"latex",bold = ifelse(`FSR90`==max(`FSR90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 1: the true DGP is ARCH(1)"=7))
```

```{r}
GARCH_t12 %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `FSR10` = cell_spec(format(`FSR10`,4),"latex",bold = ifelse(`FSR10`==max(`FSR10`),TRUE,FALSE)),
    `FSR20` = cell_spec(format(`FSR20`,4),"latex",bold = ifelse(`FSR20`==max(`FSR20`),TRUE,FALSE)),
    `FSR80` = cell_spec(format(`FSR80`,4),"latex",bold = ifelse(`FSR80`==max(`FSR80`),TRUE,FALSE)),
    `FSR90` = cell_spec(format(`FSR90`,4),"latex",bold = ifelse(`FSR90`==max(`FSR90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 2: the true DGP is GARCH(1,1) with degree of freedom = 12"=7))
```


```{r}
GARCH_t3 %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `FSR10` = cell_spec(format(`FSR10`,4),"latex",bold = ifelse(`FSR10`==max(`FSR10`),TRUE,FALSE)),
    `FSR20` = cell_spec(format(`FSR20`,4),"latex",bold = ifelse(`FSR20`==max(`FSR20`),TRUE,FALSE)),
    `FSR80` = cell_spec(format(`FSR80`,4),"latex",bold = ifelse(`FSR80`==max(`FSR80`),TRUE,FALSE)),
    `FSR90` = cell_spec(format(`FSR90`,4),"latex",bold = ifelse(`FSR90`==max(`FSR90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 3: the true DGP is GARCH(1,1) with degree of freedom = 3"=7))
```

The results from Table 1-3 show one particular type of model misspecification. That is, the underlying model ARCH(1) cannot capture the volatility clustering feature of the true data generation process (DGP), thus misspecification. Besides, with the degree of freedom increases, the data generated from GARCH(1,1) model is more like data that are from ARCH(1) and therefore, the degree of model misspecification decreases. When degree of freedom is large enough, we consider ARCH(1) as a correct specified model as shown in Table 1. 

From Table 3. we could observe strict coherence when degree of model misspecification is high and coherence in Table 1 which is stated as correct specification case. These results are consistent with conclusions from (Loiaza-Maya et al., 2020). It simply means that we can improve prediction accuracy in the presence of model misspecification ?? , and gain more from using optimal forecasts method while the degree of model misspecification is high. In other words, we are expected to gain more from using focused score when we observe 'strict coherence' while comparing two forecasts. 

```{r}

Copula0_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `FSR10` = cell_spec(format(`FSR10`,4),"latex",bold = ifelse(`FSR10`==max(`FSR10`),TRUE,FALSE)),
    `FSR20` = cell_spec(format(`FSR20`,4),"latex",bold = ifelse(`FSR20`==max(`FSR20`),TRUE,FALSE)),
    `FSR80` = cell_spec(format(`FSR80`,4),"latex",bold = ifelse(`FSR80`==max(`FSR80`),TRUE,FALSE)),
    `FSR90` = cell_spec(format(`FSR90`,4),"latex",bold = ifelse(`FSR90`==max(`FSR90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 4: the true DGP is copula with shape parameter = 0"=7))
```


```{r}
Copula3_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `FSR10` = cell_spec(format(`FSR10`,4),"latex",bold = ifelse(`FSR10`==max(`FSR10`),TRUE,FALSE)),
    `FSR20` = cell_spec(format(`FSR20`,4),"latex",bold = ifelse(`FSR20`==max(`FSR20`),TRUE,FALSE)),
    `FSR80` = cell_spec(format(`FSR80`,4),"latex",bold = ifelse(`FSR80`==max(`FSR80`),TRUE,FALSE)),
    `FSR90` = cell_spec(format(`FSR90`,4),"latex",bold = ifelse(`FSR90`==max(`FSR90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 5: the true DGP is copula with shape parameter = -3"=7))
```


```{r}
Copula5_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `FSR10` = cell_spec(format(`FSR10`,4),"latex",bold = ifelse(`FSR10`==max(`FSR10`),TRUE,FALSE)),
    `FSR20` = cell_spec(format(`FSR20`,4),"latex",bold = ifelse(`FSR20`==max(`FSR20`),TRUE,FALSE)),
    `FSR80` = cell_spec(format(`FSR80`,4),"latex",bold = ifelse(`FSR80`==max(`FSR80`),TRUE,FALSE)),
    `FSR90` = cell_spec(format(`FSR90`,4),"latex",bold = ifelse(`FSR90`==max(`FSR90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 6: the true DGP is copula with shape parameter = -5"=7))
```

\textbf{[need copula paper citation here to explain more]}

Table 4-6 show another type of model misspecification where the underlying model cannot capture the asymmetric feature of the true DGP. The results give similar conclusions to Table 1-3. It proves that optimal forecasts perform in the same way even when we have different types of model mispecification. 

#### 2.2.3.2 Score traces plots  

As stated in previous literature, focused score tends to outperform CRPS and logarithmic score in predicting tails. Therefore, we provide score trace plots to exhibit the comparison of LS, CRPS and FSR’s performance in predicting the 10 percentile and 90 percentile in figure 1 and 2. In the left panel, it shows the performance of each score evaluated by FSR 10%. That is, if the average score value of score A used for estimation is higher than score B, then score A predicts the 10 percentile of the distribution more accurately than score B. The right panel shows the same thing but is based on performance of predicting 90 percentile. From figure 1 and 2, we could see that FSR makes no difference if the degree of model misspecification is very small but as it increases, FSR starts to outperform CRPS and MLE (LS) in predicting tails. The more misspecified the model is, the more benefits can we gain from using FSR. It is beneficial for econometricians to improve forecast accuracy without working so hard on finding a ‘correct’ model in the fact that in practice, a correct model does not exist. 

```{r}
# need to adjust the size. 
knitr::include_graphics("figure 1.pdf")
```

```{r}
knitr::include_graphics("figure 2.pdf")
```

Moreover, the predictive density plots (Appendix 1) in figure 3 to 8, further support our conclusions drawn from the average score tables and score trace plots. They show that predictions obtained from optimizing focused scores aim to match the shape of tails of the true distribution and therefore, outperform predictions from using LS. 

## 2.3 Empirical analysis 

From the results in section 2.2, we can conclude that optimal forecasts based on focused score can improve the prediction accuracy in tails and its effect is clearest when the degree of model misspecification is high and does not vary much when we have different types of model misspecification. In section 2.3, we apply this method to an empirical setting and investigate how it performs in practice. 

The data used in the analysis is the continuously compounded daily log returns of S&P500 listed in U.S.A financial market over a time period that precedes the 2008 Global financial crisis (GFC) and extends to the latest period in which COVID-19 has impacts on financial market. The methodology used here is the same as in the section 2.2.2 with initial training sample size set to be 1000 and 5000 out-of-sample observations. 

### 2.3.1 Empirical analysis results 

```{r}
# data cleaning and import 
sp <- read.csv("^GSPC.csv")
sp <- sp %>% dplyr::select("Date","Adj.Close")

sp$log.returns <- c(NA,diff(log(sp$Adj.Close)))
logret <- sp[-c(1:41),c(1,3)] # delete the adjusted price col, only keep 6000 obs

# export the dataset to csv. including only trading days and log returns 
temp <- logret[,c(2)]
temp <- data.frame(temp)
write_csv(temp,"sp500log.csv")
```


```{r, results='hide'}
# descriptive statistics 
summary(logret$log.returns) # ----> need to be tabulated 
range(logret$log.returns)
# some test results aginst normal dist h_null
# JB test
jb.norm.test(logret$log.returns)
# Ljung box test
Box.test(logret$log.returns,lag = 2,type = "Lj",fitdf = 1)

skewness(logret$log.returns)
kurtosis(logret$log.returns)
```

```{r}
sum <- data.frame(Stock="S&P500",Min=-0.1276522,Median=0.0006105,Mean=0.0002453,Max=0.1095720,Skewness= -0.3807978,Kurtosis=13.5123,JB.Test=27772,LB.Test=60.627)
sum %>% 
  kable("latex",booktabs = T) %>% 
  kable_styling("bordered") %>% 
  add_header_above(c("Table ?: Descriptive statistics"=9))
```


```{r,message=FALSE,fig.align="center"}

# time series plot
logret$Date <- as.Date(logret$Date,format = "%Y-%m-%d")

p1 <- logret %>% ggplot(aes(x = Date, y = log.returns)) +
  geom_line()+theme_classic()+labs(x="Year",y="Daily log returns")+
  #ggtitle("Figure 9: S&P500 daily log returns")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_y_continuous(labels = scales::percent) + 
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.12,label="Global Financial Crisis",color="red",size=3)+
  annotate(geom = 'text',x=as.Date("2020-04-01"),y=-0.12,label="COVID-19",color="red",size=2)+
  theme_clean()
  #+labs(caption = "Figure 9: the time series plot of S&P500 daily log returns")

# histogram plots
p2 <- logret %>% ggplot()+geom_histogram(aes(x=log.returns))+xlim(-0.13,0.13)+
  #ggtitle("Figure 10")+
  scale_x_continuous(labels = scales::percent)+theme_clean()
  #+labs(caption = "Figure 10: the distribution of S&P500 log returns is asymmetric")

# ACF plots to see the autocorrelation 
logret <- logret %>% 
  mutate(trading_day = row_number()) %>%
  as_tsibble(index = trading_day,regular = TRUE)

p3 <- logret %>% ACF((log.returns)^2) %>% autoplot() + 
  #ggtitle("Figure 11:Squared log returns autocorrelation")+
  xlab("number of lags")+ylab("ACF")+theme_clean()
# +labs(caption = "Figure 11: the autocorrelation plot shows there is a strong serial correlation in volatility of S&P500")

ggarrange(p1,p2,p3,ncol=2,nrow = 2, labels = c("A: Time series plot","B: Histogram","C: ACF"),legend = "bottom")
```

The descriptive statistics shows that the distribution of S&P500 log returns are negatively skewed and have fat tails and strong serial correlation. Besides, during the GFC and COVID-19 pandemic periods, the volatility is very high due to high uncertainty. 

The reality is always the most complicated and the underlying ARCH(1) should have a high degree of model misspecification. Therefore, we should expect that predictions produced by optimizing FSR do a better job in predicting two tails of the distribution. The results of Table 9 support our conjecture. However, the difference between using FSR10(80) and FSR20(90) is very small. 

```{r}
sp_table <- read.csv("sp_table.csv")
sp_table <- rename(sp_table, "In-sample optimizers" = Row,"FSR10" = FSR10, "FSR20"=FSR20, "FSR80"=FSR80,"FSR90"=FSR90)
sp_table$`In-sample optimizers`<-c("LS","CRPS","FSR10","FSR20","FSR80","FSR90")

sp_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `FSR10` = cell_spec(format(`FSR10`,4),"latex",bold = ifelse(`FSR10`==max(`FSR10`),TRUE,FALSE)),
    `FSR20` = cell_spec(format(`FSR20`,4),"latex",bold = ifelse(`FSR20`==max(`FSR20`),TRUE,FALSE)),
    `FSR80` = cell_spec(format(`FSR80`,4),"latex",bold = ifelse(`FSR80`==max(`FSR80`),TRUE,FALSE)),
    `FSR90` = cell_spec(format(`FSR90`,4),"latex",bold = ifelse(`FSR90`==max(`FSR90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table 9: Average scores for S&P500"=7))
```

Unlike simulation, the score trace plots show how different scoring rules perform in real time points which correspond to some important events. In figure 12, one interesting phenomenon is that after the GFC, focused score performs much better than CRPS and MLE in terms of predicting both upper and lower tails. And this 'jump' effect continues until the very recent date. It indicates that optimal probabilistic forecasts might predict financial markets better in and after times of stock market turbulance, which could represent a great contribution to the existing financial analyses of risks and returns. 

```{r,fig.width=10,fig.height=5}
# to compress all the calculated plotting average scores together in the same df 
trace_fsr10 <- read.csv("fsr10.csv")
trace_fsr20 <- read.csv("fsr20.csv")
trace_10ls <- read.csv("fsr10_ls.csv")
trace_10crps <- read.csv("fsr10_crps.csv")

fsr10 <- data.frame(trace_fsr10,trace_fsr20,trace_10ls,trace_10crps)

# add back dates to the fsr10 df, initial step = 1000

fsr10 <- fsr10 %>% mutate(Date = logret$Date[1001:6000])

# plot scores --- lower tail 
fsr10$Date <- as.Date(fsr10$Date,format = "%Y-%m-%d")

color1 <- c("FSR10"="red","FSR20"="black","MLE"="purple","CRPS"="green")

fsr10plot <- fsr10 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = FSR10,color = "FSR10"),size=0.25)+
  geom_line(aes(y = FSR20,color = "FSR20"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="Average Scores",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-lower tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color1)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=0.08,label="Global Financial Crisis",color="darkred")+
  theme_clean()
  

# plot scores --- upper tail 
trace_fsr90 <- read.csv("fsr90.csv")
trace_fsr80 <- read.csv("fsr80.csv")
trace_90ls <- read.csv("fsr90_ls.csv")
trace_90crps <- read.csv("fsr90_crps.csv")

fsr90 <- data.frame(trace_fsr90,trace_fsr80,trace_90ls,trace_90crps)

# add back dates to the fsr10 df, initial step = 1000

fsr90 <- fsr90 %>% mutate(Date = logret$Date[1001:6000])

# plot scores --- lower tail 
fsr90$Date <- as.Date(fsr90$Date,format = "%Y-%m-%d")

color2 <- c("FSR90"="red","FSR80"="black","MLE"="purple","CRPS"="green")

fsr90plot <- fsr90 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = FSR90,color = "FSR90"),size=0.25)+
  geom_line(aes(y = FSR80,color = "FSR80"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-upper tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color2)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.05,label="Global Financial Crisis",color="darkred")+
  theme_clean()
  
# put two plots side by side 
ggarrange(fsr10plot,fsr90plot,ncol=2,legend = "bottom",labels = c("Figure 12(a) Lower Tail","Figure 12(b) Upper Tail"))

```

# 3. Finanical application: Value-at-Risk
## 3.1 Overview of Value-at-Risk

## 3.2 Value-at-Risk for simulation analysis
We illustrate our Value-at-Risk analysis by firstly using simulation data from section 2. 

```{r}
copula5var <- read.csv("copula5var.csv")

VaR_0.1 <- as.numeric(copula5var[,c(1,5,9,13,17,21)])
VaR_0.2 <- as.numeric(copula5var[,c(2,6,10,14,18,22)])
VaR_0.8 <- as.numeric(copula5var[,c(3,7,11,15,19,23)])
VaR_0.9 <- as.numeric(copula5var[,c(4,8,12,16,20,24)])

Optimizers <- c("LS","CRPS","FSR10","FSR20","FSR80","FSR90")

df <- data.frame(Optimizers,VaR_0.1,VaR_0.2,VaR_0.8,VaR_0.9)
df <- rename(df,"VaR at 10\\%"=VaR_0.1, "VaR at 20\\%"=VaR_0.2, "VaR at 80\\%"=VaR_0.8, "VaR at 90\\%"=VaR_0.9)

df %>% 
  mutate(
    `VaR at 10\\%` = cell_spec(format(`VaR at 10\\%`,4),"latex",bold = ifelse(abs(`VaR at 10\\%`-0.1) == min(abs(`VaR at 10\\%`-0.1)),TRUE,FALSE)),
    `VaR at 20\\%` = cell_spec(format(`VaR at 20\\%`,4),"latex",bold = ifelse(abs(`VaR at 20\\%`-0.2) == min(abs(`VaR at 20\\%`-0.2)),TRUE,FALSE)),
    `VaR at 80\\%` = cell_spec(format(`VaR at 80\\%`,4),"latex",bold = ifelse(abs(`VaR at 80\\%`-0.8) == min(abs(`VaR at 80\\%`-0.8)),TRUE,FALSE)),
    `VaR at 90\\%` = cell_spec(format(`VaR at 90\\%`,4),"latex",bold = ifelse(abs(`VaR at 90\\%`-0.9) == min(abs(`VaR at 90\\%`-0.9)),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F,booktabs=T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  add_header_above(c(" ","Out-of-sample exceedances"=4)) %>%
  add_header_above(c("Table 7: VaR when true DGP is copula (shape = -5)"=5))
```


```{r}
garch3var <- read.csv("garch3var.csv")

VaR0.1 <- as.numeric(garch3var[,c(1,5,9,13,17,21)])
VaR0.2 <- as.numeric(garch3var[,c(2,6,10,14,18,22)])
VaR0.8 <- as.numeric(garch3var[,c(3,7,11,15,19,23)])
VaR0.9 <- as.numeric(garch3var[,c(4,8,12,16,20,24)])

df <- data.frame(Optimizers,VaR0.1,VaR0.2,VaR0.8,VaR0.9)
df <- rename(df,"VaR at 10\\%"=VaR0.1, "VaR at 20\\%"=VaR0.2, "VaR at 80\\%"=VaR0.8, "VaR at 90\\%"=VaR0.9)

df %>% 
  mutate(
    `VaR at 10\\%` = cell_spec(format(`VaR at 10\\%`,4),"latex",bold = ifelse(abs(`VaR at 10\\%`-0.1) == min(abs(`VaR at 10\\%`-0.1)),TRUE,FALSE)),
    `VaR at 20\\%` = cell_spec(format(`VaR at 20\\%`,4),"latex",bold = ifelse(abs(`VaR at 20\\%`-0.2) == min(abs(`VaR at 20\\%`-0.2)),TRUE,FALSE)),
    `VaR at 80\\%` = cell_spec(format(`VaR at 80\\%`,4),"latex",bold = ifelse(abs(`VaR at 80\\%`-0.8) == min(abs(`VaR at 80\\%`-0.8)),TRUE,FALSE)),
    `VaR at 90\\%` = cell_spec(format(`VaR at 90\\%`,4),"latex",bold = ifelse(abs(`VaR at 90\\%`-0.9) == min(abs(`VaR at 90\\%`-0.9)),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs=T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  add_header_above(c(" ","Out-of-sample exceedances"=4)) %>%
  add_header_above(c("Table 8: VaR when true DGP is GARCH with degree of freedom = 3"=5))
```

The values showed in these tables refer to the proportion of exceedances over the value at risk indicated by the column. Thus, the closer the values to nominal VaR levels, the more accurate are the optimal predictions. We can see that in two different forms of model misspecification, FSR is strictly superior to other scores in the upper tails, while in the lower tails, the difference is quite small when not strictly preferable. This result helps us understand what to expect in practical VaR analysis. 

## 3.3 Value-at-Risk for empirical analysis

```{r}
spvar <- read.csv("spvar1_table.csv")

VaR_0.1 <- as.numeric(spvar[,c(1,5,9,13,17,21)])
VaR_0.2 <- as.numeric(spvar[,c(2,6,10,14,18,22)])
VaR_0.8 <- as.numeric(spvar[,c(3,7,11,15,19,23)])
VaR_0.9 <- as.numeric(spvar[,c(4,8,12,16,20,24)])

df <- data.frame(Optimizers,VaR_0.1,VaR_0.2,VaR_0.8,VaR_0.9)
df <- rename(df,"VaR at 10\\%"=VaR_0.1, "VaR at 20\\%"=VaR_0.2, "VaR at 80\\%"=VaR_0.8, "VaR at 90\\%"=VaR_0.9)

df %>% 
  mutate(
    `VaR at 10\\%` = cell_spec(format(`VaR at 10\\%`,4),"latex",bold = ifelse(abs(`VaR at 10\\%`-0.1) == min(abs(`VaR at 10\\%`-0.1)),TRUE,FALSE)),
    `VaR at 20\\%` = cell_spec(format(`VaR at 20\\%`,4),"latex",bold = ifelse(abs(`VaR at 20\\%`-0.2) == min(abs(`VaR at 20\\%`-0.2)),TRUE,FALSE)),
    `VaR at 80\\%` = cell_spec(format(`VaR at 80\\%`,4),"latex",bold = ifelse(abs(`VaR at 80\\%`-0.8) == min(abs(`VaR at 80\\%`-0.8)),TRUE,FALSE)),
    `VaR at 90\\%` = cell_spec(format(`VaR at 90\\%`,4),"latex",bold = ifelse(abs(`VaR at 90\\%`-0.9) == min(abs(`VaR at 90\\%`-0.9)),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs=T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  add_header_above(c(" ","Out-of-sample exceedances"=4)) %>%
  add_header_above(c("Table 11: VaR for S&P500 before GFC"=5))

# formattable(df,list(VaR_0.1 = max_bold1,VaR_0.2 = max_bold2,VaR_0.8 = max_bold8,VaR_0.9 = max_bold9))
```


```{r}
spvar <- read.csv("spvar2_table.csv")
spvar <- round(spvar,digits = 4)

VaR_0.1 <- as.numeric(spvar[,c(1,5,9,13,17,21)])
VaR_0.2 <- as.numeric(spvar[,c(2,6,10,14,18,22)])
VaR_0.8 <- as.numeric(spvar[,c(3,7,11,15,19,23)])
VaR_0.9 <- as.numeric(spvar[,c(4,8,12,16,20,24)])

df <- data.frame(Optimizers,VaR_0.1,VaR_0.2,VaR_0.8,VaR_0.9)
df <- rename(df,"VaR at 10\\%"=VaR_0.1, "VaR at 20\\%"=VaR_0.2, "VaR at 80\\%"=VaR_0.8, "VaR at 90\\%"=VaR_0.9)

df %>% 
  mutate(
    `VaR at 10\\%` = cell_spec(format(`VaR at 10\\%`,4),"latex",bold = ifelse(abs(`VaR at 10\\%`-0.1) == min(abs(`VaR at 10\\%`-0.1)),TRUE,FALSE)),
    `VaR at 20\\%` = cell_spec(format(`VaR at 20\\%`,4),"latex",bold = ifelse(abs(`VaR at 20\\%`-0.2) == min(abs(`VaR at 20\\%`-0.2)),TRUE,FALSE)),
    `VaR at 80\\%` = cell_spec(format(`VaR at 80\\%`,4),"latex",bold = ifelse(abs(`VaR at 80\\%`-0.8) == min(abs(`VaR at 80\\%`-0.8)),TRUE,FALSE)),
    `VaR at 90\\%` = cell_spec(format(`VaR at 90\\%`,4),"latex",bold = ifelse(abs(`VaR at 90\\%`-0.9) == min(abs(`VaR at 90\\%`-0.9)),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs=T, linesep = "") %>%
  kable_styling("bordered",full_width = F) %>%
  add_header_above(c(" ","Out-of-sample exceedances"=4)) %>%
  add_header_above(c("Table 12: VaR for S&P500 after GFC"=5))

# formattable(df,list(VaR_0.1 = max_bold1,VaR_0.2 = max_bold2,VaR_0.8 = max_bold8,VaR_0.9 = max_bold9))
```


# 4. VIX analysis 
## 4.1 Background and notation 

Previously, we investigate how scoring rules perform in predicting financial returns and volatility. However, the volatility modelled/simulated in (G)ARCH models are ...

VIX is different from volatility modelled through any models. It is a measure for implied volatility, which is non-parametric. 

## 4.2 Model specification

The difference and similarity between realised volatility and VIX. why we can use model for RV to model VIX. 

The following tables and figures are produced by CBOE VIX data from 13 Aug,1996 - 16 July,2020 with daily frequency. 

Initial sample size = 1000

Out-of-sample size = 5000

### Predictive model: 
Let $y_t$ = $log(VIX_t)$, then 
$$y_{t+1}=\beta_0+\beta_1y_t+\beta_2y_{t-5,t}+\beta_3y_{t-22,t}+\sigma_{t+1}z_{t+1}, \\z_{t+1} \sim N(0,1)$$ 
$$\hat y_{t+1}=\hat \beta_0+ \hat \beta_1y_t+\hat \beta_2y_{t-5,t}+\hat \beta_3y_{t-22,t}$$
$$\hat y_{t+2}=\hat \beta_0+\hat \beta_1 \hat y_{t+1}+\hat \beta_2y_{t-4,t+1}+\hat \beta_3y_{t-21,t+1}$$
$$y_{t-5,t}=\frac{1}{5}(y_t+...+y_{t-4})$$
$$y_{t-22,t}=\frac{1}{22}(y_t+...+y_{t-21})$$

```{r,message=F}
# data cleaning and import 
vix <- read.csv("^VIX.csv")
vix <- vix %>% dplyr::select("Date","Adj.Close")

vix$logvix <- log(vix$Adj.Close)

# previous day log vix values
prev <- matrix(0,nrow = nrow(vix)+1,ncol = 1)
for (i in 1:nrow(vix)){
  prev[1,1] <- c("NA")
  prev[i+1,1] <- vix$logvix[i]
}
prev <- as.data.frame(prev)
prev <- prev[-c(1:178,6179),1]

# need to be very careful about the time index -- dependent variable starting with t+1
MA5 <- matrix(0,nrow=nrow(vix)-5,ncol = 1)
for (i in 1:(nrow(vix)-5)){
  sum = 0
  for (j in 1:5){
    sum = sum + vix$logvix[i+j-1]
  }
  MA5[i,1] = (1/5)*sum
}

MA22 <- matrix(0,nrow = nrow(vix)-22,ncol = 1)
for (i in 1:(nrow(vix)-22)){
  sum = 0
  for (j in 1:22){
    sum = sum + vix$logvix[i+j-1]
  }
  MA22[i,1] = (1/22)*sum
}


#require(zoo)
#MA5 <- rollmean(vix$logvix,5,align = "center")
MA5 <- as.data.frame(MA5) # average of past week
MA5 <- MA5[-c(1:173),1] # including only 6000 obs 

#MA22 <- rollmean(vix$logvix,22,align = "center")
MA22 <- as.data.frame(MA22) # average of past month
MA22 <- MA22[-c(1:156),1] # including only 6000 obs

logvix <- vix[-c(1:178),c(1,3)] # delete the adjusted vix col, only keep 6000 obs
logvix <- cbind(logvix,prev,MA5,MA22)
# export the dataset to csv. including only trading days and log returns

# temp <- logvix[,2:5]
# write_csv(temp,"vixlog.csv")

# the logvix, previous log vix and moving average values have the same length in the final file. 
```


```{r, results='hide'}
# descriptive statistics 
summary(vix$logvix) # ----> need to be tabulated 
# summary(vix$Adj.Close)
range(vix$logvix)
# some test results aginst normal dist h_null
# JB test
jb.norm.test(vix$logvix)
# Ljung box test
Box.test(vix$logvix,lag = 2,type = "Lj",fitdf = 1)

vixsk <- skewness(vix$logvix)
vixkt <- kurtosis(vix$logvix)
```


```{r}
sum <- data.frame(Series="Log(VIX)",Min = 2.213, Median = 2.916, Mean = 2.935, Max = 4.415,Skewness= vixsk,Kurtosis=vixkt,JB.Test=415.06,LB.Test=11744)
sum %>% 
  kable("latex",booktabs = T) %>% 
  kable_styling("bordered")%>% 
  add_header_above(c("Table ?: Descriptive statistics"=9))
```


```{r,message=FALSE,fig.align='center'}

# time series plot
logvix$Date <- as.Date(logvix$Date,format = "%Y-%m-%d")

logvix %>% ggplot(aes(x = Date, y = logvix)) +
  geom_line()+theme_classic()+labs(x="Year",y="Daily log(VIX) values")+
  ggtitle("Figure ?: Daily values for Log(VIX)")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  #scale_y_continuous(labels = scales::percent) + 
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=2,label="Global Financial Crisis",color="red",size=3)+
  annotate(geom = 'text',x=as.Date("2020-04-01"),y=2,label="COVID-19",color="red",size=3)+
  theme_clean()+
  labs(caption = "Figure ?: the time series plot of daily log(VIX) values")

# histogram plots
logvix %>% ggplot()+geom_histogram(aes(x=logvix))+ggtitle("Figure ?")+
  #scale_x_continuous(labels = scales::percent)+
  theme_clean()+
  labs(caption = "Figure ?: the distribution of log(VIX) is asymmetric")

# ACF plots to see the autocorrelation 
logvix <- logvix %>% 
  mutate(trading_day = row_number()) %>%
  as_tsibble(index = trading_day,regular = TRUE)

logvix %>% ACF((logvix)^2) %>% autoplot() + ggtitle("Figure ?:autocorrelation of log(VIX)")+xlab("number of lags")+ylab("ACF")+theme_clean()+labs(caption = "Figure 11: the autocorrelation plot shows there is a strong serial correlation in variance of log(VIX)")
```


```{r}
vix_table <- read.csv("vix_table.csv")
vix_table <- rename(vix_table, "In-sample optimizers" = Row,"FSR10" = FSR10, "FSR20"=FSR20, "FSR80"=FSR80,"FSR90"=FSR90)
vix_table$`In-sample optimizers`<-c("LS","CRPS","FSR10","FSR20","FSR80","FSR90")

vix_table %>% 
  mutate(
    LS = cell_spec(format(LS,4),"latex",bold = ifelse(LS == max(LS),TRUE,FALSE)),
    CRPS = cell_spec(format(CRPS,4),"latex",bold = ifelse(CRPS==max(CRPS),TRUE,FALSE)),
    `FSR10` = cell_spec(format(`FSR10`,4),"latex",bold = ifelse(`FSR10`==max(`FSR10`),TRUE,FALSE)),
    `FSR20` = cell_spec(format(`FSR20`,4),"latex",bold = ifelse(`FSR20`==max(`FSR20`),TRUE,FALSE)),
    `FSR80` = cell_spec(format(`FSR80`,4),"latex",bold = ifelse(`FSR80`==max(`FSR80`),TRUE,FALSE)),
    `FSR90` = cell_spec(format(`FSR90`,4),"latex",bold = ifelse(`FSR90`==max(`FSR90`),TRUE,FALSE))
  ) %>%
  kable("latex",escape = F, booktabs = T, linesep = "") %>%
  kable_styling("bordered",full_width = F)%>%
  add_header_above(c("","Average out-of-sample scores"=6)) %>% 
  add_header_above(c("Table ?: Average scores for VIX"=7))
```


The lower (upper) tail shows the prediction evaluation of 4 scoring rules by FSR10 (FSR90). 

The scoring rules used for optimization are {MLE, CRPS, FSR10(90), FSR20(80)}. 

```{r,fig.width=10,fig.height=5}
# to compress all the calculated plotting average scores together in the same df 
trace_fsr10 <- read.csv("vixfsr10.csv")
trace_fsr20 <- read.csv("vixfsr20.csv")
trace_10ls <- read.csv("vixfsr10_ls.csv")
trace_10crps <- read.csv("vixfsr10_crps.csv")

fsr10 <- data.frame(trace_fsr10,trace_fsr20,trace_10ls,trace_10crps)

# add back dates to the fsr10 df, initial step = 1000

fsr10 <- fsr10 %>% mutate(Date = vix$Date[1001:6000])

# plot scores --- lower tail 
fsr10$Date <- as.Date(fsr10$Date,format = "%Y-%m-%d")

color1 <- c("FSR10"="red","FSR20"="black","MLE"="purple","CRPS"="green")

fsr10plot <- fsr10 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = FSR10,color = "FSR10"),size=0.25)+
  geom_line(aes(y = FSR20,color = "FSR20"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="Average Scores",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-lower tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color1)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=0.08,label="Global Financial Crisis",color="darkred")+
  theme_clean()
  

# plot scores --- upper tail 
trace_fsr90 <- read.csv("vixfsr90.csv")
trace_fsr80 <- read.csv("vixfsr80.csv")
trace_90ls <- read.csv("vixfsr90_ls.csv")
trace_90crps <- read.csv("vixfsr90_crps.csv")

fsr90 <- data.frame(trace_fsr90,trace_fsr80,trace_90ls,trace_90crps)

# add back dates to the fsr10 df, initial step = 1000

fsr90 <- fsr90 %>% mutate(Date = vix$Date[1001:6000])

# plot scores --- lower tail 
fsr90$Date <- as.Date(fsr90$Date,format = "%Y-%m-%d")

color2 <- c("FSR90"="red","FSR80"="black","MLE"="purple","CRPS"="green")

fsr90plot <- fsr90 %>% ggplot(aes(x=Date)) +
  geom_line(aes(y = FSR90,color = "FSR90"),size=0.25)+
  geom_line(aes(y = FSR80,color = "FSR80"),size=0.25)+
  geom_line(aes(y = MLE,color = "MLE"),size=0.25)+
  geom_line(aes(y = CRPS,color = "CRPS"),size=0.25)+
  labs(x="Year",y="",color="legend")+
  #ggtitle("Figure 12: S&P500 Score traces-upper tail")+
  scale_x_date(date_breaks = "years",date_labels = "%y")+
  scale_color_manual(values = color2)+
  annotate(geom = 'text',x=as.Date("2008-09-18"),y=-0.05,label="Global Financial Crisis",color="darkred")+
  theme_clean()
  
# put two plots side by side 
ggarrange(fsr10plot,fsr90plot,ncol=2,legend = "bottom",labels = c("Figure ?(a) Lower Tail","Figure ?(b) Upper Tail"))

```

# References

DON'T forget alphabetical order!! 

cite packages used in the draft
```{r}
citation("formattable")
```

# Appendix 
## Appendix 1: Predictive Densities

The predictive density plots provide a straightforward way to visualize how optimal probabilistic forecast matches the shape of true DGP distribution through the simulation. Here we provide 4 different ‘time points’ to show how these scoring rules work, especially the focused score in figure 3 to 8. The blue line is the marginal distribution from the true DGP, and these figures show that predictive density produced from optimizing LS fits the area with high probability mass the best, which is also what LS is designed for. However, it performs poorly in terms of predicting tails when the degree of model misspecification is high in both GARCH and copula examples. This is when we benefit from using focused score, and we can see from figure 5 and 8 that predictions produced from optimizing focused score aim to match the shape of two tails of the true distribution. They outperform predictions from using Maximum likelihood estimation (MLE). 

```{r}
knitr::include_graphics("figure 3.pdf")
```

```{r}
knitr::include_graphics("figure 4.pdf")
```

```{r}
knitr::include_graphics("figure 5.pdf")
```

```{r}
knitr::include_graphics("figure 6.pdf")
```

```{r}
knitr::include_graphics("figure 7.pdf")
```

```{r}
knitr::include_graphics("figure 8.pdf")
```
